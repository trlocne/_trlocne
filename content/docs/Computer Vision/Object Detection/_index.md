---
title: Object Detection (Part 1 - Traditional Algorithms)
blog: true
weight: 4
math: true
comments: true
---

## 1. Intro to Object Detection

### 1.1. M·ª•c ti√™u ch√≠nh c·ªßa Object Detection

D·ª±a v√†o h√¨nh ·∫£nh b√™n d∆∞·ªõi, ta th·∫•y Object Detection (ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng) kh√¥ng ch·ªâ nh·∫≠n d·∫°ng (classification) ƒë·ªëi t∆∞·ª£ng trong ·∫£nh l√† g√¨ (v√≠ d·ª•: xe bu√Ωt, √¥ t√¥) m√† c√≤n x√°c ƒë·ªãnh v·ªã tr√≠ (localization) c·ªßa n√≥ b·∫±ng c√°ch khoanh v√πng, t·∫°o ra m·ªôt bounding box cho ƒë·ªëi t∆∞·ª£ng ƒë√≥ trong ·∫£nh.

N√≥i c√°ch kh√°c, **Object Detection** = **Classification** + **Localization**.

![alt text](image.png 'Object Detection')

### 1.2. L·ªãch s·ª≠ c·ªßa Object Detection

- **1998** : **Yan Lecun** t·∫°o ra m·∫°ng **LeNet**, m·ªôt trong nh·ªØng m·∫°ng CNN ƒë·∫ßu ti√™n. Tuy nhi√™n th·ªùi gian x·ª≠ l√Ω kh√° l√† ch·∫≠m.
- V·ªÅ sau **Object Detection** ƒë∆∞·ª£c chia l√†m 2 nh√°nh ch√≠nh:
  - **Multi-stage object detector**: ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng theo nhi·ªÅu giai ƒëo·∫°n.
  - **Single-stage object detector**: ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng tr·ªçng m·ªôt giai ƒë·ªçan.

{{% details title="Single-stage object detector" closed="true" %}}
- **2016**: YOLO (You Only Look Once) ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Joseph Redmon, s·ª≠ d·ª•ng single neural network ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng trong m·ªôt l·∫ßn duy nh·∫•t.
- **2016**: SSD (Single Shot MultiBox Detector) c≈©ng ra ƒë·ªùi, s·ª≠ d·ª•ng multi-reference v√† multi-resolution detection.
- **2017**: RetinaNet ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi T.-Y. Lin, s·ª≠ d·ª•ng focal loss ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ m·∫•t c√¢n b·∫±ng l·ªõp.
- **2019**: MobileNetV3 ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Andrew Howard, s·ª≠ d·ª•ng thu·∫≠t to√°n NAS+NetAdapt ƒë·ªÉ t·ªëi ∆∞u cho thi·∫øt b·ªã di ƒë·ªông.
- **2020**: DERT (Detection Transformer) ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Carion Nicolas, s·ª≠ d·ª•ng end-to-end attention mechanism.
{{% /details %}} 

{{% details title="Multi-stage object detector:" closed="true" %}}
- **2001**: HOG (Histogram of Oriented Gradients) ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi N. Dalal & B. Triggs, s·ª≠ d·ª•ng ƒë·ªÉ ph√°t hi·ªán c√°c l·ªõp ƒë·ªëi t∆∞·ª£ng kh√°c nhau.
- **2008**: DPM (Deformable Part Model) ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi P. Felzenszwalb & R. Girshick, s·ª≠ d·ª•ng root-filter v√† part-filter ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng.
- **2012**: AlexNet ra ƒë·ªùi, ƒë√°nh d·∫•u b∆∞·ªõc ph√°t tri·ªÉn v∆∞·ª£t b·∫≠c c·ªßa CNN (Convolutional Neural Network) trong th·ªã gi√°c m√°y t√≠nh.
- **2014**: R-CNN ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi R. Girshick, s·ª≠ d·ª•ng CNN ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ c√°c v√πng ƒë·ªÅ xu·∫•t (object candidate boxes).
- **2015**:
    - SPPNet ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi K. He, s·ª≠ d·ª•ng Spatial Pyramid Pooling ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa R-CNN.
    - Fast R-CNN c≈©ng do R. Girshick ƒë·ªÅ xu·∫•t, c·∫£i thi·ªán t·ªëc ƒë·ªô x·ª≠ l√Ω c·ªßa R-CNN.
- **2016**: Faster R-CNN ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi S. Ren, s·ª≠ d·ª•ng Region Proposal Network (RPN) ƒë·ªÉ t·∫°o v√πng ƒë·ªÅ xu·∫•t, tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω ƒë√°ng k·ªÉ.
- **2017**: Pyramid Networks ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi T.-Y. Lin, s·ª≠ d·ª•ng ki·∫øn tr√∫c top-down v·ªõi c√°c k·∫øt n·ªëi lateral ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng ·ªü nhi·ªÅu t·ªâ l·ªá kh√°c nhau.
- **2019**: Context R-CNN ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Beery S et.al, s·ª≠ d·ª•ng long term memory bank v√† contextual feature ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c.
- **2020**: MnasFPN ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Chen B et.al, s·ª≠ d·ª•ng ki·∫øn tr√∫c latency-aware ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô x·ª≠ l√Ω.
{{% /details %}}

![alt text](image-1.png 'Overview')

Ngu·ªìn: [https://onlinelibrary.wiley.com/doi/10.1155/2021/5808206](https://onlinelibrary.wiley.com/doi/10.1155/2021/5808206)


>[!NOTE]
>G·∫ßn ƒë√¢y, s·ª± xu·∫•t hi·ªán c·ªßa **Transformer** ƒë√£ th√∫c ƒë·∫©y m·∫°nh m·∫Ω s·ª± ph√°t tri·ªÉn c·ªßa **Object Detection**, mang l·∫°i hi·ªáu su·∫•t v∆∞·ª£t tr·ªôi so v·ªõi c√°c ph∆∞∆°ng ph√°p tr∆∞·ªõc ƒë√≥.
>C√°c m√¥ h√¨nh d·ª±a tr√™n **Transformer** ƒëang d·∫ßn tr·ªü th√†nh xu h∆∞·ªõng ch·ªß ƒë·∫°o trong lƒ©nh v·ª±c n√†y.


### 1.3. C√°c m√¥ h√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng cho b√†i to√°n Object Detection

1. **Giai ƒëo·∫°n ƒë·∫ßu** (*tr∆∞·ªõc Deep Learning*):

+ **DPM-v1 (2008)**: ƒê√¢y l√† m·ªôt ph∆∞∆°ng ph√°p ML truy·ªÅn th·ªëng, s·ª≠ d·ª•ng Deformable Part Model, ƒë·∫°t hi·ªáu su·∫•t mAP (mean Average Precision) kho·∫£ng 21 tr√™n t·∫≠p d·ªØ li·ªáu VOC07.

2. **Giai ƒëo·∫°n Deep Learning l√™n ng√¥i**:

+ **R-CNN (2014)**: ƒê√°nh d·∫•u s·ª± kh·ªüi ƒë·∫ßu c·ªßa vi·ªác √°p d·ª•ng CNN v√†o Object Detection, ƒë·∫°t mAP 58.5 tr√™n VOC07.
+ **Fast R-CNN (2015)** & **Faster R-CNN (2015)**: C·∫£i thi·ªán ƒë√°ng k·ªÉ t·ªëc ƒë·ªô v√† hi·ªáu su·∫•t so v·ªõi **R-CNN**.
+ **SSD (2016)**: Single Shot MultiBox Detector, m·ªôt ph∆∞∆°ng ph√°p **single-stage detector**, ƒë·∫°t t·ªëc ƒë·ªô x·ª≠ l√Ω cao.
+ **FPN (2017)** & **RetinaNet (2017)**: Ti·∫øp t·ª•c c·∫£i thi·ªán hi·ªáu su·∫•t ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng, ƒë·∫∑c bi·ªát l√† v·ªõi c√°c ƒë·ªëi t∆∞·ª£ng nh·ªè.

3. **Giai ƒëo·∫°n Transformers**:

- **DETR (2020):** M√¥ h√¨nh ƒë·∫ßu ti√™n s·ª≠ d·ª•ng Transformer cho Object Detection, m·ªü ra h∆∞·ªõng nghi√™n c·ª©u m·ªõi.
- **Deformable DETR (2021):** C·∫£i thi·ªán DETR b·∫±ng c√°ch th√™m c∆° ch·∫ø attention linh ho·∫°t h∆°n, ƒë·∫°t hi·ªáu su·∫•t v∆∞·ª£t tr·ªôi tr√™n c√°c t·∫≠p d·ªØ li·ªáu VOC07, VOC12 v√† COCO.
- **Swin Transformer (2021):** Ki·∫øn tr√∫c Transformer ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho th·ªã gi√°c m√°y t√≠nh, ƒë·∫°t hi·ªáu su·∫•t cao nh·∫•t trong t·∫•t c·∫£ c√°c m√¥ h√¨nh ƒë∆∞·ª£c li·ªát k√™, v·ªõi mAP 83.8 tr√™n VOC07.


{{% details title="Ph√¢n c·∫•p c√°c ph∆∞∆°ng ph√°p tƒÉng t·ªëc x·ª≠ l√≠ cho object detection" closed="true" %}}

![alt text](image-3.png)

**ƒê·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω OD, ng∆∞·ªùi ta chia c√°c ph∆∞∆°ng ph√°p ra th√†nh 3 level** (3 h∆∞·ªõng ti·∫øp c·∫≠n) ch√≠nh, t√πy theo t√≠nh ch·∫•t c·ªßa b√†i to√°n m√† nh√† nghi√™n c·ª©u c√≥ th·ªÉ ti·∫øp c·∫≠n theo level kh√°c nhau.

{{% details title="Speed up of numerical computations" closed="true" %}}

ƒê√¢y l√† c·∫•p ƒë·ªô th·∫•p nh·∫•t, t·∫≠p trung v√†o vi·ªác tƒÉng t·ªëc c√°c ph√©p t√≠nh s·ªë h·ªçc c∆° b·∫£n. 

C√°c k·ªπ thu·∫≠t ·ªü level n√†y bao g·ªìm:

- **Integral image:** S·ª≠ d·ª•ng c√°c ph√©p t√≠ch ph√¢n tr√™n ·∫£nh ƒë·ªÉ t√≠nh to√°n nhanh c√°c ƒë·∫∑c tr∆∞ng.
- **FFT:** S·ª≠ d·ª•ng Fast Fourier Transform ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô t√≠nh to√°n c√°c ph√©p t√≠nh t√≠ch ch·∫≠p.
- **Vector Quantization:** L∆∞·ª£ng t·ª≠ h√≥a vector ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc d·ªØ li·ªáu v√† tƒÉng t·ªëc ƒë·ªô t√≠nh to√°n.
- **Reduced rank approximation:** Lo·∫°i b·ªè c√°c th√¥ng tin d∆∞ th·ª´a b·∫±ng c√°ch gi·∫£m b·ªõt dimension kh√¥ng c·∫ßn thi·∫øt trong c√°c ma tr·∫≠n Convolution ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng ph√©p t√≠nh.

{{% /details %}}


{{% details title="Speed up of detec. engine" closed="true" %}}

C·∫•p ƒë·ªô n√†y t·∫≠p trung v√†o vi·ªác tƒÉng t·ªëc ƒë·ªô c·ªßa thu·∫≠t to√°n v√† m√¥ h√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng. 

**C√°c k·ªπ thu·∫≠t ·ªü level n√†y bao g·ªìm**:

- **Network pruning and quantification:** C·∫Øt t·ªâa v√† l∆∞·ª£ng t·ª≠ h√≥a m·∫°ng neuron ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc v√† tƒÉng t·ªëc ƒë·ªô.
- **Lightweight network design:** Thi·∫øt k·∫ø c√°c m·∫°ng neuron g·ªçn nh·∫π, y√™u c·∫ßu √≠t t√†i nguy√™n t√≠nh to√°n.


{{% /details %}}


{{% details title="Speed up of detec. pipeline" closed="true" %}}

ƒê√¢y l√† c·∫•p ƒë·ªô cao nh·∫•t, t·∫≠p trung v√†o vi·ªác tƒÉng t·ªëc to√†n b·ªô quy tr√¨nh ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng (detection pipeline). 

**C√°c k·ªπ thu·∫≠t ·ªü level n√†y bao g·ªìm**:

- **Feat. map shared comput.:** Chia s·∫ª t√≠nh to√°n tr√™n c√°c feature map ƒë·ªÉ tr√°nh t√≠nh to√°n d∆∞ th·ª´a.
- **Cascaded detection:** S·ª≠ d·ª•ng ki·∫øn tr√∫c cascaded ƒë·ªÉ lo·∫°i b·ªè s·ªõm c√°c v√πng kh√¥ng ch·ª©a ƒë·ªëi t∆∞·ª£ng.
{{% /details %}}

{{% /details %}}

![alt text](image-2.png)

{{% details title=" Gi·∫£i th√≠ch t·ª´ kh√≥a Multi-scale trong Object Detection" closed="true" %}}
**Multi-scale** (ƒëa t·ª∑ l·ªá) ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác x·ª≠ l√Ω v√† ph√°t hi·ªán c√°c ƒë·ªëi t∆∞·ª£ng ·ªü nhi·ªÅu k√≠ch th∆∞·ªõc kh√°c nhau trong m·ªôt b·ª©c ·∫£nh. V·∫•n ƒë·ªÅ n√†y ph√°t sinh do trong th·ª±c t·∫ø, c√°c ƒë·ªëi t∆∞·ª£ng c√≥ th·ªÉ xu·∫•t hi·ªán v·ªõi k√≠ch th∆∞·ªõc r·∫•t ƒëa d·∫°ng, t·ª´ nh·ªè b√© (v√≠ d·ª•: con ki·∫øn) ƒë·∫øn c·ª±c l·ªõn (v√≠ d·ª•: t√≤a nh√†).

N·∫øu ch·ªâ s·ª≠ d·ª•ng m·ªôt k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh ƒë·ªÉ x·ª≠ l√Ω ·∫£nh, m√¥ h√¨nh c√≥ th·ªÉ g·∫∑p kh√≥ khƒÉn trong vi·ªác ph√°t hi·ªán c√°c ƒë·ªëi t∆∞·ª£ng c√≥ k√≠ch th∆∞·ªõc qu√° kh√°c bi·ªát so v·ªõi k√≠ch th∆∞·ªõc ƒë√≥. V√≠ d·ª•, n·∫øu m√¥ h√¨nh ch·ªâ ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n c√°c ƒë·ªëi t∆∞·ª£ng l·ªõn, n√≥ c√≥ th·ªÉ b·ªè s√≥t c√°c ƒë·ªëi t∆∞·ª£ng nh·ªè ho·∫∑c ng∆∞·ª£c l·∫°i.

![alt text](image-4.png)

{{% /details %}}


### 1.3. Multi-scale trong Object Detection

{{% details title="Gi·∫£i quy·∫øt Mutli Scale" closed="true" %}}
M·ª•c ti√™u l√† l√†m sao cho m√¥ h√¨nh c√≥ th·ªÉ ‚Äúnh√¨n th·∫•y‚Äù h√¨nh ·∫£nh ·ªü nhi·ªÅu m·ª©c ƒë·ªô chi ti·∫øt, t·ªâ l·ªá kh√°c nhau. 

C√≥ hai c√°ch ti·∫øp c·∫≠n ch√≠nh:

+ **Image pyramid:** Chia h√¨nh ·∫£nh ƒë·∫ßu v√†o th√†nh nhi·ªÅu phi√™n b·∫£n v·ªõi k√≠ch th∆∞·ªõc kh√°c nhau (t·∫°o th√†nh m·ªôt kim t·ª± th√°p ·∫£nh), sau ƒë√≥ x·ª≠ l√Ω t·ª´ng phi√™n b·∫£n ƒë·ªôc l·∫≠p.

![alt text](image-5.png)

Ngu·ªìn: [link](https://huytranvan2010.github.io/Image-pyramid-with-Python/)

+ **Feature pyramid:**  T·∫°o ra m·ªôt kim t·ª± th√°p c√°c feature map (b·∫£n ƒë·ªì ƒë·∫∑c tr∆∞ng) t·ª´ c√°c t·∫ßng kh√°c nhau c·ªßa m·∫°ng CNN. C√°c feature map ·ªü t·∫ßng th·∫•p ch·ª©a th√¥ng tin chi ti·∫øt v·ªÅ c√°c ƒë·ªëi t∆∞·ª£ng nh·ªè, trong khi c√°c feature map ·ªü t·∫ßng cao ch·ª©a th√¥ng tin ng·ªØ nghƒ©a v·ªÅ c√°c ƒë·ªëi t∆∞·ª£ng l·ªõn.

![alt text](image-6.png)

| Ti√™u ch√≠ | Image Pyramid | Feature Pyramid |
|----------|---------------|-----------------|
| Nguy√™n l√Ω | X·ª≠ l√Ω nhi·ªÅu phi√™n b·∫£n thu nh·ªè c·ªßa ·∫£nh g·ªëc | T·∫≠n d·ª•ng feature map t·ª´ c√°c t·∫ßng c·ªßa m·∫°ng, h·ªçc th√¥ng tin tr·ª´u t∆∞·ª£ng |
| ∆Øu ƒëi·ªÉm | D·ªÖ hi·ªÉu, hi·ªáu qu·∫£ v·ªõi ƒë·ªëi t∆∞·ª£ng nh·ªè | Hi·ªáu qu·∫£ cao, ti·∫øt ki·ªám t√†i nguy√™n |
| Nh∆∞·ª£c ƒëi·ªÉm | T·ªën k√©m t√†i nguy√™n, tƒÉng th·ªùi gian suy lu·∫≠n | Kh√≥ tri·ªÉn khai, hi·ªáu qu·∫£ ph·ª• thu·ªôc ki·∫øn tr√∫c m·∫°ng |
| T√≠nh interpretable | Cao, d·ªÖ d√†ng h√¨nh dung th√¥ng tin ·ªü m·ªói t·∫ßng do kh√¥ng bi·∫øn ƒë·ªïi input | Th·∫•p, kh√≥ hi·ªÉu √Ω nghƒ©a c·ªßa feature map |

>[!NOTE]
>Feature pyramid h·ªçc ƒë∆∞·ª£c th√¥ng tin tr·ª´u t∆∞·ª£ng h∆°n, gi√∫p m√¥ h√¨nh "hi·ªÉu" ·∫£nh s√¢u h∆°n v√† cho k·∫øt qu·∫£ ch√≠nh x√°c h∆°n, nh∆∞ng ƒë√°nh ƒë·ªïi b·∫±ng vi·ªác gi·∫£m t√≠nh interpretable. Ng∆∞·ª£c l·∫°i, Image pyramid d·ªÖ hi·ªÉu v√† di·ªÖn gi·∫£i h∆°n, nh∆∞ng k√©m hi·ªáu qu·∫£ v·ªÅ m·∫∑t t√†i nguy√™n.

{{% /details %}}

{{% details title="Evoluation of Multi-scale Detection" closed="true" %}}
![alt text](image-7.png)


{{% details title="Feature Pyramids and Sliding Windows (2001 - 2013)" closed="true" %}}
- ƒê√¢y l√† giai ƒëo·∫°n ƒë·∫ßu c·ªßa **Multi-scale Detection**, s·ª≠ d·ª•ng **Feature pyramid** k·∫øt h·ª£p v·ªõi **Sliding windows** ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng ·ªü nhi·ªÅu t·ª∑ l·ªá kh√°c nhau.
- M·ªôt s·ªë ph∆∞∆°ng ph√°p ti√™u bi·ªÉu trong giai ƒëo·∫°n n√†y:
 - **VJ Det. (2001):** S·ª≠ d·ª•ng Haar-like features v√† AdaBoost ƒë·ªÉ ph√°t hi·ªán khu√¥n m·∫∑t.
 - **HOG Det. (2005):** S·ª≠ d·ª•ng Histogram of Oriented Gradients (HOG) ƒë·ªÉ ph√°t hi·ªán ng∆∞·ªùi ƒëi b·ªô.
 - **DPM (2008):** S·ª≠ d·ª•ng Deformable Part Model (DPM) ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng.

{{% /details %}}


{{% details title="Detection with Object Proposals (2014 - 2015)" closed="true" %}}

Giai ƒëo·∫°n n√†y ƒë√°nh d·∫•u s·ª± chuy·ªÉn d·ªãch sang s·ª≠ d·ª•ng **Object Proposals** (ƒë·ªÅ xu·∫•t ƒë·ªëi t∆∞·ª£ng) ƒë·ªÉ gi·∫£m kh√¥ng gian t√¨m ki·∫øm v√† tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω.

C√°c ph∆∞∆°ng ph√°p ti√™u bi·ªÉu:

- **RCNN (2014):** S·ª≠ d·ª•ng **Selective Search** ƒë·ªÉ t·∫°o v√πng ƒë·ªÅ xu·∫•t, sau ƒë√≥ s·ª≠ d·ª•ng **CNN ƒë·ªÉ ph√¢n lo·∫°i**.
- **SPPNet (2014):** Gi·ªõi thi·ªáu **Spatial Pyramid Pooling** ƒë·ªÉ x·ª≠ l√Ω c√°c v√πng ƒë·ªÅ xu·∫•t c√≥ k√≠ch th∆∞·ªõc kh√°c nhau.
- **Fast RCNN (2015):** C·∫£i thi·ªán **t·ªëc ƒë·ªô x·ª≠ l√Ω c·ªßa RCNN**.
- **Faster RCNN (2015):** S·ª≠ d·ª•ng **Region Proposal Network (RPN)** ƒë·ªÉ t·∫°o v√πng ƒë·ªÅ xu·∫•t, tƒÉng t·ªëc ƒë·ªô ƒë√°ng k·ªÉ.
{{% /details %}}


{{% details title="Anchor-free detection (2018 - 2020)" closed="true" %}}
Giai ƒëo·∫°n n√†y t·∫≠p trung v√†o c√°c ph∆∞∆°ng ph√°p **Anchor-free**, **kh√¥ng** s·ª≠ d·ª•ng **anchor boxes** ƒë·ªÉ d·ª± ƒëo√°n v·ªã tr√≠ ƒë·ªëi t∆∞·ª£ng.

M·ªôt s·ªë ph∆∞∆°ng ph√°p ti√™u bi·ªÉu:

- **CornerNet (2018):** Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng b·∫±ng c√°ch d·ª± ƒëo√°n c√°c g√≥c c·ªßa bounding box.
- **CenterNet (2019):** Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng b·∫±ng c√°ch d·ª± ƒëo√°n t√¢m c·ªßa bounding box.
- **Reppoints (2019):** S·ª≠ d·ª•ng c√°c ƒëi·ªÉm ƒë·∫°i di·ªán ƒë·ªÉ bi·ªÉu di·ªÖn ƒë·ªëi t∆∞·ª£ng.

{{% /details %}}

{{% details title="Multi-reference Detection (2016 - 2020)" closed="true" %}}
Giai ƒëo·∫°n n√†y t·∫≠p trung v√†o vi·ªác s·ª≠ d·ª•ng nhi·ªÅu **reference boxes** (h·ªôp tham chi·∫øu) ƒë·ªÉ d·ª± ƒëo√°n **bounding box** ch√≠nh x√°c h∆°n.

C√°c ph∆∞∆°ng ph√°p ti√™u bi·ªÉu:

- **SSD (2016):** S·ª≠ d·ª•ng nhi·ªÅu anchor boxes v·ªõi c√°c t·ª∑ l·ªá v√† k√≠ch th∆∞·ªõc kh√°c nhau.
- **FPN (2017):** K·∫øt h·ª£p feature map t·ª´ c√°c t·∫ßng kh√°c nhau ƒë·ªÉ t·∫°o **feature pyramid**.
- **RetinaNet (2017):** S·ª≠ d·ª•ng **focal loss** ƒë·ªÉ gi·∫£i quy·∫øt m·∫•t c√¢n b·∫±ng l·ªõp.

{{% /details %}}

{{% details title="Multi-resolution Detection (2016 - 2021)" closed="true" %}}
Giai ƒëo·∫°n n√†y t·∫≠p trung v√†o vi·ªác x·ª≠ l√Ω h√¨nh ·∫£nh ·ªü nhi·ªÅu ƒë·ªô ph√¢n gi·∫£i kh√°c nhau ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng ·ªü nhi·ªÅu t·ª∑ l·ªá.

C√°c ph∆∞∆°ng ph√°p ti√™u bi·ªÉu:

- **SSD (2016):** S·ª≠ d·ª•ng feature map t·ª´ c√°c t·∫ßng kh√°c nhau ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng ·ªü nhi·ªÅu t·ª∑ l·ªá.
- **YOLOv4 (2020):** S·ª≠ d·ª•ng k·ªπ thu·∫≠t multi-scale training ƒë·ªÉ tƒÉng c∆∞·ªùng kh·∫£ nƒÉng ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng.
- **Swin Transformer (2021):** Ki·∫øn tr√∫c Transformer ƒë∆∞·ª£c thi·∫øt k·∫ø cho th·ªã gi√°c m√°y t√≠nh, ƒë·∫°t hi·ªáu su·∫•t cao trong Object Detection.

{{% /details %}}

{{% /details %}}


{{% details title="Object Proposals" closed="true" %}}

Thay v√¨ ph·∫£i xem x√©t t·∫•t c·∫£ c√°c v√πng c√≥ th·ªÉ c√≥ trong ·∫£nh ƒë·ªÉ t√¨m ki·∫øm ƒë·ªëi t∆∞·ª£ng, **Object Proposals** s·∫Ω t·∫°o ra m·ªôt t·∫≠p h·ª£p c√°c v√πng *"ti·ªÅm nƒÉng" (candidate regions)* c√≥ kh·∫£ nƒÉng cao ch·ª©a ƒë·ªëi t∆∞·ª£ng. C√°c v√πng n√†y ƒë∆∞·ª£c g·ªçi l√† "object proposals" hay "region proposals".

![alt text](image-8.png 'https://www.koen.me/research/pub/vandesande-iccv2011-poster.pdf')

ƒêi·ªÅu n√†y gi√∫p:

+ Gi·∫£m kh√¥ng gian t√¨m ki·∫øm, b·ªè qua c√°c v√πng kh√¥ng gian background.
+ TƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω, v√¨ ch·ªâ c·∫ßn xem x√©t c√°c v√πng ƒë·ªÅ xu·∫•t thay v√¨ to√†n b·ªô ·∫£nh nh∆∞ sliding windows.

{{% details title="C√°c ph∆∞∆°ng ph√°p Object Detections ph·ªï bi·∫øn" closed="true" %}}

+ **Selective Search:** D·ª±a tr√™n c√°c ƒë·∫∑c tr∆∞ng m√†u s·∫Øc, k·∫øt c·∫•u, v√† k√≠ch th∆∞·ªõc ƒë·ªÉ nh√≥m c√°c pixel l·∫°i v·ªõi nhau v√† t·∫°o th√†nh c√°c v√πng ƒë·ªÅ xu·∫•t.

![alt text](image-9.png 'https://www.koen.me/research/pub/vandesande-iccv2011-poster.pdf')

![alt text](image-10.png 'https://www.koen.me/research/pub/vandesande-iccv2011-poster.pdf')

+ **EdgeBoxes**: S·ª≠ d·ª•ng c√°c c·∫°nh trong ·∫£nh ƒë·ªÉ t·∫°o v√πng ƒë·ªÅ xu·∫•t -> s·ª≠ d·ª•ng c√°c features nh∆∞ sobel ƒë·ªÉ detect c·∫°nh ph·∫ßn n√†o nhi·ªÅu c·∫°nh xu·∫•t hi·ªán ph·∫ßn ƒë√≥ s·∫Ω l√† v√πng c·∫ßn ƒë∆∞·ª£c detect.

+ **Region Proposal Network (RPN)**: M·ªôt th√†nh ph·∫ßn trong ki·∫øn tr√∫c **Faster R-CNN**, s·ª≠ d·ª•ng m·∫°ng n∆°-ron ƒë·ªÉ t·∫°o v√πng ƒë·ªÅ xu·∫•t -> s·ª≠ d·ª•ng m·ªôt m·∫°ng n∆° ron ƒë·ªÉ h·ªçc v√† detect.

{{% /details %}}


{{% /details %}}


{{% details title="Anchor box" closed="true" %}}
Anchor box l√† c√°c h·ªôp c√≥ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh v√† t·ª∑ l·ªá c·ªë ƒë·ªãnh ƒë·∫∑t s·∫µn trong ·∫£nh, ƒë√≥ng vai tr√≤ nh∆∞ ƒëi·ªÉm tham chi·∫øu ƒë·ªÉ m√¥ h√¨nh d·ª± ƒëo√°n bounding box c·ªßa ƒë·ªëi t∆∞·ª£ng.

![alt text](image-11.png 'https://www.researchgate.net/figure/Anchor-boxes-are-used-to-predict-the-face-area-inside-the-bounding-box-of-YOLO-algorithm_fig4_331975215')

{{% /details %}}

{{% details title="S·ª± kh√°c bi·ªát gi·ªØa Anchor box v√† Region box" closed="true" %}}

![alt text](image-12.png 'https://www.thinkautonomous.ai/blog/anchor-boxes/')

Khi th·ª±c hi·ªán object detection, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng region proposal ƒë·ªÉ khoanh v√πng tr∆∞·ªõc khu v·ª±c c√≥ kh·∫£ nƒÉng cao ch·ª©a ƒë·ªëi t∆∞·ª£ng trong ·∫£nh. C√°c thu·∫≠t to√°n nh∆∞ Selective Search hay Region Proposal Network th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ t·∫°o ra c√°c v√πng ƒë·ªÅ xu·∫•t n√†y.

Sau ƒë√≥, t·∫°i m·ªói v√πng ƒë·ªÅ xu·∫•t, ch√∫ng ta s·∫Ω ƒë·∫∑t m·ªôt s·ªë l∆∞·ª£ng anchor box v·ªõi k√≠ch th∆∞·ªõc v√† t·ª∑ l·ªá c·ªë ƒë·ªãnh kh√°c nhau. C√°c anchor box n√†y ƒë√≥ng vai tr√≤ nh∆∞ "m·∫´u" ƒë·ªÉ m√¥ h√¨nh d·ª± ƒëo√°n bounding box c·ªßa ƒë·ªëi t∆∞·ª£ng b·∫±ng c√°ch d·ª± ƒëo√°n ƒë·ªô l·ªách t·ª´ anchor box ƒë·∫øn v·ªã tr√≠ th·ª±c t·∫ø c·ªßa ƒë·ªëi t∆∞·ª£ng. 

Tuy nhi√™n, c·∫ßn l∆∞u √Ω r·∫±ng kh√¥ng ph·∫£i m√¥ h√¨nh object detection n√†o c≈©ng s·ª≠ d·ª•ng c·∫£ region proposal v√† anchor box.


{{< callout type="error" >}}
V·∫≠y th√¨ c√¢u h·ªèi ƒë·∫∑t ra ·ªü ƒë√¢y l√† bao nhi√™u region proposal l√† ƒë·ªß t·ªët?
C√°c ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng c√≥ th·ªÉ tr·∫£ v·ªÅ l√™n t·ªõi v√†i trƒÉm, v√†i ng√†n region proposal kh√°c nhau.
ƒê√¢y c≈©ng ch√≠nh l√† khuy·∫øt ƒëi·ªÉm l·ªõn v√† l√† motivation cho h∆∞·ªõng Anchor-free.
{{< /callout >}}

.

---

**H·∫°n ch·∫ø c·ªßa Anchor box:**

- **Nhi·ªÅu si√™u tham s·ªë c·∫ßn ƒëi·ªÅu ch·ªânh:** C·∫ßn ph·∫£i x√°c ƒë·ªãnh **s·ªë l∆∞·ª£ng**, **k√≠ch th∆∞·ªõc**, **t·ª∑ l·ªá c·ªßa anchor box** sao cho ph√π h·ª£p v·ªõi t·∫≠p d·ªØ li·ªáu.
- **T√≠nh to√°n ph·ª©c t·∫°p:** C·∫ßn ph·∫£i t√≠nh to√°n **IoU (Intersection over Union)** gi·ªØa anchor box v√† ground-truth box cho m·ªói v·ªã tr√≠ tr√™n feature map.
- **M·∫•t c√¢n b·∫±ng gi·ªØa positive v√† negative anchor boxes:** Th∆∞·ªùng c√≥ **r·∫•t nhi·ªÅu anchor box kh√¥ng ch·ª©a ƒë·ªëi t∆∞·ª£ng (negative),** d·∫´n ƒë·∫øn m·∫•t c√¢n b·∫±ng trong qu√° tr√¨nh hu·∫•n luy·ªán.

{{% /details %}}


{{% details title="Achor-free" closed="true" %}}

Thay v√¨ d·ª±a v√†o anchor box, c√°c ph∆∞∆°ng ph√°p **anchor-free** d·ª± ƒëo√°n **tr·ª±c ti·∫øp v·ªã tr√≠** v√† **k√≠ch th∆∞·ªõc** c·ªßa ƒë·ªëi t∆∞·ª£ng d·ª±a tr√™n c√°c ƒë·∫∑c tr∆∞ng c·ªßa feature map.

M·ªôt s·ªë thu·∫≠t to√°n anchor-free ph·ªï bi·∫øn:

- **CornerNet:** Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng b·∫±ng c√°ch d·ª± ƒëo√°n hai g√≥c c·ªßa bounding box (g√≥c tr√™n b√™n tr√°i v√† g√≥c d∆∞·ªõi b√™n ph·∫£i).
- **CenterNet:** Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng b·∫±ng c√°ch d·ª± ƒëo√°n t√¢m c·ªßa bounding box v√† k√≠ch th∆∞·ªõc c·ªßa ƒë·ªëi t∆∞·ª£ng.
- **FCOS (Fully Convolutional One-Stage Object Detection):** D·ª± ƒëo√°n tr·ª±c ti·∫øp bounding box cho m·ªói ƒëi·ªÉm tr√™n feature map.
- **RepPoints:** S·ª≠ d·ª•ng m·ªôt t·∫≠p h·ª£p c√°c ƒëi·ªÉm ƒë·∫°i di·ªán ƒë·ªÉ bi·ªÉu di·ªÖn ƒë·ªëi t∆∞·ª£ng, sau ƒë√≥ d·ª± ƒëo√°n bounding box d·ª±a tr√™n c√°c ƒëi·ªÉm n√†y.

{{% details title="Anchor-free c√≥ s·ª≠ d·ª•ng region proposal kh√¥ng?" closed="true" %}}
C√¢u tr·∫£ l·ªùi l√† c√≥, c√≥ m·ªôt s·ªë ph∆∞∆°ng ph√°p anchor-free s·ª≠ d·ª•ng region proposal, nh∆∞ng kh√¥ng ph·∫£i l√† t·∫•t c·∫£.

ƒê·ªÉ hi·ªÉu r√µ ta c·∫ßn ph√¢n bi·ªát hai kh√°i ni·ªám sau:

+ Region proposal: l√† b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω nh·∫±m t·∫°o ra c√°c v√πng c√≥ ti·ªÅm nƒÉng ch·ª© ƒë·ªëi t∆∞·ª£ng gi√∫p gi·∫£m kh√¥ng gian t√¨m ki·∫øm cho m√¥ h√¨nh.
+ Anchor box: l√† c√°c h·ªôp c√≥ k√≠ch th∆∞·ªõc v√† t·ª∑ l·ªá c·ªë ƒë·ªãnh, ƒë∆∞·ª£c ƒë·∫∑t tr√™n ·∫£nh ƒë·ªÉ l√†m ƒëi·ªÉm tham chi·∫øu cho vi·ªác d·ª± ƒëo√°n bounding box.

Anchor-free b·∫£n ch·∫•t l√† lo·∫°i b·ªè vi·ªác s·ª≠ d·ª•ng anchor box nh∆∞ng kh√¥ng nh·∫•t thi·∫øt ph·∫£i lo·∫°i b·ªè region proposal.
V√≠ d·ª•:

+ **FCOS (Fully Convolutional One-Stage Object Detection)**: ƒê√¢y l√† m·ªôt ph∆∞∆°ng ph√°p anchor-free kh√¥ng s·ª≠ d·ª•ng region proposal. FCOS d·ª± ƒëo√°n tr·ª±c ti·∫øp bounding box cho m·ªói ƒëi·ªÉm tr√™n feature map.

+ **RepPoints**: T∆∞∆°ng t·ª± **FCOS**, **RepPoints** c≈©ng l√† m·ªôt ph∆∞∆°ng ph√°p anchor-free kh√¥ng s·ª≠ d·ª•ng region proposal. **RepPoints** s·ª≠ d·ª•ng m·ªôt t·∫≠p h·ª£p c√°c ƒëi·ªÉm ƒë·∫°i di·ªán ƒë·ªÉ bi·ªÉu di·ªÖn ƒë·ªëi t∆∞·ª£ng.

+ **CornerNet**: M·∫∑c d√π l√† anchor-free, **CornerNet** v·∫´n s·ª≠ d·ª•ng m·ªôt h√¨nh th·ª©c region proposal. N√≥ s·ª≠ d·ª•ng **corner pooling** ƒë·ªÉ t·∫°o ra c√°c "**heatmaps**" (b·∫£n ƒë·ªì nhi·ªát) bi·ªÉu di·ªÖn kh·∫£ nƒÉng xu·∫•t hi·ªán g√≥c c·ªßa ƒë·ªëi t∆∞·ª£ng t·∫°i m·ªói v·ªã tr√≠ tr√™n ·∫£nh. C√°c ƒëi·ªÉm c√≥ **gi√° tr·ªã cao** tr√™n **heatmap** c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† **m·ªôt d·∫°ng region proposal**.

![alt text](image-13.png 'https://arxiv.org/pdf/1904.08189v3)


{{< callout >}}
  T√≥m l·∫°i:

  + **Anchor-free** t·∫≠p trung v√†o vi·ªác lo·∫°i b·ªè **anchor box**, **kh√¥ng ph·∫£i region proposal**.
  + M·ªôt s·ªë ph∆∞∆°ng ph√°p **anchor-free** c√≥ th·ªÉ s·ª≠ d·ª•ng **region proposal** ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t ho·∫∑c ƒë∆°n gi·∫£n h√≥a qu√° tr√¨nh hu·∫•n luy·ªán.
  
  Vi·ªác s·ª≠ d·ª•ng hay kh√¥ng s·ª≠ d·ª•ng region proposal trong anchor-free ph·ª• thu·ªôc v√†o thi·∫øt k·∫ø c·ª• th·ªÉ c·ªßa t·ª´ng ph∆∞∆°ng ph√°p.
{{< /callout >}}

{{% /details %}}

{{% /details %}}


## 2. Evoluation of Context Priming in Object Detection

![alt text](image-14.png 'https://arxiv.org/abs/1905.05055')

{{% details title="Context Priming l√† g√¨?" closed="true" %}}

**Context Priming** trong **Object Detection** l√† m·ªôt k·ªπ thu·∫≠t t·∫≠n d·ª•ng th√¥ng tin ng·ªØ c·∫£nh (contextual information) trong ·∫£nh ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa vi·ªác ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng. √ù t∆∞·ªüng ch√≠nh l√† c√°c **ƒë·ªëi t∆∞·ª£ng th∆∞·ªùng xu·∫•t hi·ªán trong nh·ªØng ng·ªØ c·∫£nh nh·∫•t ƒë·ªãnh**, v√† vi·ªác **hi·ªÉu ƒë∆∞·ª£c ng·ªØ c·∫£nh n√†y c√≥ th·ªÉ gi√∫p m√¥ h√¨nh d·ª± ƒëo√°n ch√≠nh x√°c h∆°n**.

V√≠ d·ª•, n·∫øu ta th·∫•y m·ªôt c√°i b√†n, ta c√≥ th·ªÉ mong ƒë·ª£i s·∫Ω th·∫•y c√°c ƒë·ªëi t∆∞·ª£ng li√™n quan nh∆∞ gh·∫ø, m√°y t√≠nh, s√°ch v·ªü,... ·ªü g·∫ßn ƒë√≥. Ho·∫∑c n·∫øu ta th·∫•y m·ªôt ng∆∞·ªùi ƒëang c·∫ßm v·ª£t tennis, ta c√≥ th·ªÉ suy ra r·∫±ng c√≥ kh·∫£ nƒÉng cao s·∫Ω c√≥ m·ªôt qu·∫£ b√≥ng tennis ho·∫∑c m·ªôt s√¢n tennis trong ·∫£nh.

L∆∞u √Ω r·∫±ng ta ch·ªâ l·∫•y th√™m 1 ph·∫ßn nh·ªè xung quanh ƒë·ªëi t∆∞·ª£ng m·ªõi c·∫£i thi·ªán ƒë∆∞·ª£c performance, kh√¥ng l·∫•y qu√° nhi·ªÅu ‚Üí **local context**.


{{% /details %}}


{{% details title="Global context l√† g√¨?" closed="true" %}}

Global Context l√† vi·ªác n·∫Øm b·∫Øt th√¥ng tin to√†n b·ªô b·ª©c ·∫£nh.

CNN truy·ªÅn th·ªëng ch·ªß y·∫øu t·∫≠p trung v√†o local context, nh∆∞ng c√≥ th·ªÉ k·∫øt h·ª£p v·ªõi c√°c k·ªπ thu·∫≠t nh∆∞ *Global Average Pooling (GAP)* ho·∫∑c *Attention mechanism* ƒë·ªÉ l·∫•y th√¥ng tin to√†n c·ª•c t·ª´ ·∫£nh.
{{% /details %}}


{{% details title="Context Interactives l√† g√¨?" closed="true" %}}
Thay v√¨ ch·ªâ xem x√©t ng·ªØ c·∫£nh nh∆∞ m·ªôt y·∫øu t·ªë b·ªï sung, Context Interactives t·∫≠p trung v√†o vi·ªác ph√¢n t√≠ch v√† khai th√°c c√°c m·ªëi quan h·ªá ph·ª©c t·∫°p gi·ªØa c√°c ƒë·ªëi t∆∞·ª£ng v√† m√¥i tr∆∞·ªùng xung quanh.

{{% /details %}}


## 3. Evoluation of Hard Negative Mining

![alt text](image-15.png 'https://arxiv.org/abs/1905.05055')

{{% details title="Hard Negative Mining l√† g√¨" closed="true" %}}
Hard Negative Mining l√† m·ªôt k·ªπ thu·∫≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng trong Object Detection ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh b·∫±ng c√°ch t·∫≠p trung v√†o nh·ªØng hard negative samples.

Negative samples l√† nh·ªØng v√πng trong ·∫£nh kh√¥ng ch·ª©a ƒë·ªëi t∆∞·ª£ng m√† ch√∫ng ta quan t√¢m, trong qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh object detection, ch√∫ng ta c·∫ßn cung c·∫•p cho m√¥ h√¨nh c·∫£ Positive samples (ch·ª©a ƒë·ªëi t∆∞·ª£ng) v√† Negative samples (kh√¥ng ch·ª©a ƒë·ªëi t∆∞·ª£ng ) ƒë·ªÉ m√¥ h√¨nh ph√¢n bi·ªát n·∫øu kh√¥ng th√¨ m√¥ h√¨nh s·∫Ω r·∫•t d·ªÖ b·ªã bias c≈©ng nh∆∞ kh√¥gn d·ª± ƒëo√°n t·ªët ƒë∆∞·ª£c class b·ªã imbalanced (trainning tr√™n s·ªë l∆∞·ª£ng √≠t h∆°n nhi·ªÅu so v·ªõi c√°c class kh√°c)

![alt text](image-16.png 'https://arxiv.org/html/2209.00078v2')

{{% /details %}}


{{% details title="**Bootstrap** (1994 - 2008)" closed="true" %}}
Giai ƒëo·∫°n ƒë·∫ßu, **Bootstrap** ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i ƒë·ªÉ x·ª≠ l√Ω c√°c v·∫•n ƒë·ªÅ v·ªÅ h·∫°n ch·∫ø t√†i nguy√™n t√≠nh to√°n.

+ **√ù t∆∞·ªüng**: Bootstrap l√† m·ªôt k·ªπ thu·∫≠t t·ªëi ∆∞u trong l·∫•y m·∫´u, trong ƒë√≥ ta l·∫•y m·∫´u ng·∫´u nhi√™n c√≥ th·ªÉ l·∫∑p l·∫°i t·ª´ t·∫≠p d·ªØ li·ªáu g·ªëc ƒë·ªÉ t·∫°o t·∫≠p d·ªØ li·ªáu m·ªõi (gi·ªëng bootstrap trong decision tree nh·ªâ).

+ **·ª®ng d·ª•ng trong Hard Negative Mining**: Bootstrap ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o ra c√°c t·∫≠p hu·∫•n luy·ªán ch·ª©a nhi·ªÅu m·∫´u negative kh√≥ h∆°n, gi√∫p c·∫£i thi·ªán hi·ªáu qu·∫£ hu·∫•n luy·ªán khi t√†i nguy√™n t√≠nh to√°n c√≤n h·∫°n ch·∫ø.

+ M·ªôt s·ªë ph∆∞∆°ng ph√°p ti√™u bi·ªÉu:
  + Face Det. (H. A. Rowley et al-CMUTechRep1995)
  + Haar Det. (C. P. Papageorgiou et al-ICCV1998)
  + VJ Det. (P. Viola et al-CVPR2001)
  + HOG Det. (N. Dalal et al-CVPR2005)
  + DPM (P. Felzenszwalb et al-CVPR2008, Œ§Œ°ŒëŒú12010)
{{% /details %}}

{{% details title="Without Hard Negative Mining (2014 - 2015):" closed="true" %}}
Giai ƒëo·∫°n n√†y, c√°c ph∆∞∆°ng ph√°p **kh√¥ng t·∫≠p trung **v√†o **Hard Negative Mining** m√† ch·ªß y·∫øu **c√¢n b·∫±ng tr·ªçng s·ªë gi·ªØa c√°c l·ªõp ƒë·ªëi t∆∞·ª£ng v√† l·ªõp n·ªÅn**.

+ √ù t∆∞·ªüng: thay v√¨ t√¨m ki·∫øm c√°c m·∫´u √¢m kh√≥, c√°c ph∆∞∆°ng ph√°p t·∫≠p trung v√†o vi·ªác ƒëi·ªÅu ch·ªânh tr·ªçng s·ªë (weight) c·ªßa c√°c l·ªõp trong h√†m loss, gi√∫p c√¢n b·∫±ng gi·ªØa m·∫´u negative v√† positive.  

- **M·ªôt s·ªë ph∆∞∆°ng ph√°p ti√™u bi·ªÉu:**
    - RCNN (R. Girshick et al-CVPR2014)
    - SPPNet (K. He et al-ECCV2014)
    - Fast RCNN (R. Girshick-ICCV2015)
    - Faster RCNN (S. Ren et al-NIPS2015)
    - YOLO (J. Redmon et al-CVPR2016)
{{% /details %}}


{{% details title="Bootstrap + New Loss Functions (2016 - 2021)" closed="true" %}}
Giai ƒëo·∫°n n√†y ƒë√°nh d·∫•u s·ª± tr·ªü l·∫°i c·ªßa **Bootstrap**, k·∫øt h·ª£p v·ªõi c√°c **h√†m loss m·ªõi**, t·∫≠p trung v√†o vi·ªác khai th√°c c√°c m·∫´u kh√≥.

- **√ù t∆∞·ªüng:** S·ª≠ d·ª•ng Bootstrap ƒë·ªÉ t·∫°o ra c√°c t·∫≠p hu·∫•n luy·ªán ch·ª©a nhi·ªÅu m·∫´u √¢m kh√≥, ƒë·ªìng th·ªùi s·ª≠ d·ª•ng c√°c h√†m loss m·ªõi ƒë·ªÉ t·∫≠p trung v√†o vi·ªác hu·∫•n luy·ªán tr√™n c√°c m·∫´u n√†y.
- **M·ªôt s·ªë ph∆∞∆°ng ph√°p ti√™u bi·ªÉu:**
    - SSD (W. Liu et al-ECCV2016)
    - FasterPed (L. Zhang et al-ECCV2016)
    - OHEM (A. Shrivastava et al-CVPR2016)
    - RetinaNet (T. Y. Lin et al-ICCV2017)
    - RefineDet (Zhang et al-CVPR18)
    - FCOS (Z. Tian et al-ICCV2019)
    - YOLOv4 (A. Bochkovskiy et al-arXiv2020)

{{% /details %}}


## 4. Evoluation of Non-max suppression.

![alt text](image-17.png)

{{% details title="Non-maximum Suppression" closed="true" %}}
Non-maximum suppression (NMS) l√† m·ªôt k·ªπ thu·∫≠t post-processing ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ lo·∫°i b·ªè c√°c bounding box tr√πng l·∫∑p v√† ch·ªâ gi·ªØ l·∫°i c√°c bounding box t·ªët c√≥ ƒë·ªô tin c·∫≠y cao nh·∫•t.

![alt text](image-18.png 'https://thepythoncode.com/article/non-maximum-suppression-using-opencv-in-python')

{{% /details %}}

{{% details title="Greedy Selection" closed="true" %}}
ƒê√¢y l√† ph∆∞∆°ng ph√°p NMS truy·ªÅn th·ªëng s·ª≠ d·ª•ng thu·∫≠t to√°n tham lam (greedy algorithm) ƒë·ªÉ lo·∫°i b·ªè c√°c bounding box tr√πng l·∫∑p.

**√ù t∆∞·ªüng:** Ch·ªçn bounding box c√≥ ƒë·ªô tin c·∫≠y cao nh·∫•t, sau ƒë√≥ b·ªè c√°c bounding box ch·ªìng ch√©o v·ªõi n√≥. L·∫∑p l·∫°i q√∫a tr√¨nh ƒë·∫øn khi kh√¥ng c√≤n bounding box n√†o.

**H·∫°n ch·∫ø:** C√≥ th·ªÉ lo·∫°i b·ªè nh·∫ßm c√°c bouding box c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng g·∫ßn nhau.

{{% /details %}}

{{% details title="Bounding box aggregation" closed="true" %}}

Nh√°nh n√†y t·∫≠p trung v√†o vi·ªác t·ªïng h·ª£p c√°c bounding box ch·ªìng ch√©o ƒë·ªÉ t·∫°o ra bounding box ch√≠nh x√°c h∆°n.

**√ù t∆∞·ªüng:** Thay v√¨ lo·∫°i b·ªè c√°c bounding box ch·ªìng ch√©o, c√°c ph∆∞∆°ng ph√°p n√†y k·∫øt h·ª£p ch√∫ng l·∫°i ƒë·ªÉ t·∫°o ra bounding box m·ªõi c√≥ v·ªã tr√≠ v√† k√≠ch th∆∞·ªõc ch√≠nh x√°c h∆°n.

**L·ª£i √≠ch:** C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c, ƒë·∫∑c bi·ªát l√† khi c√°c ƒë·ªëi t∆∞·ª£ng g·∫ßn nhau.

{{% /details %}}

{{% details title="Learning to NMS (2016 - 2019)" closed="true" %}}
Nh√°nh n√†y s·ª≠ d·ª•ng h·ªçc m√°y ƒë·ªÉ h·ªçc c√°ch th·ª±c hi·ªán NMS.

**√ù t∆∞·ªüng:** Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh ƒë·ªÉ h·ªçc c√°ch l·ª±a ch·ªçn bounding box t·ªëi ∆∞u, thay v√¨ s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n tham lam c·ªë ƒë·ªãnh.

**L·ª£i √≠ch:** C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c v√† kh·∫£ nƒÉng th√≠ch ·ª©ng v·ªõi c√°c t·∫≠p d·ªØ li·ªáu kh√°c nhau.

{{% /details %}}

{{% details title="NMS-free Detector (2017 - 2021)" closed="true" %}}

Nh√°nh n√†y ph√°t tri·ªÉn c√°c m√¥ h√¨nh Object Detection kh√¥ng c·∫ßn s·ª≠ d·ª•ng NMS.

**√ù t∆∞·ªüng:** Thi·∫øt k·∫ø ki·∫øn tr√∫c m·∫°ng v√† h√†m loss ƒë·ªÉ m√¥ h√¨nh t·ª± ƒë·ªông d·ª± ƒëo√°n c√°c bounding box kh√¥ng tr√πng l·∫∑p.

**L·ª£i √≠ch:** ƒê∆°n gi·∫£n h√≥a quy tr√¨nh, gi·∫£m th·ªùi gian x·ª≠ l√Ω.
{{% /details %}}


## 5. Overview Object Detection.

### 5.1. Overview YOLO

![alt text](image-19.png)

![alt text](image-21.png 'YOLO with transformers')


{{% details title="B·∫£ng t·ªïng h·ª£p Lightweight object detection models" closed="true" %}}

| **S. No.** | **Authors** | **Detector** | **Type** | **Input image** | **Published in** | **URL link** |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | Li et al. ([2017](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | YOLOv3-DarkNet53 | Anchor-based | 320*320 | arXiv 2018 | [https://github.com/westerndigitalcorporation/YOLOv3-in-PyTorch](https://github.com/westerndigitalcorporation/YOLOv3-in-PyTorch) |
| 2 | Howard et al. ([2017](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | MobileNet-SSD | Anchor-based | 300*300 | arXiv 2017 | [https://github.com/chuanqi305/MobileNet-SSD](https://github.com/chuanqi305/MobileNet-SSD) |
| 3 | Sandler et al. ([2018](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | MobileNetv2-SSDLite | Anchor-based | 320*320 | CVPR 2018 | [https://github.com/tranleanh/mobilenets-ssd-pytorch](https://github.com/tranleanh/mobilenets-ssd-pytorch) |
| 4 | Li et al. ([2018](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | Tiny-DSOD | Anchor-based | 300*300 | arXiv 2018 | [https://github.com/lyxok1/Tiny-DSOD](https://github.com/lyxok1/Tiny-DSOD) |
| 5 | Wang et al. ([2018](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | Pelee | Anchor-based | 304*304 | NeurIPS 2018 | [https://github.com/Robert-JunWang/Pelee](https://github.com/Robert-JunWang/Pelee) |
| 6 | Qin et al. ([2019](https://link.springer.com/article/10.1007/s10462-024-10877-1)), Huang et al. ([2018](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | YOLO-LITE | Anchor-based | 416*416 | ICBD 2018 | [https://github.com/reu2018DL/YOLO-LITE](https://github.com/reu2018DL/YOLO-LITE) |
| 7 | Qin et al. ([2019](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | ThunderNet | Anchor-based | 320*320 | ICCV 2019 | [https://github.com/DayBreak-u/Thundernet_Pytorch](https://github.com/DayBreak-u/Thundernet_Pytorch) |
| 8 | Tan et al. ([2019](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | MnasNet-A1‚Äâ+‚ÄâSSDLite | Anchor-based | 320*320 | CVPR 2019 | [https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet) |
| 9 | Tang et al. ([2020a](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2020b](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | LightDet | Anchor-based | 320*320 | ICASSP 2020 | Not available |
| 10 | Yi et al. ([2019](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | YOLOV3-Tiny | Anchor-based | 416*416 | ICACCS 2020 | [https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg](https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg) |
| 11 | Long et al. ([2020a](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2020b](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | PP‚ÄêYOLO | Anchor-based | 608*608 | CVPR 2020 | [https://github.com/PaddlePaddle/PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) |
| 12 | Long et al. ([2020a](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2020b](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | YOLOv4-Tiny | Anchor-based | 416*416 | arXiv 2020 | [https://github.com/truong2710-cyber/Mask-Detection-YOLOv4-tiny-Kaggle-Dataset](https://github.com/truong2710-cyber/Mask-Detection-YOLOv4-tiny-Kaggle-Dataset) |
| 13 | Tan et al. ([2020](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | EfficientDet | Anchor-based | 512*512 | CVPR 2020 | [https://github.com/xuannianz/EfficientDet](https://github.com/xuannianz/EfficientDet) |
| 14 | Huang et al. ([2021](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | PP‚ÄêYOLOv2 | Anchor-based | 640*640 | arXiv 2021 | [https://github.com/PaddlePaddle/PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) |
| 15 | Ge et al. ([2021](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | YOLOX-Nano | Anchor-free | 416*416 | ICSP 2022 | [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) |
| 16 | Ge et al. ([2021](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | YOLOX-Tiny | Anchor-free | 416*416 | IJCINI 2022 | [https://github.com/TexasInstruments/edgeai-yolox/blob/main/exps/default/yolox_tiny.py](https://github.com/TexasInstruments/edgeai-yolox/blob/main/exps/default/yolox_tiny.py) |
| 17 | Cai et al. ([2021](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | YOLObile | Anchor-based | 320*320 | AAAI 2021 | [https://github.com/nightsnack/YOLObile](https://github.com/nightsnack/YOLObile) |
| 18 | Wang et al. ([2021](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | Scaled YOLOv4 | Anchor-based | 608*608 | CVPR 2021 | [https://github.com/WongKinYiu/ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4) |
| 19 | Wang et al. ([2021](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | Trident YOLO | Anchor-based | 416*416 | Wiley IET | Not available |
| 20 | Li et al. ([2021a](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2021b](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2021c](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | NanoDet | Anchor-free | 320*320 | Journals of Radar | [https://github.com/RangiLyu/nanodet](https://github.com/RangiLyu/nanodet) |
| 21 | Yu et al. ([2021](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | PP-PicoDet | Anchor-free | 416*416 | arXiv 2021 | [https://github.com/PaddlePaddle/PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) |
| 22 | Ding et al. ([2022](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | Slim YOLOv4 | Anchor-free | 416*416 | JRIP | Not available |
| 23 | Liu et al. ([2022a](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2022b](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | Mini YOLO | Anchor-free | 320*320 | Wiley Journal | Not available |
| 24 | Xu et al. ([2022](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | PP-YOLOE-S | Anchor-free | 640*640 | arXiv 2022 | [https://github.com/PaddlePaddle/PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) |
| 25 | Wang et al. ([2022a](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2022b](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2022c](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2022d](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | YOLOv7-X | Anchor-free | 640*640 | arXiv 2022 | [https://github.com/WongKinYiu/yolov7](https://github.com/WongKinYiu/yolov7) |
| 26 | Li et al. ([2022a](https://link.springer.com/article/10.1007/s10462-024-10877-1),¬†[2022b](https://link.springer.com/article/10.1007/s10462-024-10877-1)) | L-DETR | Anchor-free | 800*1333 | IEEE Access | [https://github.com/wangjian123799/L-DETR.git](https://github.com/wangjian123799/L-DETR.git) |

        
| **No.** | **Detector** | **Backbone** | **Loss function** | **AP** | **Proposal** |
| --- | --- | --- | --- | --- | --- |
| 1 | YOLOv3-DarkNet53 | DarkNet53 | Logistic regression | 38.1 | Network structure makes greater use of the GPU, making it faster to evaluate than Darknet-19 |
| 2 | MobileNet-SSD | MobileNet | Smooth L1 loss | 19.3 | Lightweight deep network is constructed using depth-wise separable convolutions |
| 3 | MobileNetv2-SSDLite | MobileNetv2 | Smooth L1 loss | 22.1 | With fewer parameters and less computational complexity, gets competitive accuracy |
| 4 | Tiny-DSOD | DDB-Net | Smooth L1 loss | 23.2 | For resource-constrained uses based on DDB and D-FPN blocks |
| 5 | Pelee | PeleeNet | Smooth L1 loss | 22.4 | Variant of DenseNet, built with conventional convolution |
| 6 | YOLO-LITE | Darknet-53 | L1 loss | 12.2 | A real-time detection model developed to run on portable devices lacking a GPU |
| 7 | ThunderNet | SNet535 | Sigmoid | 28.1 | Embedded context enhancement and spatial attention module |
| 8 | MnasNet-A1‚Äâ+‚ÄâSSDLite | MnasNet-A1 | Smooth L1 loss | 23.0 | Directly measures real-world inference latency by executing the model on edge devices |
| 9 | LightDet | Modified ShuffleV2 | Smooth L1 loss | 24.0 | Introduce an efficient feature-preserving and refinement module |
| 10 | YOLOv3-Tiny | Reduced Darknet-53 | Logistic regression | 16.6 | Lightweight model of YOLOv3, which takes reduced training time |
| 11 | PP‚ÄêYOLO | MobileNetV3 | IoU aware loss | 45.9 | Balanced efficiency, directly applied in real application scenarios |
| 12 | YOLOv4-Tiny | CSP-ResNet | CIoU loss | 28.7 | Reduced parameters, makes it suitable for edge devices |
| 13 | EfficientDet | EfficientNet | Focal loss | 34.6 | Proposed a weighted bi-directional FPN for feature fusion |
| 14 | PP‚ÄêYOLOv2 | ResNet101 | IoU aware loss | 49.5 | Increasing the input size and follow the design of PAN to aggregate the top-down information |
| 15 | YOLOX-Nano | DarkNet53 | GIoU loss | 25.8 | Dynamic label assignment strategy SimOTA |
| 16 | YOLOX-Tiny | CBAM | GIoU loss | 32.8 | Fuses convolutional attention and mixup data enhancement strategy |
| 17 | YOLObile | CSP-DarkNet53 | GIoU loss | 31.6 | Offers mobile acceleration and block-punched pruning with a mobile GPU-CPU collaborative strategy |
| 18 | Scaled YOLOv4 | CSPNet-15 | CIoU loss | 28.7 | Propose a network scaling approach that modifies the width, and resolution of network |
| 19 | TridentYOLO | CSP-RFBs | Focal loss | 40.3 | Propose a trident feature pyramid network |
| 20 | NanoDet | ShuffleNetV2 | Focal loss | 20.6 | Based on visual saliency and perform feature learning on samples added with saliency maps |
| 21 | PP-PicoDet | Enhanced ShuffleNet | GIoU Loss | 30.6 | Improved detection One-Shot NAS pipeline |
| 22 | Slim YOLOv4 | MobileNetV2 | CIoU loss | 29.2 | Efficient network computing methods for convolution |
| 23 | MiniYOLO | DSLightNet | CIoU | 21.4 | Adopted depthwise separable convolution |
| 24 | PP-YOLOE-S | ResNet50-vd | IoU aware loss | 43.1 | Scalable backbone-neck architecture, and refined loss function |
| 25 | YOLOv7-X | RepCSPResNet | Assistant loss | 53.1 | Propose the trainable bag-of-freebies method to enhance accuracy |
| 26 | L-DETR | PP-LCNet | H-sigmoid function | ‚Äì | Embedded group normalization |
        
| **No.** | **Light-weight object detector** | **Backbone** | **FLOPs** | **Inference time (ms)** | **FPS** | **Parameters (M)** | **Real-time applications** |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | YOLOv3-DarkNet53 | DarkNet53 | 1453B | 22 | 171 | ‚Äì | ‚àö |
| 2 | MobileNet-SSD | MobileNet | 1.2G | ‚Äì | 59.3 | 4.31 | * |
| 3 | MobileNetv2-SSDLite | MobileNetv2 | 0.8G | ‚Äì | ‚Äì | 3.38 | * |
| 4 | Tiny-DSOD | DDB-Net | 1.12G | ‚Äì | 105 | 0.95 | * |
| 5 | Pelee | PeleeNet | 1.21B | ‚Äì | 205 | 5.98 | * |
| 6 | YOLO-LITE | Darknet-53 | 0.48G | ‚Äì | 21 | ‚Äì | * |
| 7 | ThunderNet | SNet535 | 0.47 | ‚Äì | 248 | ‚Äì | ‚àö |
| 8 | MnasNet-A1‚Äâ+‚ÄâSSDLite | MnasNet-A1 | 0.8B | 203 | ‚Äì | 4.9 | * |
| 9 | LightDet | Modified ShuffleV2 | 0.50G | ‚Äì | 250 | ‚Äì | ‚àö |
| 10 | YOLOv3-Tiny | Reduced Darknet-53 | 5.56 B | 4.5 | 368 | 8.86 | * |
| 11 | PP‚ÄêYOLO | MobileNetV3 | 1.02G | 10.48 | 73 | 1.08 | ‚àö |
| 12 | YOLOv4-Tiny | CSP-ResNet | ‚Äì | ‚Äì | 371 | 6.06 | ‚àö |
| 13 | EfficientDet | EfficientNet | 2.5B | 10.20 | 98 | 3.9 | ‚àö |
| 14 | PP‚ÄêYOLOv2 | ResNet101 | 0.115G | 14.50 | 68.9 | 1.08 | ‚àö |
| 15 | YOLOX-Nano | DarkNet53 | 1.08G | 19.23 | 90.1 | 0.91 | ‚àö |
| 16 | YOLOX-Tiny | CBAM | 6.48G | 32.77 | ‚Äì | 5.06 | ‚àö |
| 17 | YOLObile | CSP-DarkNet53 | 3.95G | ‚Äì | 17 | 4.59 | ‚àö |
| 18 | Scaled YOLOv4 | CSPNet-15 | 6.3B | ‚Äì | 62 | 53 | ‚àö |
| 19 | TridentYOLO | CSP-RFBs | 5.19B | ‚Äì | 29.3 | ‚Äì | ‚àö |
| 20 | NanoDet | ShuffleNetV2 | 1.2G | 13.35 | ‚Äì | 0.95 | * |
| 21 | PP-PicoDet | Enhanced ShuffleNet | 0.73G | 8.13 | ‚Äì | 0.99 | ‚àö |
| 22 | Slim YOLOv4 | MobileNetV2 | ‚Äì | ‚Äì | 60.19 | ‚Äì | ‚àö |
| 23 | MiniYOLO | DSLightNet | ‚Äì | ‚Äì | ‚Äì | 2.06 | * |
| 24 | PP-YOLOE-S | ResNet50-vd | 110.7G | 12.8 | 208.3 | 52.20 | ‚àö |
| 25 | YOLOv7-X | RepCSPResNet | 189.9G | ‚Äì | 114 | 71.3 | ‚àö |
| 26 | L-DETR | PP-LCNet | ‚Äì | ‚Äì | ‚Äì | ‚Äì | ‚àö |

{{% /details %}}


### 5.2. Overview v·ªÅ Detection Transformer (DETR)

![alt text](image-22.png 'https://arxiv.org/pdf/2306.04670')

### 5.3. Overview v·ªÅ Dataset

![alt text](image-23.png)


![alt text](image-24.png 'https://arxiv.org/abs/1905.05055')


### 5.4. Traditional vs Deeplearning Method

![alt text](image-25.png 'https://www.researchgate.net/figure/Pipeline-comparison-of-traditional-and-deep-learning-approaches-for-object-detection-In_fig3_353596790')


### 5.5. Object Detection Milestiones

{{< callout type="info" >}}
  ![alt text](image-26.png)
{{< /callout >}}

## 6. Sliding windows


{{< callout emoji="üìå" >}}

Ph·∫ßn ti·∫øp theo ch√∫ng ta s·∫Ω ƒëi l·∫°i t·ª´ th·ªùi ƒëi·ªÉm ban ƒë·∫ßu, khi c√≤n s·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng, naive ƒë·ªÉ OD.
ƒê·∫ßu ti√™n s·∫Ω l√† sliding window, gi·ªëng nh∆∞ CNN, n√≥ s·∫Ω l·∫ßn l∆∞·ª£t tr∆∞·ª£t t·ª´ tr√°i ‚Üí ph·∫£i, t·ª´ tr√™n xu·ªëng d∆∞·ªõi h·∫øt to√†n b·ªô b·ª©c ·∫£nh.
Tuy nhi√™n ƒëi·ªÅu n√†y kh√¥ng gi√∫p m√°y t√≠nh ‚Äúhi·ªÉu‚Äù/x√°c ƒë·ªãnh ƒë∆∞·ª£c ƒë·ªëi t∆∞·ª£ng.
![](https://pyimagesearch.com/wp-content/uploads/2014/10/sliding_window_example.gif)
{{< /callout >}}


{{% details title="Hai nhi·ªám v·ª• m√† ta c·∫ßn ƒë·∫∑t ra sau khi √°p d·ª•ng Sliding window" closed="true" %}}

+ L√†m sao **tr√≠ch xu·∫•t ƒë∆∞·ª£c th√¥ng tin**/**feature** c√≥ trong window ƒë√≥.
+ T·ª´ c√°c th√¥ng tin tr√≠ch xu·∫•t ƒë∆∞·ª£c, l√†m sao ƒë·ªÉ **ph√¢n lo·∫°i ƒë·ªëi t∆∞·ª£ng** ƒë√≥ ch√≠nh x√°c

![alt text](image-27.png)


{{% /details %}}


**Region of Interest (ROI)** l√† thu·∫≠t ng·ªØ ch·ªâ c√°c v√πng tr√≠ch xu·∫•t/v√πng ƒë·ªëi t∆∞·ª£ng quan t√¢m t·ª´ sliding window.

{{% details title="M·ªôt s·ªë ph∆∞ogn ph√°p truy·ªÅn th·ªëng ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin t·ª´ ROI" closed="true" %}}

Ta c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c th√¥ng tin c√≥ s·∫µn trong image nh∆∞:

**Haar-like features**: C√°c ƒë·∫∑c tr∆∞ng haar-like l√† c√°c b·ªô l·ªçc ƒë∆°n gi·∫£n d·ª±a tr√™n s·ª± kh√°c bi·ªát c·ªßa c∆∞·ªùng ƒë·ªô c√°c pixel gi·ªØa c√°c v√πng h√¨nh ch·ªØ nh·∫≠t. Ch√∫ng t√¨m ki·∫øm th√¥ng tin s·∫µn c√≥ trong pixel nh∆∞:

+ **Edge (C·∫°nh)**: L√† ranh gi·ªõi gi·ªØa c√°c v√πng c√≥ m√†u s·∫Øc ho·∫∑c c∆∞·ªùng ƒë·ªô s√°ng kh√°c nhau.
+ **Corners (G√≥c):** L√† ƒëi·ªÉm giao c·ªßa 2 c·∫°nh ho·∫∑c l√† ƒëi·ªÉm c√≥ ƒë·ªô cong cao.

![alt text](image-28.png 'https://levelup.gitconnected.com/haar-like-features-seeing-in-black-and-white-1a240caaf1e3')

**SIFT (Scale-Invariant Feature Transform):** l√† m·ªôt thu·∫≠t to√°n tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (feature extraction) gi√∫p t√¨m c√°c ƒëi·ªÉm quan tr·ªçng b·∫•t bi·∫øn v·ªõi t·ªâ l·ªá xoay, bi·∫øn ƒë·ªïi √°nh s√°ng.

![alt text](image-29.png 'https://medium.com/@deepanshut041/introduction-to-sift-scale-invariant-feature-transform-65d7f3a72d40')


{{% /details %}}

{{% details title="∆Øu ƒëi·ªÉm v√† h·∫°n ch·∫ø c·ªßa sliding windows" closed="true" %}}

+ **∆Øu ƒëi·ªÉm:**
  + D·ªÖ t√≠nh to√°n, hi·ªáu qu·∫£ v·ªõi c√°c t√°c v·ª• ƒë∆°n gi·∫£n.
+ **Nh∆∞·ª£c ƒëi·ªÉm:**
  + Ph·∫£i x·ª≠ l√Ω r·∫•t nhi·ªÅu ROI t·ª´ window
  + Do ƒë√≥ d√π d·ªÖ t√≠nh v√† interpretable nh∆∞ng t·ªïng th·ªÉ c·ª±c k√¨ ch·∫≠m ch·∫°p.
  + Kh√≥ hi·ªÉu c√°c ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p nh·∫°t c·∫£m v·ªõi nhi·ªÖu v√† c√°c bi·∫øn ƒë·ªïi nh·ªè.
{{% /details %}}


{{% details title="C√°c ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng c·ªßa Face Detection" closed="true" %}}

{{% details title="Ph∆∞∆°ng ph√°p d·ª±a tr√™n ki·∫øn th·ª©c (knowledge-based model)" closed="true" %}}

S·ª≠ d·ª•ng c√°c quy t·∫Øc v√† ki·∫øn th·ª©c v·ªÅ khu√¥n m·∫∑t ng∆∞·ªùi ƒë·ªÉ x√°c ƒë·ªãnh c√°c ƒë·∫∑c ƒëi·ªÉm v√† v·ªã tr√≠ c·ªßa khu√¥n m·∫∑t trong ·∫£nh.

- V√≠ d·ª•: khu√¥n m·∫∑t ng∆∞·ªùi th∆∞·ªùng c√≥ hai m·∫Øt, m·ªôt m≈©i, m·ªôt mi·ªáng v√† ch√∫ng c√≥ v·ªã tr√≠ t∆∞∆°ng ƒë·ªëi c·ªë ƒë·ªãnh v·ªõi nhau.
- H·∫°n ch·∫ø: C√°c ph∆∞∆°ng ph√°p n√†y th∆∞·ªùng kh√≥ khƒÉn khi x·ª≠ l√Ω c√°c bi·∫øn ƒë·ªïi v·ªÅ t∆∞ th·∫ø, bi·ªÉu c·∫£m, √°nh s√°ng v√† che khu·∫•t.

![alt text](image-30.png 'https://www.researchgate.net/figure/manually-identified-facial-features-57-C1996-IEEE_fig2_220635738')

{{% /details %}}

{{% details title="Ph∆∞∆°ng ph√°p d·ª±a tr√™n ƒë·∫∑c tr∆∞ng (Feature-based methods)" closed="true" %}}
C√°c ph∆∞∆°ng ph√°p n√†y tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh t·ª´ ·∫£nh, sau ƒë√≥ s·ª≠ d·ª•ng c√°c ƒë·∫∑c tr∆∞ng n√†y ƒë·ªÉ ph√¢n lo·∫°i c√°c v√πng trong ·∫£nh l√† khu√¥n m·∫∑t hay kh√¥ng ph·∫£i khu√¥n m·∫∑t.

- V√≠ d·ª•: c√°c ƒë·∫∑c tr∆∞ng c√≥ th·ªÉ l√† m√†u s·∫Øc da, k·∫øt c·∫•u da, c·∫°nh, g√≥c, ho·∫∑c c√°c ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p h∆°n nh∆∞ HOG (Histogram of Oriented Gradients), LBP (Local Binary Patterns).
- M·ªôt s·ªë ph∆∞∆°ng ph√°p ph·ªï bi·∫øn:
    - **Eigenfaces:** S·ª≠ d·ª•ng ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh (PCA) ƒë·ªÉ tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng ch√≠nh c·ªßa khu√¥n m·∫∑t.
    - **Fisherfaces:** S·ª≠ d·ª•ng ph√¢n t√≠ch ph√¢n bi·ªát tuy·∫øn t√≠nh (LDA) ƒë·ªÉ tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng ph√¢n bi·ªát gi·ªØa c√°c khu√¥n m·∫∑t.

![alt text](image-31.png)

{{% /details %}}


{{% details title="Ph∆∞∆°ng ph√°p d·ª±a tr√™n template (Template-matching methods)" closed="true" %}}
C√°c ph∆∞∆°ng ph√°p n√†y s·ª≠ d·ª•ng m·ªôt ho·∫∑c nhi·ªÅu khu√¥n m·∫´u (template) c·ªßa khu√¥n m·∫∑t, sau ƒë√≥ so s√°nh c√°c khu√¥n m·∫´u n√†y v·ªõi c√°c v√πng trong ·∫£nh ƒë·ªÉ t√¨m ra s·ª± t∆∞∆°ng ƒë·ªìng.

**H·∫°n ch·∫ø:** C√°c ph∆∞∆°ng ph√°p n√†y th∆∞·ªùng nh·∫°y c·∫£m v·ªõi c√°c bi·∫øn ƒë·ªïi v·ªÅ **k√≠ch th∆∞·ªõc**, **t∆∞ th·∫ø** v√† **g√≥c nh√¨n c·ªßa khu√¥n m·∫∑t**.

![alt text](image-32.png 'https://www.scitepress.org/papers/2014/49063/49063.pdf')

{{% /details %}}


{{% details title="Ph∆∞∆°ng ph√°p ph√°t hi·ªán d·ª±a tr√™n s·ª± xu·∫•t hi·ªán (Appearance-based methods)" closed="true" %}}

C√°c ph∆∞∆°ng ph√°p n√†y s·ª≠ d·ª•ng c√°c m√¥ h√¨nh th·ªëng k√™ ƒë·ªÉ bi·ªÉu di·ªÖn s·ª± bi·∫øn ƒë·ªïi c·ªßa khu√¥n m·∫∑t.

V√≠ d·ª•: **Active Appearance Models (AAMs)** s·ª≠ d·ª•ng m·ªôt t·∫≠p h·ª£p c√°c ƒëi·ªÉm **landmark** ƒë·ªÉ m√¥ t·∫£ h√¨nh d·∫°ng khu√¥n m·∫∑t v√† m·ªôt m√¥ h√¨nh th·ªëng k√™ ƒë·ªÉ bi·ªÉu di·ªÖn s·ª± bi·∫øn ƒë·ªïi c·ªßa h√¨nh d·∫°ng v√† k·∫øt c·∫•u.

![alt text](image-33.png 'https://www.semanticscholar.org/paper/Active-Appearance-Models-for-Automatic-Fitting-of-Faggian-Paplinski/767e21c3c2457f8c0160bbffbdb3fab3b80e6337/figure/0')


{{% /details %}}
{{% /details %}}


{{< callout emoji="üìå" >}}

Ph·∫ßn ti·∫øp theo ch√∫ng ta s·∫Ω c√πng t√¨m hi·ªÉu m·ªôt trong nh·ªØng gi·∫£i thu·∫≠t truy·ªÅn th·ªëng n·ªïi ti·∫øng nh·∫•t l√∫c b·∫•y gi·ªù l√† **Viola-Jones Algorithm** s·ª≠ d·ª•ng **Haar Wavelets** hay **Haar Features**.
  
{{< /callout >}}

## 7. Viola-Jones Algorithm

>[!NOTE]
>B·ªëi c·∫£nh l√∫c n√†y ƒëang l√† ch√∫ng ta bi·∫øt ƒë∆∞·ª£c g∆∞∆°ng m·∫∑t s·∫Ω lu√¥n c√≥ nh·ªØng ƒë·∫∑c ƒëi·ªÉm nh∆∞: **eyes**, **eyebrows**, **nose**, **lips**,‚Ä¶
>- C√¢u h·ªèi hi·ªán t·∫°i l√† l√†m sao **extract** ƒë∆∞·ª£c face feature n√†y?
>- B√™n c·∫°nh vi·ªác nh·∫≠n bi·∫øt nh·ªØng ƒë·∫∑c ƒëi·ªÉm n√†y, l√†m sao ƒë·ªÉ **face detect real-time**.

### 7.1. Gi·ªõi thi·ªáu v·ªÅ Viola-Jones

**Viola-Jones** d√πng c√°c kernel ƒë∆°n gi·∫£n ƒë·ªÉ t√¨m ƒë·∫∑c tr∆∞ng khu√¥n m·∫∑t, k·∫øt h·ª£p v·ªõi b·ªô ph√¢n lo·∫°i theo c√°c stage ƒë·ªÉ lo·∫°i b·ªè nhanh v√πng kh√¥ng ph·∫£i khu√¥n m·∫∑t, cu·ªëi c√πng d√πng **sliding window** ƒë·ªÉ qu√©t qua ·∫£nh v√† **t√¨m ra khu√¥n m·∫∑t.**

C·ª• th·ªÉ h∆°n, m·ªói stage xem nh∆∞ 1 conditional feature m√† ROI ƒë√≥ ph·∫£i ƒë√°p ·ª©ng. N·∫øu **kh√¥ng ƒë√°p ·ª©ng** ‚Üí **xem nh∆∞ background v√† lo·∫°i b·ªè**, kh√¥ng c·∫ßn x√©t ƒë·∫øn nh·ªØng stage ti·∫øp theo.

![alt text](image-34.png 'https://github.com/heejoojin/face_recognition_using_raspberry_pi')


### 7.2. Gi·ªõi thi·ªáu v·ªÅ Haar features

**Haar feature** t√≠nh to√°n s·ª± kh√°c bi·ªát v·ªÅ c∆∞·ªùng ƒë·ªô pixel gi·ªØa c√°c v√πng h√¨nh ch·ªØ nh·∫≠t tr√™n ·∫£nh.

N√≥ s·ª≠ d·ª•ng kernel ƒë∆°n gi·∫£n, ƒë∆∞·ª£c c√°c chuy√™n gia ƒë·ªãnh nghƒ©a tr∆∞·ªõc (c√°c h√¨nh ch·ªØ nh·∫≠t ƒëen tr·∫Øng nh∆∞ h√¨nh b√™n d∆∞·ªõi) cho t·ª´ng b·ªô ph·∫≠n kh√°c nhau. 

C√°c v√πng ƒëen bi·ªÉu th·ªã tr·ªçng s·ªë √¢m, c√°c v√πng tr·∫Øng bi·ªÉu th·ªã tr·ªçng s·ªë d∆∞∆°ng.

Gi√° tr·ªã c·ªßa **Haar feature** ƒë∆∞·ª£c t√≠nh b·∫±ng c√°ch l·∫•y t·ªïng gi√° tr·ªã pixel trong c√°c v√πng tr·∫Øng tr·ª´ ƒëi t·ªïng gi√° tr·ªã pixel trong c√°c v√πng ƒëen. Vi·ªác t√≠nh to√°n n√†y c√≥ th·ªÉ ƒë∆∞·ª£c th·ª±c hi·ªán r·∫•t nhanh ch√≥ng nh·ªù s·ª≠ d·ª•ng **Integral Image**.

![alt text](image-35.png)


>[!NOTE]
>V·∫≠y ƒë·ªÉ detect ƒë∆∞·ª£c g∆∞∆°ng m·∫∑t, ta s·∫Ω ph·∫£i s·ª≠ d·ª•ng m·ªôt b·ªô kernel chuy√™n bi·ªát nh∆∞ sau:
>![alt text](image-36.png)


{{% details title="H·∫°n ch·∫ø c·ªßa c√°ch l√†m" closed="true" %}}
Kh√¥ng ph·∫£i **√°p b·ªô kernel** n√†y trong 1 l·∫ßn m√† l√† ph·∫£i **Sliding window** r·∫•t nhi·ªÅu l·∫ßn

![alt text](image-37.png)

![alt text](image-38.png)

{{% /details %}}

{{% details title="V·ªõi m·ªôt b·ª©c ·∫£nh c√≥ size 24x24 th√¨ s·ªë l∆∞·ª£ng feature combination ƒë·ªÉ face detect l√† bao nhi√™u? 28x28 l√† bao nhi√™u? " closed="true" %}}
>‚ÄúViola-Jones uses 24*24 as base window size and calculates the above features all over the image shifting by 1 PX (Template + Feature).
>There are over 160,000 possible feature combinations that can fit into a 24x24 pixel image, and over 250,000 for a 28x28 image.‚Äù 

{{% /details %}}

{{% details title="V·ªõi s·ªë l∆∞·ª£ng l·ªõn feature combination nh∆∞ v·∫≠y, l√†m sao ƒë·ªÉ ƒë√°p ·ª©ng vi·ªác t√≠nh to√°n Haar feature real-time?" closed="true" %}}
*Gi·∫£ s·ª≠ ta c√≥ 1 ROI ƒë∆∞·ª£c l∆∞u d∆∞·ªõi m√°y t√≠nh nh∆∞ sau:*

![alt text](image-39.png)

*V√† filter ƒë·ªÉ detect m≈©i nh∆∞ sau:*

![alt text](image-40.png)

{{< callout type="info" >}}
  L∆∞u √Ω r·∫±ng m√¨nh d√πng t·ª´ filter ·ªü ƒë√¢y thay v√¨ kernel nh·∫±m m·ª•c ƒë√≠ch nh·∫•n m·∫°nh c√°c gi√° tr·ªã ·ªü ƒë√¢y ch·ªâ nh·∫±m m·ª•c ƒë√≠ch x√°c ƒë·ªãnh v·ªã tr√≠, kh√¥ng c√≥ th·ª±c hi·ªán nh√¢n t∆∞∆°ng ·ª©ng nh∆∞ Convolution. L√∫c n√†y ch∆∞a c√≥ kh√°i ni·ªám Convolution cho x·ª≠ l√Ω ·∫£nh
{{< /callout >}}

*Ta s·∫Ω l·∫•y t·ªïng to√†n b·ªô gi√° tr·ªã pixel trong v√πng m√†u ƒëen - t·ªïng pixel trong v√πng m√†u tr·∫Øng.*

![alt text](image-41.png)

V·∫≠y b·∫°n c√≥ th·ªÉ thay ƒë·ªïi gi√° tr·ªã filter th√†nh con s·ªë b·∫•t k√¨, n√≥ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn c√°c gi√° tr·ªã c·ªßa image.


{{< callout type="error" >}}
 **H·∫°n ch·∫ø:**

+ T·ªën nhi·ªÅu th·ªùi gian v√† chi ph√≠ t√≠nh to√°n.

+ C√πng 1 v√πng ch√¢n m√†y b√™n tr√°i, b·∫°n s·∫Ω ph·∫£i apply nhi·ªÅu kernel kh√°c nhau v√† t√≠nh sum cho t·ª´ng kernel t∆∞∆°ng ·ª©ng. 

+ B·∫°n nghƒ© ch·ªâ c√≥ 5 kernel th√¥i th√¨ c≈©ng nhanh m√† ph·∫£i kh√¥ng? V·∫≠y n·∫øu b√†i to√°n Object Detection m·ªü r·ªông l√™n 100 class kh√°c nhau th√¨ sao, m·ªói class l·∫°i c√≥ 5 kernel th√¨ b·∫°n s·∫Ω ph·ªâ t√≠nh 500 l·∫ßn summation nh∆∞ v·∫≠y.
{{< /callout >}}

{{% /details %}}


{{% details title="C√°ch t√≠nh Integral Image?" closed="true" %}}
![alt text](image-42.png 'https://medium.com/@aaronward6210/facial-detection-understanding-viola-jones-algorithm-116d1a9db218')

Integral Image. Idea ch√≠nh c·ªßa n√≥ l√† l√†m ƒë·∫©y qua 1 b∆∞·ªõc trung gian, t√≠nh **t·ªïng 1 l·∫ßn ban ƒë·∫ßu**, sau ƒë√≥ ta c√≥ th·ªÉ ƒë∆∞a c√°c b∆∞·ªõc t√≠nh t·ªïng ti·∫øp theo

Ta s·∫Ω t·∫°o m·ªôt ma tr·∫≠n m·ªõi v·ªõi c√°c gi√° tr·ªã c·ªßa n√≥ s·∫Ω ƒë∆∞·ª£c t√≠nh b·∫±ng **t·ªïng** c√°c gi√° tr·ªã ·ªü **v·ªã tr√≠ b√™n tr√°i** v√† **ph√≠a tr√™n** c·ªßa b·ª©c ·∫£nh g·ªëc

![[https://towardsdatascience.com/face-detection-with-haar-cascade-727f68dafd08](https://towardsdatascience.com/face-detection-with-haar-cascade-727f68dafd08)](https://miro.medium.com/v2/resize:fit:1400/1*K2r9aTsaU-spymgjcMCWAA.gif 'https://towardsdatascience.com/face-detection-with-haar-cascade-727f68dafd08')


Nh√¨n v√†o h√¨nh minh h·ªça b√™n d∆∞·ªõi, ta c√≥ th·ªÉ th·∫•y

- C√°ch th√¥ng th∆∞·ªùng ‚Üí long way $\mathcal O(N)$ do ph·∫£i ch·∫°y v√≤ng l·∫∑p t√≠nh t·ªïng to√†n b·ªô pixel ·∫£nh g·ªëc.
- Short way ‚Üí $\mathcal O(1)$ do ch·ªâ c·∫ßn l·∫•y gi√° tr·ªã g√≥c ph·∫£i c·ªßa ROI l√† 178 - ƒëi g√≥c ph·∫£i c·ªßa v√πng li·ªÅn k·ªÅ b√™n tr√°i l√† 90.

![alt text](image-43.png)


>[!NOTE]
>V·∫≠y c√¥ng th·ª©c t·ªïng qu√°t ƒë·ªÉ t√≠nh t·ªïng c·ªßa m·ªôt v√πng filter b·∫•t k√¨ ·ªü original image th√¥ng qua integral image s·∫Ω l√† 
>\text{Gi√° tr·ªã t·ªïng c·ªßa index (pixel integral image) : } 4 -(2+3) + 1

L√≠ do ta ph·∫£i c·ªông l·∫°i cho 1 l√† v√¨ $4-(2+3)$ v√¥ t√¨nh tr·ª´ ƒëi 2 l·∫ßn v√πng $A$

![alt text](image-44.png)


{{% /details %}}


{{< callout type="error" >}}
C√¢u chuy·ªán th·ª±c t·∫ø t∆∞∆°ng ƒë·ªëi ph·ª©c t·∫°p, 
1. C√πng 1 object nh∆∞ng s·∫Ω c√≥ c√°c size kh√°c nhau ‚Üí **Multi-scale problem**
2. Ngo√†i ra ta ph·∫£i s·ª≠ d·ª•ng r·∫•t nhi·ªÅu **Haar Feature** kh√°c nhau, kh√¥ng ph·∫£i c≈©ng c√≥ √≠ch cho vi·ªác d·ª± ƒëo√°n.
{{< /callout >}}

{{% details title="C√°ch gi·∫£i quy·∫øt" closed="true" %}}
**AdaBoost** gi√∫p ch·ªçn ra m·ªôt t·∫≠p h·ª£p nh·ªè c√°c ƒë·∫∑c tr∆∞ng t·ªët nh·∫•t, c√≥ kh·∫£ nƒÉng ph√¢n lo·∫°i cao nh·∫•t.

- M·ªói ƒë·∫∑c tr∆∞ng Haar-like t∆∞∆°ng ·ª©ng v·ªõi m·ªôt b·ªô ph√¢n lo·∫°i y·∫øu. **AdaBoost** s·∫Ω hu·∫•n luy·ªán c√°c b·ªô ph√¢n lo·∫°i y·∫øu n√†y tr√™n t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán.
- **AdaBoost** s·∫Ω g√°n tr·ªçng s·ªë cho t·ª´ng m·∫´u hu·∫•n luy·ªán. C√°c m·∫´u b·ªã ph√¢n lo·∫°i sai s·∫Ω ƒë∆∞·ª£c g√°n tr·ªçng s·ªë cao h∆°n.
- Sau ƒë√≥ ch·ªçn ra nh·ªØng b·ªô ph√¢n lo·∫°i y·∫øu c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t tr√™n t·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c g√°n tr·ªçng s·ªë.
- V√† cu·ªëi c√πng l√† k·∫øt h·ª£p c√°c b·ªô ph√¢n lo·∫°i y·∫øu ƒë√£ ch·ªçn th√†nh m·ªôt b·ªô ph√¢n lo·∫°i m·∫°nh (strong classifier).

![alt text](image-45.png)

![alt text](image-46.png)

{{% /details %}}

{{% details title="ƒêi·ªÅu n√†y ƒë√£ ƒë·ªß ƒë√°p ·ª©ng real-time application hay ch∆∞a? N·∫øu ch∆∞a th√¨ h·ªç l√†m th√™m c√°ch n√†o?" closed="true" %}}
V√¨ **top-K filter** ƒë√£ ƒë·ªß ƒë·ªÉ **100% positive samples**, v·∫≠y ch·ªâ c·∫ßn nh·ªØng ROI n√†o **kh√¥ng ƒë√°p ·ª©ng top-K filter ƒë√≥ th√¨ ta lo·∫°i b·ªè**. C√≤n n·∫øu ƒë√°p ·ª©ng th√¨ ti·∫øp t·ª•c apply nh·ªØng filter ti·∫øp theo ƒë·ªÉ tƒÉng accuracy. Do ƒë√≥ h√¨nh v·∫Ω ban ƒë·∫ßu m·ªõi chia ra th√†nh nhi·ªÅu stage v·ªõi s·ªë l∆∞·ª£ng filter l·ªõn h∆°n.

Ph∆∞∆°ng ph√°p n√†y ƒë∆∞·ª£c g·ªçi l√† **Cascade Classifier**.

![alt text](image-47.png 'https://towardsdatascience.com/face-detection-with-haar-cascade-727f68dafd08')

![linklink](https://miro.medium.com/v2/resize:fit:786/format:webp/1*gQdlL1v88PVsIaSyfV_Gkw.gif)
{{% /details %}}


### 7.3. Summary

![alt text](image-48.png)

## 8. Apply SVM v√†o Object Detection

S·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p **x·ª≠ l√Ω ·∫£nh truy·ªÅn th·ªëng**: **HOG (Histogram of Oriented Gradients)**, **SIFT (Scale-Invariant Feature Transform)**, **LBP (Local Binary Patterns)** l√† nh·ªØng ph∆∞∆°ng ph√°p ph·ªï bi·∫øn ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh.

### 8.1. HOG (Histogram of Oriented Gradients)

**HOG (Histogram of Oriented Gradients)** l√† m√¥ t·∫£ ƒë·ªëi t∆∞·ª£ng trong ·∫£nh d·ª±a tr√™n s·ª± ph√¢n b·ªë c·ªßa h∆∞·ªõng gradient.

N√≥i m·ªôt c√°ch ƒë∆°n gi·∫£n, HOG s·∫Ω xem x√©t c√°c c·∫°nh (edge) trong ·∫£nh v√† ph√¢n t√≠ch h∆∞·ªõng c·ªßa c√°c c·∫°nh n√†y. T·ª´ ƒë√≥, **HOG t·∫°o ra m·ªôt histogram** bi·ªÉu di·ªÖn **t·∫ßn su·∫•t xu·∫•t hi·ªán** c·ªßa c√°c **h∆∞·ªõng gradient** kh√°c nhau **trong m·ªôt v√πng ·∫£nh nh·∫•t ƒë·ªãnh**.

![alt text](image-49.png)


Input l√† m·ªôt b·ª©c ·∫£nh size 720x475. Gi·∫£ s·ª≠ b·∫±ng c√°ch th·∫ßn k·ª≥ n√†o ƒë√≥ ta crop ƒë∆∞·ª£c h√¨nh ·∫£nh con ng∆∞·ªùi 100x200 v√† resize n√≥ v·ªÅ chu·∫©n 64x128.

![alt text](image-50.png)


Output s·∫Ω gi√∫p l√†m n·ªïi b·∫≠t c√°c c·∫°nh v√† bi√™n c·ªßa con ng∆∞·ªùi trong b·ª©c ·∫£nh.

![alt text](image-51.png)


N√≥ gi√∫p gi·∫£m s·ªë pixel c·∫ßn ph·∫£i x·ª≠ l√Ω t·ª´ 64 x 128 x 3 = 24576 (input) v·ªÅ c√≤n 3780 (output). T·ª´ ƒë√≥ ƒë·∫©y v√†o SVM ph√¢n lo·∫°i cho d·ªÖ.

{{% details title="C√°ch t√≠nh gradient trong HOG" closed="true" %}}
T·∫°i m·ªói pixel, ch√∫ng ta s·∫Ω quan t√¢m ƒë·∫øn 2 gi√° tr·ªã:

1. **ƒê·ªô l·ªõn (magnitude)**
2. **H∆∞·ªõng (direction)**


<div style='display: flex;'>
  <img src="image-52.png" alt="Gradient" width="400" height="200" style="float: left; margin-right: 10px;" />
<div>
  <img src="image-53.png" alt="Gradient" width="400" height="200" style="float: left; margin-right: 10px;" />
  <img src="image-54.png" alt="Gradient" width="400" height="200" style="float: left; margin-right: 10px;" />
</div>
</div>


{{% /details %}}


{{% details title="C√°c b∆∞·ªõc th·ª±c hi·ªán thu·∫≠t to√°n HOG" closed="true" %}}
Gi·∫£ s·ª≠ ta crop ƒë∆∞·ª£c h√¨nh ·∫£nh ng∆∞·ªùi ƒë√†n √¥ng nh∆∞ b·ª©c h√¨nh b√™n d∆∞·ªõi (l√†m sao crop ƒë∆∞·ª£c th√¨ kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong b√†i gi·∫£ng).

1. Ta resize n√≥ v·ªÅ 64x128

![alt text](image-55.png)

2. Sau ƒë√≥ l·∫°i ti·∫øp t·ª•c chia n√≥ th√†nh m·ªôt grid, m·ªói √¥ 8x8, √°p d·ª•ng kernel nh∆∞ c√¢u 40 ta s·∫Ω ra ƒë∆∞·ª£c 2 ma tr·∫≠n nh∆∞ sau:

![alt text](image-56.png)

3.  Chia ·∫£nh th√†nh c√°c √¥ nh·ªè (cells). Th∆∞·ªùng k√≠ch th∆∞·ªõc m·ªói √¥ l√† 8x8 pixels.

Trong m·ªói cell, x√¢y d·ª±ng histogram 9 bins theo h∆∞·ªõng gradient (t·ª´ 0¬∞ ƒë·∫øn 180¬∞, ho·∫∑c 0¬∞ ƒë·∫øn 360¬∞ t√πy ch·ªçn).

{{% /details %}}

>[!NOTE]
>N√≥i chung ch·ªâ c·∫ßn nh·ªõ ƒë·∫©y th·∫≥ng image g·ªëc v√†o SVM kh√¥ng ƒë∆∞·ª£c, c√°c pixel kh√¥ng ƒë∆∞·ª£c c·∫•u tr√∫c r√µ r√†ng nh∆∞ numerical feature trong tabular data ƒë·ªÉ SVM ho·∫°t ƒë·ªông ƒë∆∞·ª£c t·ªët.
>Ph·∫£i qua m·ªôt b∆∞·ªõc trung gian nh∆∞ HOG ƒë·ªÉ tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng nh∆∞ c·∫°nh v√† bi√™n r·ªìi ƒë·∫©y v√†o SVM m·ªõi t·ªët.


{{% details title="Image pyramids" closed="true" %}}
Image pyramid l√† vi·ªác bi·ªÉu di·ªÖn m·ªôt h√¨nh ·∫£nh ·ªü nhi·ªÅu ƒë·ªô ph√¢n gi·∫£i (resolution) hay multi-scale kh√°c nhau. 

![alt text](image-57.png)


C√≥ hai lo·∫°i Image pyramids ch√≠nh:

- **Gaussian Image pyramids :** M·ªói c·∫•p ƒë·ªô trong kim t·ª± th√°p ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch l√†m m·ªù (Gaussian blur) v√† sau ƒë√≥ l·∫•y m·∫´u xu·ªëng (downsampling) phi√™n b·∫£n tr∆∞·ªõc ƒë√≥.
- **Laplacian Image pyramids:** Kim t·ª± th√°p n√†y l∆∞u tr·ªØ s·ª± kh√°c bi·ªát gi·ªØa c√°c c·∫•p ƒë·ªô trong kim t·ª± th√°p Gaussian. N√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√°i t·∫°o l·∫°i h√¨nh ·∫£nh g·ªëc t·ª´ kim t·ª± th√°p Gaussian.

{{% /details %}}


{{% details title="Soft NMS" closed="true" %}}
Soft NMS l√† m·ªôt c·∫£i ti·∫øn c·ªßa thu·∫≠t to√°n Non-Maximum Suppression (NMS) truy·ªÅn th·ªëng, ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ lo·∫°i b·ªè nh·∫ßm c√°c bounding box ch·ª©a ƒë·ªëi t∆∞·ª£ng.

![alt text](image-58.png)


Thay v√¨ lo·∫°i b·ªè ho√†n to√†n c√°c bounding box c√≥ IoU l·ªõn, Soft NMS s·∫Ω gi·∫£m confidence score c·ªßa ch√∫ng. M·ª©c ƒë·ªô gi·∫£m ƒëi·ªÉm s·ªë ph·ª• thu·ªôc v√†o IoU gi·ªØa c√°c bounding box.

![alt text](image-59.png)

![alt text](image-60.png)

{{% /details %}}

## 9. Quy tr√¨nh Object Detection v·ªõi CNN

```mermaid
graph TD
A[Step 1: Input Image] --> B[Step 2: Construct Image Pyramid]
B --> C[Step 3: Run sliding window at each scale of image pyramid]
C --> D[Step 3.1: For each step of sliding window, extract ROI]
D --> E[Step 3.2: Take ROI and pass it through CNN for classification]
E --> F[Step 3.3: If min probability test passes, record class label and bounding box location]
F --> G[Step 4: Apply NMS]
G --> H[Step 5: Return Result]
```

![alt text](image-61.png)


{{% details title="Decouple head" closed="true" %}}
Decoupled head l√† ph·∫ßn head c·ªßa m√¥ h√¨nh ƒë∆∞·ª£c t√°ch th√†nh hai nh√°nh ƒë·ªôc l·∫≠p, th∆∞·ªùng l√†:

1. **Branch 1: Classification Head** - Ch·ªãu tr√°ch nhi·ªám d·ª± ƒëo√°n nh√£n l·ªõp c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng trong ·∫£nh.
2. **Branch 2: Regression Head** - Ch·ªãu tr√°ch nhi·ªám d·ª± ƒëo√°n t·ªça ƒë·ªô h·ªôp gi·ªõi h·∫°n (bounding box) c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng.

![alt text](image-62.png)

M·ªói head ch·ªãu tr√°ch nhi·ªám kh√°c nhau 

- Classification c·∫ßn t·∫≠p trung v√†o vi·ªác ph√¢n bi·ªát c√°c ƒë·∫∑c tr∆∞ng c·ªßa c√°c l·ªõp ƒë·ªëi t∆∞·ª£ng.
- Regression ƒë√≤i h·ªèi ƒë·ªô ch√≠nh x√°c cao trong vi·ªác ∆∞·ªõc l∆∞·ª£ng t·ªça ƒë·ªô bounding box.

Hai nhi·ªám v·ª• n√†y c√≥ b·∫£n ch·∫•t v√† tr·ªçng t√¢m kh√°c nhau, vi·ªác d√πng chung m·ªôt head s·∫Ω khi·∫øn m√¥ h√¨nh kh√≥ t·ªëi ∆∞u t·ªët cho c·∫£ hai. N·∫øu kh√¥ng t√°ch bi·ªát, hai nhi·ªám v·ª• c√≥ th·ªÉ "c·∫°nh tranh" trong vi·ªác t·ªëi ∆∞u h√≥a c√°c tham s·ªë c·ªßa m√¥ h√¨nh, d·∫´n ƒë·∫øn hi·ªáu su·∫•t th·∫•p h∆°n.

{{% /details %}}

## 10.  CNN Limitations and Spatial Outputs

{{< callout type="error" >}}
M·ªôt c√¢u h·ªèi kh√°c ƒë·∫∑t ra l√† vi·ªác resize input image v·ªõi c√°c **resolution** kh√°c nhau l√†m m·∫•t m√°t th√¥ng tin input.

V·∫≠y c√≥ c√°ch n√†o ƒë·ªÉ gi·ªØ nguy√™n input size hay kh√¥ng?
{{< /callout >}}

{{% details title="T·∫°i sao ta b·∫Øt bu·ªôc ph·∫£i resize input image trong CNN" closed="true" %}}
V·∫•n ƒë·ªÅ n·∫±m ·ªü b∆∞·ªõc **Fully-connected** cu·ªëi khi t·ªïng h·ª£p th√¥ng tin. S·ªë neurons ·ªü b∆∞·ªõc n√†y ph·∫£i khai b√°o tr∆∞·ªõc v√† c·ªë ƒë·ªãnh cho to√†n b·ªô samples. *Do ƒë√≥ n·∫øu input size kh√°c nhau s·∫Ω kh√¥ng match v·ªõi l·ªõp FC cu·ªëi n√†y*.

![alt text](image-63.png)

ƒêi·ªÅu n√†y v√¥ t√¨nh gi√∫p CNN t·∫°o ra t√≠nh **localization**, gi·ªëng v·ªõi pooling c√≥ th·ªÉ t·ªïng h·ª£p th√¥ng tin ·ªü nhi·ªÅu v·ªã tr√≠ kh√°c nhau c·ªßa input (c√≥ nghƒ©a m·ªói c·ªôt s·∫Ω v·∫´n l√† ouput c·ªßa m·ªôt sliding windows nh∆∞ng t·∫°i v√¨ s·ªë l∆∞·ª£ng input l·ªõn h∆°n n√™n t·∫°o nhi·ªÅu sliding windows h∆°n t·∫°o ra nhi·ªÅu output c·ªßa nhi·ªÅu sliding windows h∆°n).

![alt text](image-64.png)

![alt text](image-65.png)
{{% /details %}}


## Resource

{{% details title="Slide" closed="true" %}}
{{< pdf "Slide.pdf" >}}
{{% /details %}}

{{% details title="OverFeat" closed="true" %}}
[https://skyil.tistory.com/195](https://skyil.tistory.com/195)
{{% /details %}}



