---
title: Linear Regression
type: docs
weight: 1
sidebar:
  open: true
tags:
    - Linear Regression
math: true
comments: true
password: "1234"
blog: true
---


## Introduction

**Linear Regression** lÃ  má»™t trong nhá»¯ng phÆ°Æ¡ng phÃ¡p cá»• Ä‘iá»ƒn vÃ  ná»n táº£ng nháº¥t trong thá»‘ng kÃª vÃ  há»c mÃ¡y, vá»›i lá»‹ch sá»­ phÃ¡t triá»ƒn lÃ¢u Ä‘á»i tá»« Ä‘áº§u tháº¿ ká»· 19. 

![image.png](image.png)

Ã tÆ°á»Ÿng cÆ¡ báº£n cá»§a nÃ³ xuáº¥t hiá»‡n vá»›i tÃªn gá»i **Least Squares (bÃ¬nh phÆ°Æ¡ng tá»‘i tiá»ƒu/bÃ¬nh phÆ°Æ¡ng nhá» nháº¥t)** khi nhÃ  toÃ¡n há»c ngÆ°á»i PhÃ¡p **Adrien-Marie Legendre** (1805) vÃ  nhÃ  thiÃªn vÄƒn há»c **Carl Friedrich Gauss** (1809) sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘á»ƒ dá»± Ä‘oÃ¡n quá»¹ Ä‘áº¡o cá»§a cÃ¡c thiÃªn thá»ƒ (chá»§ yáº¿u lÃ  sao chá»•i, sau Ä‘Ã³ lÃ  cÃ¡c tiá»ƒu hÃ nh tinh xung quanh TrÃ¡i Äáº¥t). 

Sau Ä‘Ã³ thuáº­t ngá»¯ **Regression** (Há»“i quy) má»›i Ä‘Æ°á»£c **Francis Galton** sá»­ dá»¥ng Ä‘á»ƒ á»©ng dá»¥ng vÃ o sinh há»c vÃ  phÃ¡t triá»ƒn cÃ¡c khÃ¡i niá»‡m nhÆ° **há»‡ sá»‘ tÆ°Æ¡ng quan (correlation coefficient)** vÃ  **Regression toward the mean**. Vá» sau chÃºng Ä‘Æ°á»£c **Udny Yule**Â vÃ Â **Karl Pearson** má»Ÿ rá»™ng má»™t cÃ¡ch tá»•ng quÃ¡t.

VÃ o nhá»¯ng nÄƒm 1950 vÃ  1960, cÃ¡c nhÃ  kinh táº¿ há»c Ä‘Ã£ sá»­ dá»¥ngÂ mÃ¡y tÃ­nh Ä‘á»ƒ bÃ nÂ cho viá»‡c tÃ­nh toÃ¡n há»“i quy. TrÆ°á»›c nÄƒm 1970, Ä‘Ã´i khi pháº£i máº¥t tá»›i 24 giá» Ä‘á»ƒ nháº­n Ä‘Æ°á»£c káº¿t quáº£.

![The ItalianÂ Programma 101, an early commercial programmable calculator produced byÂ OlivettiÂ in 1964 - [wikipedia.org](https://en.wikipedia.org/wiki/Calculator#Precursors_to_the_electronic_calculator)](image%201.png)

The ItalianÂ Programma 101, an early commercial programmable calculator produced byÂ OlivettiÂ in 1964 - [wikipedia.org](https://en.wikipedia.org/wiki/Calculator#Precursors_to_the_electronic_calculator)

### Má»™t vÃ­ dá»¥ vá» bÃ i toÃ¡n Linear Regression.
    
HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n lÃ  quáº£n lÃ½ nhÃ¢n sá»± vÃ  cáº§n dá»± Ä‘oÃ¡n tiá»n lÆ°Æ¡ng cá»§a nhÃ¢n viÃªn dá»±a trÃªn sá»‘ nÄƒm kinh nghiá»‡m cá»§a há». Báº¡n cÃ³ má»™t vÃ i thÃ´ng tin vá» má»©c lÆ°Æ¡ng vÃ  sá»‘ nÄƒm kinh nghiá»‡m cá»§a má»™t sá»‘ nhÃ¢n viÃªn, vÃ  bÃ¢y giá» báº¡n muá»‘n dá»±a vÃ o Ä‘Ã³ Ä‘á»ƒ dá»± Ä‘oÃ¡n má»©c lÆ°Æ¡ng cho nhá»¯ng nhÃ¢n viÃªn khÃ¡c.

Äá»ƒ trá»±c quan hÃ³a, ta gá»i sá»‘ nÄƒm kinh nghiá»‡m lÃ  ($x$) vÃ  sá»‘ tiá»n lÆ°Æ¡ng lÃ  ($y$). Khi Ä‘Ã³, vá»›i má»™t vÃ i thÃ´ng tin cho trÆ°á»›c cá»§a má»™t vÃ i nhÃ¢n viÃªn, ta cÃ³ thá»ƒ sá»­ dá»¥ng trá»¥c Ä‘á»“ thá»‹ $Oxy$ Ä‘á»ƒ trá»±c quan hÃ³a dá»¯ liá»‡u nhÆ° hÃ¬nh sau:

![Nguá»“n: [AI Viá»‡t Nam](https://drive.google.com/file/d/1wJPfRJ1vdGfCSLCoymLmzhLLIsOxui8r/view?fbclid=IwY2xjawFwkJJleHRuA2FlbQIxMAABHcQR6X69cg-V5EzAwxE_Z4NINrlM_s7G1l65KiHPu_WBtJfRKk2x4XHGxw_aem_zw3qi6fKXp0h6I_RdnhFQw)](image%202.png)

Nguá»“n: [AI Viá»‡t Nam](https://drive.google.com/file/d/1wJPfRJ1vdGfCSLCoymLmzhLLIsOxui8r/view?fbclid=IwY2xjawFwkJJleHRuA2FlbQIxMAABHcQR6X69cg-V5EzAwxE_Z4NINrlM_s7G1l65KiHPu_WBtJfRKk2x4XHGxw_aem_zw3qi6fKXp0h6I_RdnhFQw)
    
{{% details title="So sÃ¡nh giá»¯a Linear Regression vÃ  Logistics Regression" closed="true" %}}

- **Linear Regression**:
    - Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c.
    - Káº¿t quáº£ Ä‘áº§u ra lÃ  má»™t sá»‘ thá»±c vÃ´ háº¡n, cÃ³ thá»ƒ Ã¢m hoáº·c dÆ°Æ¡ng.
    - PhÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n dá»± Ä‘oÃ¡n giÃ¡ trá»‹ nhÆ° dá»± Ä‘oÃ¡n doanh thu, giÃ¡ nhÃ .
- **Logistic Regression**:
    - Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t, dÃ¹ng Ä‘á»ƒ **phÃ¢n loáº¡i**.
    - Káº¿t quáº£ Ä‘áº§u ra lÃ  giÃ¡ trá»‹ trong khoáº£ng tá»« 0 Ä‘áº¿n 1, hoáº·c phÃ¢n loáº¡i há»¯u háº¡n nhÆ° 0/1 (Yes/No).
    - PhÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n nhÆ° xÃ¡c Ä‘á»‹nh email spam hay khÃ´ng, bá»‡nh nhÃ¢n cÃ³ bá»‡nh hay khÃ´ng.
    
    ![Nguá»“n: [https://x.com/YuHelenYu/status/1455747442662596608](https://x.com/YuHelenYu/status/1455747442662596608)](image%203.png)
    
    Nguá»“n: [https://x.com/YuHelenYu/status/1455747442662596608](https://x.com/YuHelenYu/status/1455747442662596608)
    

$$
y = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_dx_d  = \sum^d_{i = 0} \theta_ix_i
$$


{{% /details %}}

{{% details title="CÃ´ng thá»©c á»Ÿ cÃ¢u trÃªn mÃ´ táº£ cho bÃ i toÃ¡n gÃ¬?" closed="true" %}}
    Há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n.

- **$\theta_0$ Ä‘Æ°á»£c gá»i lÃ  gÃ¬ (3 cÃ¡ch gá»i tiáº¿ng anh khÃ¡c nhau)? Táº¡i sao khÃ´ng cÃ³ $x_0$ Ä‘i kÃ¨m?**
    - Bias : Ä‘á»™ lá»‡ch, thiÃªn kiáº¿n
    - Intercept : há»‡ sá»‘ cháº·n hoáº·c há»‡ sá»‘ tá»± do
    
    Ta máº·c Ä‘á»‹nh $x_0 = 1$, khi Ä‘Ã³ chá»‰ cÃ²n phá»¥ thuá»™c hoÃ n toÃ n vÃ o $\theta_0$. Äá»™ lá»›n cá»§a $\theta_0$ chÃ­nh lÃ  Ä‘á»™ lá»›n mÃ  Ä‘Æ°á»ng há»“i quy cáº¯t trá»¥c $Oy$.
    
    ![Untitled](Untitled.png)

- $x_i$ **Ä‘Æ°á»£c gá»i lÃ  gÃ¬?**
    
    $x_i$ Ä‘Æ°á»£c gá»i lÃ  **features** (cÃ¡c Ä‘áº·c trÆ°ng) hoáº·c **independent variables** (biáº¿n Ä‘á»™c láº­p) lÃ  nhá»¯ng giÃ¡ trá»‹ mÃ  mÃ´ hÃ¬nh sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ Ä‘áº§u ra $y$. NghÄ©a lÃ  náº¿u kÃ½ hiá»‡u $x_d$ nghÄ©a lÃ  ta cÃ³ $d$ dimensions / $d$ features.
    
    Má»™t sá»‘ cÃ¡ch gá»i khÃ¡c cho $x_i$ trong tiáº¿ng Anh bao gá»“m:
    
    - **Predictors**: Biáº¿n dÃ¹ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ Ä‘áº§u ra.
    - **Explanatory variables**: Biáº¿n giáº£i thÃ­ch, vÃ¬ nÃ³ giÃºp giáº£i thÃ­ch sá»± thay Ä‘á»•i cá»§a $y$.
    - **Input variables**: Biáº¿n Ä‘áº§u vÃ o trong mÃ´ hÃ¬nh há»c mÃ¡y.
    
    > VÃ­ dá»¥ bá»™ dá»¯ liá»‡u vá» giÃ¡ nhÃ  cÃ³ 2 cá»™t lÃ  chiá»u dÃ i vÃ  chiá»u rá»™ng â†’ $x_1$ vÃ  $x_2$.
    > 
    
    DÄ© nhiÃªn lÃ  má»™t cá»™t thÃ¬ sáº½ cÃ³ nhiá»u giÃ¡ trá»‹, tÆ°Æ¡ng á»©ng vá»›i chiá»u dÃ i vÃ  chiá»u rá»™ng cá»§a nhiá»u cÄƒn nhÃ  khÃ¡c nhau. Sau nÃ y Ä‘á»ƒ phÃ¢n biá»‡t má»™t cÃ¡ch ráº¡ch rÃ²i, má»™t sá»‘ sÃ¡ch sáº½ kÃ½ hiá»‡u: 
    
    - $x_i$ Ä‘á»ƒ chá»‰ cho 1 giÃ¡ trá»‹ duy nháº¥t, vÃ­ dá»¥ $x_1 = 10, x_2 = 5$
    - $\mathrm x_i$ Ä‘á»ƒ chá»‰ cho 1 vector cá»™t, gá»“m nhiá»u giÃ¡ trá»‹:
    
    $$
    \mathrm x_1 = \begin{bmatrix} 10\\ 20 \\ 13\end{bmatrix}
    $$
    
    TÆ°Æ¡ng á»©ng vá»›i chiá»u dÃ i cá»§a cÄƒn nhÃ  thá»© 1 lÃ  10, chiá»u dÃ i cÄƒn nhÃ  thá»© 2 lÃ  20 vÃ  chiá»u dÃ i cÄƒn nhÃ  thá»© 3 lÃ  13.

{{% /details %}}
    
     
    

    

    
- **Táº¡i sao ngÆ°á»i ta gá»i $x_i$ lÃ  cÃ¡c biáº¿n Ä‘á»™c láº­p?**
    
    Tá»« Ä‘á»™c láº­p á»Ÿ Ä‘Ã¢y xuáº¥t phÃ¡t tá»« Ä‘iá»u kiá»‡n cá»§a Há»“i quy tuyáº¿n tÃ­nh, báº¯t buá»™c cÃ¡c $x_i$ pháº£i Ä‘á»™c láº­p vá»›i nhau.
    
    Trong nhiá»u trÆ°á»ng há»£p, cÃ¡c **Ä‘áº·c trÆ°ng (features)** $x_i$ Ä‘Æ°á»£c giáº£ Ä‘á»‹nh lÃ  **Ä‘á»™c láº­p vá»›i nhau**. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c biáº¿n $x_i$ khÃ´ng cÃ³ má»‘i quan há»‡ trá»±c tiáº¿p hoáº·c tÆ°Æ¡ng quan cao vá»›i nhau, tá»©c lÃ  khÃ´ng cÃ³ biáº¿n nÃ o cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t hÃ m tuyáº¿n tÃ­nh cá»§a cÃ¡c biáº¿n khÃ¡c. ÄÃ¢y lÃ  má»™t giáº£ Ä‘á»‹nh quan trá»ng trong nhiá»u mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh Ä‘á»ƒ trÃ¡nh váº¥n Ä‘á» **Ä‘a cá»™ng tuyáº¿n** (multicollinearity), nÆ¡i mÃ  má»‘i quan há»‡ tuyáº¿n tÃ­nh máº¡nh giá»¯a cÃ¡c biáº¿n Ä‘áº§u vÃ o cÃ³ thá»ƒ lÃ m cho viá»‡c Æ°á»›c lÆ°á»£ng cÃ¡c há»‡ sá»‘ $\theta_i$ trá»Ÿ nÃªn khÃ´ng á»•n Ä‘á»‹nh.
    
- $y$ **Ä‘Æ°á»£c gá»i lÃ  gÃ¬?**
    
    ![image.png](image%204.png)
    
- **HÃ£y miÃªu táº£ dataset sau dÆ°á»›i dáº¡ng phÆ°Æ¡ng trÃ¬nh há»“i quy tuyáº¿n tÃ­nh**
    
    $$
    y = \theta_0 + \theta_1x_1+ \theta_2x_2 + \theta_3x_3
    $$
    
    ![Nguá»“n: [https://www.researchgate.net/figure/A-simulated-dataset-of-100-observations-from-a-multiple-linear-regression-consisting-of-3_tbl1_337882697](https://www.researchgate.net/figure/A-simulated-dataset-of-100-observations-from-a-multiple-linear-regression-consisting-of-3_tbl1_337882697)](438e4f8a-da0b-48ef-8041-a67560fdb94a.png)
    
    Nguá»“n: [https://www.researchgate.net/figure/A-simulated-dataset-of-100-observations-from-a-multiple-linear-regression-consisting-of-3_tbl1_337882697](https://www.researchgate.net/figure/A-simulated-dataset-of-100-observations-from-a-multiple-linear-regression-consisting-of-3_tbl1_337882697)
    
- **Giáº£ sá»­ bÃ i toÃ¡n há»“i quy cá»§a chÃºng ta cÃ³ 2 features, khi Ä‘Ã³ cÃ´ng thá»©c Ä‘Æ°á»£c viáº¿t nhÆ° tháº¿ nÃ o? HÃ¬nh dáº¡ng cá»§a hÃ m há»“i quy sáº½ nhÆ° tháº¿ nÃ o?**
    
    $$
    y = \theta_0 + \theta_1x_1 + \theta_2x_2
    $$
    
    ![Untitled](Untitled%201.png)
    
- **Tá»« cÃ¢u trÃªn, Ä‘á»ƒ cÃ³ thá»ƒ váº½ hÃ¬nh trá»±c quan thÃ¬ chÃºng ta chá»‰ cÃ³ thá»ƒ váº½ Ä‘Æ°á»£c bao nhiÃªu feature?**
    
    Tá»‘i Ä‘a lÃ  2 feature
    
    ![Nguá»“n: [https://thaddeus-segura.com/multiple-linear/](https://thaddeus-segura.com/multiple-linear/)](image%205.png)
    
    Nguá»“n: [https://thaddeus-segura.com/multiple-linear/](https://thaddeus-segura.com/multiple-linear/)
    
    Náº¿u cÃ³ **nhiá»u hÆ¡n 2 feature**, viá»‡c váº½ trá»±c quan trá»Ÿ nÃªn khÃ³ khÄƒn, vÃ¬ chÃºng ta khÃ´ng thá»ƒ trá»±c quan hÃ³a khÃ´ng gian vá»›i nhiá»u chiá»u (trÃªn 3D) trÃªn máº·t pháº³ng thÃ´ng thÆ°á»ng. Trong trÆ°á»ng há»£p nÃ y, cÃ¡c ká»¹ thuáº­t trá»±c quan hÃ³a khÃ¡c nhÆ° **giáº£m chiá»u** (dimensionality reduction) hoáº·c biá»ƒu Ä‘á»“ cáº·p (pair plots) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xem xÃ©t má»‘i quan há»‡ giá»¯a cÃ¡c Ä‘áº·c trÆ°ng vÃ  Ä‘áº§u ra trong khÃ´ng gian 2D.
    
- **Giá»¯a $x$ vÃ  $\theta$ giÃ¡ trá»‹ nÃ o Ä‘Ã£ biáº¿t, giÃ¡ trá»‹ nÃ o cáº§n tÃ¬m?**
    
    **ÄÃ£ biáº¿t:** 
    
    - $x$ vÃ  $y$
    
    **Cáº§n tÃ¬m:**
    
    - $\theta$ lÃ  biáº¿n cáº§n tÃ¬m.
        
        ÄÃ¢y lÃ  cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh mÃ  báº¡n cáº§n Æ°á»›c lÆ°á»£ng thÃ´ng qua quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Má»¥c tiÃªu cá»§a há»“i quy tuyáº¿n tÃ­nh lÃ  tÃ¬m ra cÃ¡c giÃ¡ trá»‹ $\theta$ sao cho dá»± Ä‘oÃ¡n $y$ tá»« $x$ lÃ  chÃ­nh xÃ¡c nháº¥t.
        
- **Trong trÆ°á»ng há»£p tá»•ng quÃ¡t, náº¿u chá»‰ sá»­ dá»¥ng Ä‘Æ°á»ng tháº³ng Ä‘á»ƒ cáº¯t qua cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thÃ¬ ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c bao nhiÃªu Ä‘Æ°á»ng tháº³ng?**
    
    Trong trÆ°á»ng há»£p tá»•ng quÃ¡t, náº¿u chá»‰ sá»­ dá»¥ng **má»™t Ä‘Æ°á»ng tháº³ng** Ä‘á»ƒ cáº¯t qua cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u, thÃ¬ vá» lÃ½ thuyáº¿t, ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c **vÃ´ sá»‘ Ä‘Æ°á»ng tháº³ng**.
    
    - CÃ³ ráº¥t nhiá»u cÃ¡ch chá»n Ä‘á»™ dá»‘c $\theta_1$ vÃ  há»‡ sá»‘ tá»± do $\theta_0$ Ä‘á»ƒ táº¡o ra cÃ¡c Ä‘Æ°á»ng tháº³ng khÃ¡c nhau. Chá»‰ cáº§n thay Ä‘á»•i giÃ¡ trá»‹ $\theta_1$ vÃ  $\theta_0$, ta sáº½ cÃ³ má»™t Ä‘Æ°á»ng tháº³ng khÃ¡c.
        
        ![image.png](image%206.png)
        
- **Váº­y ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c má»™t Ä‘Æ°á»ng tháº³ng Ä‘i qua háº¿t táº¥t cáº£ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u trong thá»±c táº¿ khÃ´ng? VÃ  má»¥c tiÃªu cá»§a chÃºng ta cÃ³ pháº£i lÃ  tÃ¬m Ä‘Æ°á»£c hÃ m sá»‘ Ä‘i qua háº¿t toÃ n bá»™ Ä‘iá»ƒm Ä‘Ã³ khÃ´ng?**
    
    CÃ³ thá»ƒ, nhÆ°ng khÃ´ng pháº£i dÃ¹ng Ä‘Æ°á»ng tháº³ng tuyáº¿n tÃ­nh mÃ  dÃ¹ng Polynomial.
    
    Trong thá»±c táº¿ náº¿u polynomial Ä‘i qua háº¿t toÃ n bá»™ Ä‘iá»ƒm sáº½ bá»‹ overfit â†’ thay vÃ¬ há»c pattern tá»•ng quÃ¡t â†’ nÃ³ há»c luÃ´n cáº£ nhiá»…u.
    

<aside>
ğŸ“Œ

Váº­y má»¥c tiÃªu khÃ´ng pháº£i lÃ  lÃ m sao cho Ä‘Æ°á»ng tháº³ng Ä‘i qua toÃ n bá»™ cÃ¡c Ä‘iá»ƒm, nghÄ©a lÃ  cÃ³ Ä‘á»™ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i, mÃ  ta chá»‰ cáº§n má»™t Ä‘Æ°á»ng tháº³ng cho Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘á»‘i. NhÆ°ng theo má»™t nghÄ©a nÃ o Ä‘Ã³ (trÆ°á»ng há»£p tá»•ng quÃ¡t), nÃ³ láº¡i lÃ  Ä‘Æ°á»ng tháº³ng tá»‘t nháº¥t.

</aside>

- **ÄÆ°á»ng tháº³ng fit vá»›i dataset nhÆ° tháº¿ nÃ o Ä‘Æ°á»£c gá»i lÃ  tá»‘t?**
    
    Má»¥c tiÃªu cá»§a há»“i quy tuyáº¿n tÃ­nh lÃ  tÃ¬m **má»™t Ä‘Æ°á»ng tháº³ng tá»‘i Æ°u** nháº¥t. ÄÆ°á»ng tháº³ng tá»‘i Æ°u nÃ y sáº½ lÃ  Ä‘Æ°á»ng tháº³ng **tá»‘i thiá»ƒu hÃ³a tá»•ng khoáº£ng cÃ¡ch (loss)** tá»« cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘áº¿n Ä‘Æ°á»ng tháº³ng, tá»©c lÃ  Ä‘Æ°á»ng tháº³ng cÃ³ giÃ¡ trá»‹ **Mean Squared Error (MSE)** nhá» nháº¥t.
    
    Do Ä‘Ã³, máº·c dÃ¹ vá» lÃ½ thuyáº¿t cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c vÃ´ sá»‘ Ä‘Æ°á»ng tháº³ng, nhÆ°ng ta chá»‰ tÃ¬m **má»™t** Ä‘Æ°á»ng tháº³ng tá»‘t nháº¥t cho bÃ i toÃ¡n dá»± Ä‘oÃ¡n thÃ´ng qua quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a (vÃ­ dá»¥: sá»­ dá»¥ng Gradient Descent).
    
- **Ã tÆ°á»Ÿng cá»§a Linear Regression lÃ  gÃ¬? LÃ m sao Ä‘á»ƒ â€œfit a line through all datapointsâ€?**
    - Dá»‹ch chuyá»ƒn, nÃ¢ng lÃªn, háº¡ xuá»‘ng, xoay sao cho MSE nhá» nháº¥t
- **Khi thay Ä‘á»•i há»‡ sá»‘ slope $\theta_1, \theta_2,...$ thÃ¬ Ä‘iá»u gÃ¬ sáº½ xáº£y ra?**
    
    ÄÆ°á»ng tháº³ng sáº½ xoay 
    
    ![image.png](image%207.png)
    
- **Khi thay Ä‘á»•i há»‡ sá»‘ intercept $\theta_0$ thÃ¬ Ä‘iá»u gÃ¬ sáº½ xáº£y ra?**
    
    ÄÆ°á»ng tháº³ng sáº½ nÃ¢ng lÃªn, háº¡ xuá»‘ng
    
    ![image.png](image%208.png)
    
- **Ban Ä‘áº§u $\theta$ Ä‘Æ°á»£c chá»n nhÆ° tháº¿ nÃ o?**
    
    Ban Ä‘áº§u, cÃ¡c tham sá»‘ $\theta$ thÆ°á»ng Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn hoáº·c báº±ng cÃ¡c giÃ¡ trá»‹ máº·c Ä‘á»‹nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh. Viá»‡c khá»Ÿi táº¡o nÃ y khÃ´ng áº£nh hÆ°á»Ÿng nhiá»u Ä‘áº¿n káº¿t quáº£ cuá»‘i cÃ¹ng, vÃ¬ cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a nhÆ° **Gradient Descent** sáº½ Ä‘iá»u chá»‰nh cÃ¡c giÃ¡ trá»‹
    
    $\theta$ theo tá»«ng bÆ°á»›c Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a hÃ m máº¥t mÃ¡t (loss function). 
    
    Náº¿u may máº¯n khá»Ÿi táº¡o gáº§n vá»›i giÃ¡ trá»‹ chÃ­nh xÃ¡c, viá»‡c há»™i tá»¥ cÃ³ thá»ƒ dá»… dÃ ng vÃ  xáº£y ra nhanh hÆ¡n.
    
    - Random
    - NhÆ°ng mÃ  viá»‡c random cÃ³ áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ tÃ¬m nghiá»‡m. Má»™t sá»‘ thuáº­t toÃ¡n cÃ³ thá»ƒ há»— trá»£ nhÆ° Markov, Latin Hypercube Sampling, hoáº·c khá»Ÿi táº¡o theo má»™t sá»‘ phÃ¢n phá»‘i (Uniform, Gauss)
    - Hoáº·c báº¡n Ä‘Ã£ cÃ³ domain knowledge vá» nhá»¯ng há»‡ sá»‘, báº¡n cÃ³ thá»ƒ random báº±ng nhá»¯ng con sá»‘ Ä‘Ã³ (Xem vÃ­ dá»¥ vá» bÃ i toÃ¡n tÃ­nh giÃ¡ tiá»n taxi).
- **Khoáº£ng cÃ¡ch tá»« giÃ¡ trá»‹ thá»±c táº¿ Ä‘áº¿n giÃ¡ trá»‹ dá»± Ä‘oÃ¡n gá»i lÃ  gÃ¬?**
    
    Khoáº£ng cÃ¡ch tá»« giÃ¡ trá»‹ thá»±c táº¿ Ä‘áº¿n giÃ¡ trá»‹ dá»± Ä‘oÃ¡n trong há»“i quy gá»i lÃ  **sai sá»‘ (residual)**.
    
    Cá»¥ thá»ƒ, sai sá»‘ lÃ  hiá»‡u giá»¯a giÃ¡ trá»‹ thá»±c táº¿ $y$ vÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n $\hat{y}$ cá»§a mÃ´ hÃ¬nh:
    
    $$
    \text{Residual} = y - \hat{y}
    $$
    
    Sai sá»‘ thá»ƒ hiá»‡n sá»± khÃ¡c biá»‡t giá»¯a dá»¯ liá»‡u thá»±c táº¿ vÃ  giÃ¡ trá»‹ mÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n. Trong cÃ¡c bÃ i toÃ¡n há»“i quy, má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh lÃ  tá»‘i thiá»ƒu hÃ³a tá»•ng cÃ¡c sai sá»‘ (thÃ´ng qua cÃ¡c chá»‰ sá»‘ nhÆ° **Mean Squared Error - MSE**) Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a dá»± Ä‘oÃ¡n.
    
    ![Nguá»“n: [https://www.statology.org/residuals/](https://www.statology.org/residuals/)](image%209.png)
    
    Nguá»“n: [https://www.statology.org/residuals/](https://www.statology.org/residuals/)
    

![image.png](image%2010.png)

- **Ta Ä‘ang muá»‘n tÃ¬m khoáº£ng cÃ¡ch tá»« giÃ¡ trá»‹** **thá»±c táº¿ Ä‘áº¿n giÃ¡ trá»‹ dá»± Ä‘oÃ¡n sao cho ngáº¯n nháº¥t. Váº­y cÃ³ pháº£i ta chá»‰ cáº§n chiáº¿u vuÃ´ng gÃ³c Ä‘iá»ƒm dá»¯ liá»‡u xuá»‘ng Ä‘Æ°á»ng há»“i quy giá»‘ng hÃ¬nh trÃªn lÃ  tÃ¬m Ä‘Æ°á»£c khoáº£ng cÃ¡ch ngáº¯n nháº¥t khÃ´ng?**
    
    KhÃ´ng, nháº­n Ä‘á»‹nh trÃªn lÃ  sai vá» cÃ¡ch Há»“i quy tuyáº¿n tÃ­nh hoáº¡t Ä‘á»™ng.
    
    Tuy khoáº£ng cÃ¡ch ngáº¯n nháº¥t giá»¯a Ä‘iá»ƒm thá»±c táº¿ vÃ  Ä‘Æ°á»ng há»“i quy chÃ­nh lÃ  **khoáº£ng cÃ¡ch vuÃ´ng gÃ³c** tá»« Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Ã³ Ä‘áº¿n Ä‘Æ°á»ng há»“i quy, nhÆ°ng trong cÃ¡c bÃ i toÃ¡n há»“i quy tuyáº¿n tÃ­nh, mÃ´ hÃ¬nh tÃ¬m **khoáº£ng cÃ¡ch theo chiá»u dá»c** (Ä‘o tá»« Ä‘iá»ƒm thá»±c táº¿ Ä‘áº¿n Ä‘Æ°á»ng há»“i quy dá»c theo trá»¥c $y$) thay vÃ¬ khoáº£ng cÃ¡ch vuÃ´ng gÃ³c giá»¯a cÃ¡c Ä‘iá»ƒm. 
    
    Náº¿u báº¡n Ã¡p dá»¥ng ká»¹ thuáº­t nhÆ° **PCA (Principal Component Analysis)** hoáº·c cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nháº±m tÃ¬m thÃ nh pháº§n chÃ­nh chÃ­nh, lÃºc Ä‘Ã³ khoáº£ng cÃ¡ch vuÃ´ng gÃ³c nÃ y má»›i cÃ³ thá»ƒ lÃ  tiÃªu chuáº©n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.
    
- **Ta sá»­ dá»¥ng gÃ¬ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»ng tháº³ng cÃ³ â€œfitâ€ vá»›i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u tá»‘t hay khÃ´ng?**
    
    DÃ¹ng loss function.
    
    Sau khi training xong, viá»‡c sá»­ dá»¥ng cÃ¡c cÃ´ng thá»©c khÃ¡c ngoÃ i loss function Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ gá»i chung lÃ  metrics.
    
    Trong thá»±c táº¿, **MSE** vÃ  **$R^2$** lÃ  hai metrics phá»• biáº¿n nháº¥t Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ tá»‘t cá»§a mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh. 
    
- **Giáº£i thÃ­ch Ä‘á»‹nh nghÄ©a hÃ m máº¥t mÃ¡t â€œloss functionâ€ vÃ  ká»ƒ tÃªn má»™t sá»‘ hÃ m máº¥t mÃ¡t thÃ´ng dá»¥ng**
    
    **HÃ m máº¥t mÃ¡t (Loss Function)** lÃ  má»™t hÃ m sá»‘ dÃ¹ng Ä‘á»ƒ Ä‘o lÆ°á»ng má»©c Ä‘á»™ khÃ¡c biá»‡t giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vÃ  giÃ¡ trá»‹ thá»±c táº¿. Má»¥c tiÃªu cá»§a viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh lÃ  tÃ¬m ra cÃ¡c tham sá»‘ (weights) sao cho hÃ m máº¥t mÃ¡t nÃ y Ä‘Æ°á»£c tá»‘i thiá»ƒu hÃ³a, tá»©c lÃ  dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cÃ ng gáº§n vá»›i giÃ¡ trá»‹ thá»±c táº¿ cÃ ng tá»‘t.
    
    HÃ m máº¥t mÃ¡t thÆ°á»ng phá»¥ thuá»™c vÃ o loáº¡i bÃ i toÃ¡n:
    
    - **BÃ i toÃ¡n há»“i quy (Regression)**: HÃ m máº¥t mÃ¡t Ä‘o lÆ°á»ng Ä‘á»™ chÃªnh lá»‡ch giá»¯a cÃ¡c giÃ¡ trá»‹ liÃªn tá»¥c.
    - **BÃ i toÃ¡n phÃ¢n loáº¡i (Classification)**: HÃ m máº¥t mÃ¡t Ä‘o lÆ°á»ng Ä‘á»™ chÃªnh lá»‡ch giá»¯a nhÃ£n dá»± Ä‘oÃ¡n vÃ  nhÃ£n thá»±c táº¿.
    
    ### Má»™t sá»‘ hÃ m máº¥t mÃ¡t thÃ´ng dá»¥ng
    
    - Mean Squared Error (MSE)
    - Mean Absolute Error (MAE)
    - Root Mean Squared Error (RMSE)
    - Hinge Loss â†’ $\mathcal L(y, \hat y) = \max(0, 1 - y\hat y)$
    ThÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n, Ä‘áº·c biá»‡t lÃ  **SVM (Support Vector Machines)**. HÃ m nÃ y pháº¡t khi Ä‘iá»ƒm dá»± Ä‘oÃ¡n khÃ´ng náº±m Ä‘á»§ xa so vá»›i Ä‘Æ°á»ng biÃªn phÃ¢n loáº¡i.
    - Cross-Entropy Loss (Log Loss)
    - Kullback-Leibler Divergence (KL Divergence)
    - Huber Loss
        
        $$
        L_\delta(a)= \begin{cases}\frac{1}{2}(a)^2 & \text { for }|a| \leq \delta \\ \delta\left(|a|-\frac{1}{2} \delta\right) & \text { for }|a|>\delta\end{cases}
        $$
        
        Káº¿t há»£p giá»¯a MAE vÃ  MSE. NÃ³ hoáº¡t Ä‘á»™ng nhÆ° MSE khi sai sá»‘ nhá» vÃ  nhÆ° MAE khi sai sá»‘ lá»›n, giÃºp giáº£m thiá»ƒu sá»± nháº¡y cáº£m vá»›i cÃ¡c ngoáº¡i lá»‡ mÃ  váº«n Ä‘áº£m báº£o mÃ´ hÃ¬nh pháº£n há»“i tá»‘t vá»›i sai sá»‘ nhá».
        
- **Loss function vÃ  Error function khÃ¡c nhau á»Ÿ Ä‘iá»ƒm nÃ o?**
    
    ThÃ´ng thÆ°á»ng thÃ¬ 2 cÃ¡ch gá»i nhÆ° nhau vÃ  cÃ³ thá»ƒ thay tháº¿ cho nhau. Chá»‰ cÃ³ Ã´ng Bishop - Pattern Regconition â†’ Má»™t trong nhá»¯ng cuá»‘n viáº¿t ML theo toÃ¡n há»c xÃ¡c suáº¥t khÃ³ nháº¥t lÃ  Ä‘i phÃ¢n biá»‡t 2 tháº±ng nÃ y.
    
    MÃ¬nh chÆ°a hiá»ƒu rÃµ Ã½ tÃ¡c giáº£ láº¯m chá»— nÃ y, nÃªn mÃ¬nh sáº½ Ä‘Æ°a ra 2 Ã½ kiáº¿n:
    
    1. **Loss function** thÆ°á»ng Ä‘Æ°á»£c Ã¡p dá»¥ng cho **má»™t máº«u dá»¯ liá»‡u** hoáº·c **má»™t Ä‘iá»ƒm dá»¯ liá»‡u**, vÃ  nÃ³ Ä‘o lÆ°á»ng sai sá»‘ cá»§a má»™t giÃ¡ trá»‹ Ä‘áº§u ra so vá»›i giÃ¡ trá»‹ thá»±c táº¿. **Error function** thÆ°á»ng lÃ  tá»•ng hoáº·c trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ cá»§a **loss function trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u**. NÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t hÃ m má»¥c tiÃªu tá»•ng quÃ¡t Ä‘á»ƒ tá»‘i Æ°u hÃ³a toÃ n bá»™ mÃ´ hÃ¬nh.
    2. Loss function thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tá»‘i Æ°u, training Ä‘á»ƒ tÃ¬m ra tham sá»‘. CÃ²n cÃ¡c hÃ m error thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng nhÆ° cÃ¡c metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t káº¿t quáº£.
- **Giáº£i thÃ­ch cÃ´ng thá»©c Mean Square Error**
    
    Thay vÃ¬ láº¥y tá»•ng Ä‘á»™ lá»‡ch (cÃ³ giÃ¡ trá»‹ Ã¢m vÃ  dÆ°Æ¡ng), cÃ¡c sai sá»‘ nÃ y sáº½ triá»‡t tiÃªu láº«n nhau khiáº¿n cho giÃ¡ trá»‹ nÃ y vá» 0 (khÃ´ng cÃ³ lá»—i) máº·c dÃ¹ trÃªn thá»±c táº¿, dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cÃ³ thá»ƒ ráº¥t kÃ©m chÃ­nh xÃ¡c. 
    
    Do Ä‘Ã³ ngÆ°á»i ta sáº½ cÃ¢n nháº¯c lá»±a chá»n giá»¯a láº¥y trung bÃ¬nh trá»‹ tuyá»‡t Ä‘á»‘i hoáº·c bÃ¬nh phÆ°Æ¡ng. ThÃ´ng dá»¥ng nháº¥t ta sáº½ dÃ¹ng bÃ¬nh phÆ°Æ¡ng vÃ¬ cÃ¡c tÃ­nh cháº¥t Ä‘áº¹p cá»§a hÃ m sá»‘ nÃ y.
    
- **Táº¡i sao MSE dÃ¹ng bÃ¬nh phÆ°Æ¡ng Ä‘á»™ lá»‡ch mÃ  khÃ´ng dÃ¹ng giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i? Khi nÃ o nÃªn dÃ¹ng MAE, khi nÃ o nÃªn dÃ¹ng MSE?**
    
    Hint:
    
    1. 
    
    [AI VIET NAM](https://www.facebook.com/aivietnam.edu.vn/posts/pfbid02MpLuz1wcN4HSt9Qbx9AXBP1smDVC4SZWerVXme6MjihXawbN3EQyvpJAQAXdJj22l)
    
    1. 
    
    [AI VIET NAM](https://www.facebook.com/aivietnam.edu.vn/posts/pfbid05LRpL41MwJKaM2HH2UvhdoU7ysZzYkevNSRSP9cUQUASSJi1fGaPCLibvkHtbdjMl)
    
- **Giáº£i thÃ­ch hÃ¬nh váº½ sau, táº¡i sao vá»›i cÃ¡c há»‡ sá»‘ nÃ y thÃ¬ ra Ä‘Æ°á»£c dáº¡ng hÃ¬nh váº½ Ä‘Ã³?**
    
    ![Untitled](Untitled%202.png)
    
- **Giáº£i thÃ­ch cÃ´ng thá»©c Loss function**
    
    $$
    h_\theta(x) = \theta_0 + \theta_1x
    $$
    
    $$
    J(\theta) = \frac{1}{2n} \sum_{i = 1}^n \bigg(h_\theta(x^{(i)}) - y^{(i)}\bigg)^2
    $$
    
    1. Táº¡i sao ngÆ°á»i ta láº¡i Ä‘á»•i kÃ½ hiá»‡u tá»« $y$ â†’ $h_\theta(x)$?
    2. $x^{(i)}$ , $y^{(i)}$ lÃ  gÃ¬?
    3. Vá»›i má»—i $x^{(i)}$, $h_\theta(x^{(i)})$  tráº£ vá» káº¿t quáº£ lÃ  gÃ¬?
    4. Táº¡i sao láº¡i chia cho $\frac{1}{n}$?
    5. Táº¡i sao láº¡i láº¡i chia cho $\frac{1}{2}$? (Hint: liÃªn quan Ä‘áº¿n 2 cÃ¢u dÆ°á»›i)
- **MÃ¡y tÃ­nh Casio thÆ°á»ng dÃ¹ng cÃ¡ch nÃ o Ä‘á»ƒ giáº£i nghiá»‡m?**
    - ÄÃ´i lÃºc sáº½ lÃ  random, cháº¡y toÃ n bá»™ cÃ¡c sá»‘ gáº§n Ä‘Ã³. VÃ­ dá»¥ phÆ°Æ¡ng trÃ¬nh cá»§a báº¡n cÃ³ 2 nghiá»‡m 1 vÃ  -1. Báº¡n nháº­p vÃ o 0.9 nÃ³ sáº½ tráº£ vá» nghiá»‡m lÃ  1, nháº­p vÃ o -0.2 nÃ³ sáº½ tráº£ vá» -1.
- **Ta cÃ³ thá»ƒ tÃ¬m nghiá»‡m giá»‘ng mÃ¡y tÃ­nh Casio khÃ´ng?**
    
    KhÃ´ng nÃªn, vÃ¬ ráº¥t tá»‘n thá»i gian, bá»™ nhá»›. Nghiá»‡m tÃ¬m Ä‘Æ°á»£c chÆ°a cháº¯c lÃ  global minimum.
    
- **Äiá»u kiá»‡n cá»§a cÃ¡c features Ä‘á»ƒ cÃ³ thá»ƒ thá»±c hiá»‡n Linear Regression lÃ  gÃ¬?**
    - **Má»‘i quan há»‡ tuyáº¿n tÃ­nh** giá»¯a biáº¿n Ä‘á»™c láº­p $x$ vÃ  biáº¿n phá»¥ thuá»™c $y$.
    - **KhÃ´ng cÃ³ Ä‘a cá»™ng tuyáº¿n** (Multicollinearity tháº¥p) giá»¯a cÃ¡c biáº¿n Ä‘á»™c láº­p $x_i$.
    - **PhÃ¢n phá»‘i chuáº©n** cá»§a sai sá»‘ (residuals) $\varepsilon$.
    - **PhÆ°Æ¡ng sai khÃ´ng Ä‘á»•i** (Homoskedasticity).
    - **CÃ¡c quan sÃ¡t Ä‘á»™c láº­p** vá»›i nhau.

<aside>
ğŸ“Œ

Náº¿u Ä‘Æ°á»£c thÃ¬ cÃ¡c báº¡n nÃªn tÃ¬m hiá»ƒu cÃ¡ch giáº£i há»‡ phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh báº±ng phÆ°Æ¡ng phÃ¡p khá»­ Gauss-Jordan Ä‘á»ƒ hiá»ƒu táº¡i sao cáº§n pháº£i cÃ³ Ä‘iá»u kiá»‡n Ä‘á»™c láº­p tuyáº¿n tÃ­nh hoáº·c pháº£i cÃ³ cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ nhÆ° loáº¡i bá» duplicate giá»¯a cÃ¡c dÃ²ng Ä‘á»ƒ cÃ³ thá»ƒ giáº£i Ä‘Æ°á»£c nghiá»‡m.

[BÃ i táº­p: Giáº£i há»‡ phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh](https://www.notion.so/B-i-t-p-Gi-i-h-ph-ng-tr-nh-tuy-n-t-nh-17ce9755c5d08186ba82f81a993751c9?pvs=21) 

</aside>

- **Trong bÃ i toÃ¡n thá»±c táº¿ khi ta cÃ³ ráº¥t nhiá»u features, liá»‡u ta cÃ³ thá»ƒ thá»±c hiá»‡n Linear Regression khÃ´ng?**
    
    KhÃ´ng, vÃ¬ sá»‘ features cÃ ng lá»›n, kháº£ nÄƒng xuáº¥t hiá»‡n sá»± phá»¥ thuá»™c cÃ ng nhiá»u. Khi Ä‘Ã³ kháº£ nÄƒng tá»“n táº¡i nghiá»‡m ráº¥t tháº¥p.
    
- **Táº¡i sao ta cÃ³ thá»ƒ xem bÃ i toÃ¡n Linear Regression nhÆ° má»™t bÃ i toÃ¡n tá»‘i Æ°u?**
    
    BÃ i toÃ¡n tÃ¬m max / min cho Loss function Ä‘Æ°á»£c xem lÃ  má»™t bÃ i toÃ¡n tá»‘i Æ°u.
    
- **CÃ´ng thá»©c nghiá»‡m chÃ­nh xÃ¡c cá»§a bÃ i toÃ¡n Linear Regression lÃ  gÃ¬?**
    
    $$
    \hat{\beta}=\left(X^T X\right)^{-1} X^T y
    $$
    
- **Chá»©ng minh cÃ´ng thá»©c trÃªn**
    1. KhÃ´ng cáº§n quan tÃ¢m Ä‘áº¿n hÃ m loss
        
        $$
        \begin{align*} y & = X\theta \\ X^Ty & = X^TX\theta \\ (X^TX)^{-1}X^T&y = (X^TX)^{-1} X^TX\theta\end{align*}
        $$
        
        $$
        \theta = (X^TX)^{-1}X^Ty
        $$
        
    2. Náº¿u suy ra tá»« hÃ m loss MSE 
        
        $$
        \begin{aligned}
        & J(\theta)=\left(\ (X \theta)^T-y^T\right)(X \theta-y) \\
        & J(\theta)=(X \theta)^T X \theta-(X \theta)^T y-y^T(X \theta)+y^T y \\
        & J(\theta)=\theta^T X^T X \theta-2(X \theta)^T y+y^T y
        \end{aligned}
        $$
        
        Láº¥y Ä‘áº¡o hÃ m theo $\theta$, ta ra Ä‘Æ°á»£c nghiá»‡m chÃ­nh xÃ¡c tÆ°Æ¡ng tá»±
        
        $$
        \begin{aligned}
        & \frac{\partial J}{\partial \theta}=2 X^T X \theta-2 X^T y=0 \\
        & X^T X \theta=X^T y \\
        & \theta=\left(X^T X\right)^{-1} X^T y
        \end{aligned}
        $$
        
- **TrÃªn lÃ½ thuyáº¿t Ä‘á»ƒ tÃ¬m min/max cá»§a má»™t hÃ m sá»‘ ngÆ°á»i ta thÆ°á»ng lÃ m gÃ¬?**
    - TÃ­nh Ä‘áº¡o hÃ m
- **Viá»‡c tÃ­nh Ä‘áº¡o hÃ m báº­c 1 Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm minimum trong Linear Regression cÃ³ Ä‘á»§ khÃ´ng?**
    
    Trong trÆ°á»ng há»£p tá»•ng quÃ¡t, viá»‡c tÃ­nh Ä‘áº¡o hÃ m báº­c 1 vÃ  cho nÃ³ = 0 Ä‘á»ƒ tÃ¬m nghiá»‡m chá»‰ lÃ  Ä‘iá»u kiá»‡n cáº§n. Má»™t Ä‘iá»ƒm cÃ³ Ä‘áº¡o hÃ m báº±ng 0 vá»«a cÃ³ thá»ƒ lÃ  minimum vá»«a lÃ  maximum.
    
    Äá»ƒ tÃ¬m minimum cá»§a má»™t hÃ m sá»‘ báº¥t kÃ¬ tháº­t ra ta pháº£i tÃ­nh thÃªm Ä‘áº¡o hÃ m báº­c 2.
    
    Tuy nhiÃªn do hÃ m má»¥c tiÃªu cá»§a Linear Regression aka MSE lÃ  hÃ m Convex. TÃ­nh cháº¥t cá»§a hÃ m nÃ y lÃ  khÃ´ng cÃ³ maximum mÃ  chá»‰ cÃ³ minimum vÃ  cÅ©ng lÃ  global minimum. Do Ä‘Ã³ chá»‰ cáº§n tÃ­nh Ä‘áº¡o hÃ m báº­c 1 lÃ  Ä‘á»§.
    
    ![[https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3)](image%2011.png)
    
    [https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3)
    

<aside>
ğŸ“Œ

LÆ°u Ã½ ráº±ng Ä‘áº¡o hÃ m á»Ÿ Ä‘Ã¢y lÃ  tÃ­nh theo cÃ´ng thá»©c cÃ³ sáºµn, vÃ­ dá»¥ nhÆ° $(x^2)' = 2x$ chá»© khÃ´ng pháº£i nhÆ° trong mÃ¡y tÃ­nh.

</aside>

- **Äiá»ƒm yáº¿u cá»§a viá»‡c tÃ­nh Ä‘áº¡o hÃ m Ä‘á»ƒ giáº£i nghiá»‡m chÃ­nh xÃ¡c lÃ  gÃ¬? Táº¡i sao trong ML ngta khÃ´ng dÃ¹ng Ä‘áº¡o hÃ m Ä‘á»ƒ giáº£i nghiá»‡m chÃ­nh xÃ¡c?**
    1. KhÃ´ng thá»ƒ tÃ­nh toÃ¡n trong cÃ¡c hÃ m sá»‘ khÃ´ng kháº£ vi (non-differentiable problems).
    2. Hoáº·c lÃ  giáº£ sá»­ mÃ´ hÃ¬nh cÃ³ cÃ´ng thá»©c quÃ¡ phá»©c táº¡p â†’ khÃ´ng cÃ³ cÃ´ng thá»©c hoáº·c quÃ¡ phá»©c táº¡p Ä‘á»ƒ Ä‘áº¡o hÃ m.
    3. Äáº¡o hÃ m báº­c hai khÃ³ tÃ­nh vÃ  tÃ­nh siÃªu lÃ¢u.
    4. Náº¿u cÃ³ xáº£y ra sá»± phá»¥ thuá»™c tuyáº¿n tÃ­nh â†’ khÃ´ng tá»“n táº¡i nghiá»‡m chÃ­nh xÃ¡c.
    5. CÃ¡ch tÃ­nh Ä‘áº¡o hÃ m thá»§ cÃ´ng theo cÃ´ng thá»©c phá»¥ thuá»™c vÃ o biá»ƒu thá»©c cá»¥ thá»ƒ cá»§a hÃ m sá»‘. Khi hÃ m má»¥c tiÃªu thay Ä‘á»•i hoáº·c cÃ³ thÃªm biáº¿n, cÃ¡c cÃ´ng thá»©c nÃ y pháº£i Ä‘Æ°á»£c tÃ­nh láº¡i má»™t cÃ¡ch thá»§ cÃ´ng, Ä‘iá»u nÃ y khÃ´ng linh hoáº¡t trong cÃ¡c há»‡ thá»‘ng Machine Learning.
- **Polynomial Regression lÃ  gÃ¬? CÃ´ng thá»©c?**
    
    **Polynomial Regression** lÃ  má»™t dáº¡ng má»Ÿ rá»™ng cá»§a **Linear Regression**, trong Ä‘Ã³ má»‘i quan há»‡ giá»¯a biáº¿n Ä‘á»™c láº­p $x$ vÃ  biáº¿n phá»¥ thuá»™c $y$ khÃ´ng chá»‰ lÃ  má»™t Ä‘Æ°á»ng tháº³ng, mÃ  lÃ  má»™t hÃ m **Ä‘a thá»©c** (polynomial) báº­c cao. 
    
    Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh há»“i quy cÃ³ thá»ƒ fit tá»‘t hÆ¡n vá»›i cÃ¡c dá»¯ liá»‡u phi tuyáº¿n, báº±ng cÃ¡ch thÃªm cÃ¡c báº­c cao hÆ¡n cá»§a biáº¿n Ä‘á»™c láº­p.
    
    $$
    y = w_0 + w_1x + w_2x^2 + w_3x^3 + w_4x^4 + w_5x^5
    $$
    
- **Tuyáº¿n tÃ­nh trong Linear Regression Ä‘á»ƒ chá»‰ Ä‘iá»u gÃ¬? Polynomial Regression cÃ³ Ä‘Æ°á»£c gá»i lÃ  tuyáº¿n tÃ­nh khÃ´ng?**
    
    **Tuyáº¿n tÃ­nh trong Linear Regression** khÃ´ng chá»‰ Ä‘áº¿n viá»‡c sá»‘ mÅ© cá»§a cÃ¡c biáº¿n Ä‘á»™c láº­p, mÃ  thá»±c ra nÃ³ chá»‰ ráº±ng **má»‘i quan há»‡ giá»¯a cÃ¡c tham sá»‘ (há»‡ sá»‘ há»“i quy) vÃ  biáº¿n phá»¥ thuá»™c lÃ  tuyáº¿n tÃ­nh**. NghÄ©a lÃ  Ä‘ang xÃ©t Ä‘áº¿n má»‘i quan há»‡ giá»¯a $\theta$ vÃ  $y$.
    
    Äiá»u nÃ y khÃ¡c vá»›i má»‘i quan há»‡ giá»¯a $x$ vÃ  $y$.
    
    Do váº­y nÃªn **Polynomial Regression váº«n Ä‘Æ°á»£c gá»i lÃ  Linear Regression**.
    
    Váº­y chá»‰ khi ta Ä‘á»•i $\theta$ sang $\theta^2$ hoáº·c $\sin(\theta)$ thÃ¬ khi Ä‘Ã³ má»›i gá»i lÃ  Há»“i quy phi tuyáº¿n.
    
- **Náº¿u tÄƒng sá»‘ báº­c cá»§a $x \to x^2$ thÃ¬ cÃ³ cÃ²n Ä‘Æ°á»£c gá»i lÃ  Linear Regression khÃ´ng?**
    
    CÃ²n, váº«n Ä‘Æ°á»£c xem lÃ  Linear Regression.
    
- **Giáº£i thÃ­ch Global minimum vÃ  local minimum**
    
    Global : minimum nhá» nháº¥t trong toÃ n bá»™ minimum
    
    Local : chá»‰ cáº§n lÃ  minimum
    
    ![Untitled](Untitled%203.png)
    
- **Khi thá»±c hiá»‡n Linear Regression vá»›i báº­c Ä‘a thá»©c lÃ  báº­c 1 vÃ  chá»‰ cÃ³ 1 feature duy nháº¥t thÃ¬ hÃ m loss cÃ³ xuáº¥t hiá»‡n local minimum khÃ´ng? Táº¡i sao?**
    - CÃ³ duy nháº¥t 1 thg minimum, vá»«a lÃ  local vá»«a lÃ  global minimum
- **Khi thá»±c hiá»‡n Linear Regression vá»›i báº­c Ä‘a thá»©c â‰¥ 1, vÃ  cÃ³ nhiá»u hÆ¡n 1 feature thÃ¬ cÃ³ xuáº¥t hiá»‡n local minimum khÃ´ng? Táº¡i sao?**
    
    $$
    h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2+...
    $$
    
    - KhÃ´ng luÃ´n, vÃ¬ báº­c cá»§a Ä‘a thá»©c lÃ  báº­c 1.
    - Local minimum chá»‰ xáº£y ra náº¿u ta tÄƒng báº­c cá»§a $\theta$ dáº«n Ä‘áº¿n nhiá»u nghiá»‡m xáº£y ra, khÃ´ng liÃªn quan Ä‘áº¿n viá»‡c tÄƒng báº­c cá»§a $x$.
    
    ![Untitled](Untitled%204.png)
    

<aside>
ğŸ“Œ

Váº­y do hÃ m má»¥c tiÃªu (MSE) cá»§a chÃºng ta lÃ  hÃ m convex, do Ä‘Ã³ bÃ i toÃ n Linear Regression náº¿u cÃ³ nghiá»‡m thÃ¬ nÃ³ luÃ´n cÃ³ 1 nghiá»‡m duy nháº¥t vÃ  lÃ  global minimum. 

</aside>

<aside>
ğŸ“Œ

Ta chá»‰ gáº·p hiá»‡n tÆ°á»£ng local minimum khi sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n cÃ³ Ä‘á»™ phá»©c táº¡p cao, sá»­ dá»¥ng cÃ¡c hÃ m phi tuyáº¿n lÃªn trá»ng sá»‘ $w$ aka Deep Neural Network.

</aside>

- **Trong ML Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm tá»‘i Æ°u ngÆ°á»i ta dÃ¹ng thuáº­t toÃ¡n gÃ¬? Giáº£i thÃ­ch tÃªn gá»i cá»§a thuáº­t toÃ¡n Ä‘Ã³. Táº¡i sao láº¡i cÃ³ dáº¥u `(-)` trong cÃ´ng thá»©c?**
    
    ![Untitled](Untitled%205.png)
    
    <aside>
    ğŸ“Œ
    
    Gradient descent : ngÆ°á»£c chiá»u vector Ä‘áº¡o hÃ m riÃªng
    
    </aside>
    
    $\theta_j$ luÃ´n chá»‰ vá» hÆ°á»›ng tÄƒng táº¡i vÃ¬ Ä‘áº¡o hÃ m nghÄ©a lÃ  quÃ£ng Ä‘Æ°á»ng (Ä‘á»™ dá»i) Ä‘i Ä‘Æ°á»£c trong má»™t Ä‘Æ¡n vá»‹ (thá»i gian /  khoáº£ng cÃ¡ch $\varepsilon$)
    
    <aside>
    ğŸ“Œ
    
    Do Ä‘Ã³, báº¡n Ä‘á»ƒ Ã½ trong cÃ´ng thá»©c sáº½ cÃ³ $J(\theta|\theta_j +\varepsilon)$ chá»© khÃ´ng pháº£i $J(\theta|\theta_j -\varepsilon)$.
    
    Báº¡n Ä‘á»ƒ Ã½ trá»¥c hoÃ nh cá»™ng vÃ o $\theta_j +\varepsilon$ â†’ tÄƒng lÃªn â†’ Ä‘i theo chiá»u dÆ°Æ¡ng
    
    </aside>
    
    Má»¥c tiÃªu cá»§a mÃ¬nh lÃ  tÄƒng hoáº·c giáº£m $\theta$ sao cho hÃ m loss $J$ Ä‘áº¡t Ä‘iá»ƒm cá»±c tiá»ƒu.
    
    Váº­y táº¡i má»™t giÃ¡ trá»‹ $\theta$ báº¥t kÃ¬, ta sáº½ cÃ³ 2 trÆ°á»ng há»£p:
    
    ![Untitled](Untitled%206.png)
    
    TrÆ°á»ng há»£p 1: 
    
    ![Untitled](47b7f64a-74e1-423c-ad07-e20493af6efe.png)
    
    - Äiá»ƒm mÃ u Ä‘á» â†’ giÃ¡ trá»‹ loss $J$ táº¡i Ä‘iá»ƒm $\theta$
    - Äiá»ƒm mÃ u xanh â†’ giÃ¡ trá»‹ loss $J$ táº¡i Ä‘iá»ƒm $\theta + \varepsilon$
    
    <aside>
    ğŸ“Œ
    
    Báº¡n Ä‘á»ƒ Ã½ trá»¥c tung $J(\theta|\theta_j + \varepsilon)$ lá»›n hÆ¡n $J(\theta|\theta_j)$ aka Ä‘iá»ƒm mÃ u xanh cÃ³ giÃ¡ trá»‹ lá»›n hÆ¡n Ä‘iá»ƒm mÃ u Ä‘Ã³.
    
    NghÄ©a lÃ  khi ta tÄƒng giÃ¡ trá»‹ trá»¥c hoÃ nh $\theta$  lÃªn má»™t Ä‘oáº¡n $\varepsilon$ â†’ loss tÄƒng
    
    Do Ä‘Ã³ ta pháº£i giáº£m giÃ¡ trá»‹ cá»§a $\theta$  tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i Ä‘i ngÆ°á»£c láº¡i chiá»u Ä‘ang tÄƒng cá»§a Ä‘áº¡o hÃ m loss $J$. 
    
    </aside>
    
    TH2 : 
    
    ![Untitled](80b3833c-387c-435a-94ce-f6adcbd4ef5a.png)
    
    TÆ°Æ¡ng tá»±, Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m thÃ¬ ta pháº£i tÃ­nh $J(\theta|\theta_j + \varepsilon) - J(\theta|\theta_j)$.
    
    - Äiá»ƒm mÃ u Ä‘á» â†’ giÃ¡ trá»‹ loss $J$ táº¡i Ä‘iá»ƒm $\theta$
    - Äiá»ƒm mÃ u xanh â†’ giÃ¡ trá»‹ loss $J$ táº¡i Ä‘iá»ƒm $\theta + \varepsilon$
    
    <aside>
    ğŸ“Œ
    
    Láº¥y Ä‘iá»ƒm mÃ u xanh - Ä‘iá»ƒm mÃ u Ä‘á» â†’ Ä‘áº¡o hÃ m < 0.
    
    Tuy nhiÃªn nÃ³ Ä‘ang Ä‘i Ä‘Ãºng hÆ°á»›ng, Ä‘ang giáº£m loss, do Ä‘Ã³ ta cáº§n tiáº¿p tá»¥c tÄƒng $\theta$
    
    ThÃªm dáº¥u trá»« vÃ o Ä‘á»ƒ Ä‘áº¡o hÃ m nÃ³ dÆ°Æ¡ng.
    
    </aside>
    
- **Khi tÃ­nh Ä‘áº¡o hÃ m cho nhiá»u biáº¿n, cÃ¡ch ngÆ°á»i ta lÃ m lÃ  gÃ¬?**
    
    Khi tÃ­nh Ä‘áº¡o hÃ m cho cÃ¡c hÃ m cÃ³ **nhiá»u biáº¿n** (multivariate functions), ngÆ°á»i ta sá»­ dá»¥ng **Ä‘áº¡o hÃ m riÃªng (partial derivatives)**. Äáº¡o hÃ m riÃªng cho phÃ©p tÃ­nh tá»‘c Ä‘á»™ thay Ä‘á»•i cá»§a hÃ m sá»‘ theo tá»«ng biáº¿n trong khi giá»¯ cÃ¡c biáº¿n cÃ²n láº¡i cá»‘ Ä‘á»‹nh. 
    
    ![[https://www.kristakingmath.com/blog/second-order-partial-derivatives](https://www.kristakingmath.com/blog/second-order-partial-derivatives)](image%2012.png)
    
    [https://www.kristakingmath.com/blog/second-order-partial-derivatives](https://www.kristakingmath.com/blog/second-order-partial-derivatives)
    
- **Äá»ƒ tÃ­nh Ä‘áº¡o hÃ m riÃªng cho hÃ m sá»‘ thÃ¬ má»™t sá»‘ cÃ¡ch ngÆ°á»i ta sá»­ dá»¥ng trong ML lÃ  gÃ¬?**
    1. Chain Rule
        
        ![[https://www.magemma.com/?ggcid=2065960](https://www.magemma.com/?ggcid=2065960)](image%2013.png)
        
        [https://www.magemma.com/?ggcid=2065960](https://www.magemma.com/?ggcid=2065960)
        
    2. CÃ¡c ká»¹ thuáº­t autodiff (automatic differentiation). 
        
        ![[https://pub.towardsai.net/a-gentle-introduction-to-automatic-differentiation-74e7eb9a75af](https://pub.towardsai.net/a-gentle-introduction-to-automatic-differentiation-74e7eb9a75af)](image%2014.png)
        
        [https://pub.towardsai.net/a-gentle-introduction-to-automatic-differentiation-74e7eb9a75af](https://pub.towardsai.net/a-gentle-introduction-to-automatic-differentiation-74e7eb9a75af)
        
- **Váº­y cÃ³ "Ä‘áº¡o hÃ m chung" cho toÃ n bá»™ cÃ¡c biáº¿n khÃ´ng?**
    
    <aside>
    ğŸ˜€
    
    Náº¿u báº¡n biáº¿t cÃ¡ch "Ä‘áº¡o hÃ m chung" cho toÃ n bá»™ cÃ¡c biáº¿n thÃ¬ cháº¯c 10 tháº¿ ká»· sau báº¡n váº«n cÃ²n Ä‘Æ°á»£c tÃ´n vinh.
    
    </aside>
    
    Tháº­t ra Hessian Matrix cÅ©ng lÃ  má»™t cÃ¡ch Ä‘á»ƒ xem xÃ©t má»‘i quan há»‡ Ä‘áº¡o hÃ m giá»¯a cÃ¡c biáº¿n vá»›i nhau.  Hessian giÃºp cung cáº¥p thÃ´ng tin vá» Ä‘á»™ cong cá»§a hÃ m sá»‘ trong khÃ´ng gian Ä‘a chiá»u.
    
    VÃ­ dá»¥ Ä‘á»‘i vá»›i hÃ m $f(x,y,z)$, Hessian sáº½ lÃ :
    
    $$
    f(x,y,z) = H(f)=\left(\begin{array}{lll}
    \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} & \frac{\partial^2 f}{\partial x \partial z} \\
    \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} & \frac{\partial^2 f}{\partial y \partial z} \\
    \frac{\partial^2 f}{\partial z \partial x} & \frac{\partial^2 f}{\partial z \partial y} & \frac{\partial^2 f}{\partial z^2}
    \end{array}\right)
    $$
    
    Hessian thá»ƒ hiá»‡n sá»± thay Ä‘á»•i cá»§a hÃ m theo má»i biáº¿n vÃ  má»i cáº·p biáº¿n.
    
- **Äiá»ƒm yáº¿u cá»§a viá»‡c Ä‘áº¡o hÃ m theo tá»«ng biáº¿n lÃ  gÃ¬?**
    
    [Who's Adam and What's He Optimizing? | Deep Dive into Optimizers for Machine Learning!](https://www.youtube.com/watch?v=MD2fYip6QsQ&t=801s&pp=ygUVYWRhbSBncmFkaWVudCBkZXNjZW50)
    
- **Giáº£i thÃ­ch learning-rate**
    
    **Learning rate** (tá»‘c Ä‘á»™ há»c) lÃ  má»™t **siÃªu tham sá»‘** quan trá»ng trong cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a, Ä‘áº·c biá»‡t lÃ  cÃ¡c thuáº­t toÃ¡n nhÆ° **Gradient Descent** vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³. Learning rate kiá»ƒm soÃ¡t **kÃ­ch thÆ°á»›c cá»§a bÆ°á»›c nháº£y** mÃ  mÃ´ hÃ¬nh thá»±c hiá»‡n trong khÃ´ng gian tham sá»‘ má»—i láº§n cáº­p nháº­t Ä‘á»ƒ giáº£m hÃ m má»¥c tiÃªu (hÃ m máº¥t mÃ¡t).
    
    ![[https://www.jeremyjordan.me/nn-learning-rate/](https://www.jeremyjordan.me/nn-learning-rate/)](image%2015.png)
    
    [https://www.jeremyjordan.me/nn-learning-rate/](https://www.jeremyjordan.me/nn-learning-rate/)
    
- **Náº¿u learning-rate quÃ¡ lá»›n thÃ¬ sao? QuÃ¡ bÃ© thÃ¬ sao?**
    
    Náº¿u learning rate **quÃ¡ nhá»**, quÃ¡ trÃ¬nh há»c sáº½ diá»…n ra **cháº­m**. MÃ´ hÃ¬nh cáº§n ráº¥t nhiá»u vÃ²ng láº·p Ä‘á»ƒ há»™i tá»¥, dáº«n Ä‘áº¿n thá»i gian huáº¥n luyá»‡n lÃ¢u vÃ  cÃ³ thá»ƒ gÃ¢y ra hiá»‡n tÆ°á»£ng máº¯c káº¹t táº¡i cÃ¡c cá»±c trá»‹ cá»¥c bá»™.
    
    Náº¿u learning rate **quÃ¡ lá»›n**, mÃ´ hÃ¬nh cÃ³ thá»ƒ **nháº£y quÃ¡ xa** qua láº¡i giá»¯a cÃ¡c Ä‘iá»ƒm mÃ  khÃ´ng thá»±c sá»± giáº£m giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t. Äiá»u nÃ y cÃ³ thá»ƒ lÃ m cho mÃ´ hÃ¬nh khÃ´ng há»™i tá»¥, hoáº·c tá»‡ hÆ¡n, dáº«n Ä‘áº¿n sá»± dao Ä‘á»™ng khÃ´ng á»•n Ä‘á»‹nh trong giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t.
    
    ![Untitled](Untitled%207.png)
    
- **Ban Ä‘áº§u nÃªn chá»n learning-rate lá»›n hay bÃ©?**
    
    Ban Ä‘áº§u nÃªn chá»n lá»›n, Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ há»™i tá»¥ + vÆ°á»£t qua local minimum.
    
    - Trong thá»±c táº¿, ngÆ°á»i ta thÆ°á»ng thá»­ nghiá»‡m vá»›i nhiá»u giÃ¡ trá»‹ learning rate khÃ¡c nhau Ä‘á»ƒ tÃ¬m ra giÃ¡ trá»‹ tá»‘t nháº¥t cho bÃ i toÃ¡n cá»¥ thá»ƒ.
- **Ká»ƒ tÃªn má»™t sá»‘ phÆ°Æ¡ng phÃ¡p thay Ä‘á»•i learning-rate trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh**
    
    CÃ³ thá»ƒ giáº£m lr theo lá»™ trÃ¬nh (schedule), nghÄ©a lÃ  ban Ä‘áº§u lá»›n, sau Ä‘Ã³ giáº£m dáº§n. Ta sáº½ quyáº¿t Ä‘á»‹nh tá»‘c Ä‘á»™ giáº£m báº±ng má»™t sá»‘ cÃ¡ch nhÆ°:
    
    - LinearDecay : giáº£m 1 cÃ¡ch tuyáº¿n tÃ­nh
    - ExponentialDecay : giáº£m theo hÃ m mÅ©
    - NgoÃ i ra cÃ³ thá»ƒ chá»n cÃ¡c bá»™ tá»‘i Æ°u tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh lr nhÆ°: Adam, LBFGS, ISTA, FISTA, FISTA restart.
- **Nháº¯c láº¡i vá» bÃ i toÃ¡n tÃ­nh tiá»n taxi vÃ  trá»ng sá»‘ cá»§a hÃ m Há»“i quy tuyáº¿n tÃ­nh**
    
    ***VÃ­ dá»¥***: Chá»‰ cáº§n báº¡n thuÃª xe taxi vÃ  bÆ°á»›c chÃ¢n lÃªn xe, ngÆ°á»i ta sáº½ tÃ­nh tiá»n báº¡n trÆ°á»›c má»™t khoáº£ng phÃ­, vÃ­ dá»¥ 5000. VÃ  sau Ä‘Ã³ 12.500Ä‘ cho má»—i km di chuyá»ƒn. Váº­y hÃ m sá»‘ mÃ´ táº£ cho viá»‡c Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ° sau
    
    $$
    y = 5000 + 12500x
    $$
    
    Vá»›i $y$ lÃ  sá»‘ tiá»n pháº£i tráº£ vÃ  $x$ lÃ  sá»‘ km báº¡n Ä‘Ã£ Ä‘i.
    
    <aside>
    ğŸ’¡ Váº­y báº£n cháº¥t cá»§a Há»“i quy tuyáº¿n tÃ­nh lÃ  Ä‘i tÃ¬m cÃ¡c con sá»‘ $\theta_0 = 5000$ vÃ  $\theta_1 = 12500$ nÃ y, nghÄ©a lÃ  nÃ³ cÃ³ Ã½ nghÄ©a nÃ o Ä‘Ã³ vá»›i thá»±c táº¿. Náº¿u báº¡n tÃ¬m ra $\theta_1 = 500$ Ä‘á»“ng thÃ¬ nÃªn kiá»ƒm tra láº¡i
    
    1. Thuáº­t toÃ¡n bá»‹ sai, khÃ´ng thá»ƒ nÃ o taxi tÃ­nh tiá»n 1 km di chuyá»ƒn vá»›i giÃ¡ 500 Ä‘á»“ng.
    2. $x$ khÃ´ng thá»ƒ hiá»‡n cho sá»‘ km di chuyá»ƒn mÃ  Ä‘áº¡i diá»‡n cho má»™t cÃ¡i gÃ¬ Ä‘Ã³ khÃ¡c
    3. ÄÆ¡n vá»‹ cá»§a $x$ khÃ´ng pháº£i lÃ  1 km mÃ  cÃ³ thá»ƒ lÃ  100 m hoáº·c 500 m.
    </aside>
    
- **Tá»« vÃ­ dá»¥ trÃªn, hÃ£y giáº£i thÃ­ch workflow sau Ä‘Ã¢y**
    
    1. 
    
    ![Untitled](Untitled%208.png)
    
    1. Gradient Descent : greedy
        
        ![Untitled](Untitled%209.png)
        
    
    1. 
        
        ![Untitled](Untitled%2010.png)
        
    2. 
    
    ![Untitled](Untitled%2011.png)
    
    1. 
        
        ![Untitled](Untitled%2012.png)
        
    2. 
        
        ![Untitled](Untitled%2013.png)
        
    3. 
        
        ![Untitled](Untitled%2014.png)
        
        1. 
            
            ![Untitled](Untitled%2015.png)
            
        2. 
        
        ![Untitled](Untitled%2016.png)
        
- **CÃ³ máº¥y phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n dá»¯ liá»‡u theo batch? Ká»ƒ tÃªn, giáº£i thÃ­ch tá»«ng cÃ¡i, nÃªu Æ°u Ä‘iá»ƒm, khuyáº¿t Ä‘iá»ƒm?**
    - SGD : chá»n 1 Ä‘iá»ƒm random
    - Batch (Full-batch) : cáº­p nháº­t dá»±a trÃªn toÃ n bá»™ Ä‘iá»ƒm dá»¯ liá»‡u
    - Mini-Batch : chá»n sample ngáº«u nhiÃªn
- **Má»¥c tiÃªu cá»§a chÃºng ta lÃ  cáº£i thiá»‡n hiá»‡u suáº¥t (performance) cá»§a thuáº­t toÃ¡n, tá»« â€œhiá»‡u suáº¥tâ€ á»Ÿ Ä‘Ã¢y bao gá»“m nhá»¯ng yáº¿u tá»‘ nÃ o?**
    - Time
    - Computional cost/store
    - Accuracy
    - â€¦
- **Äiá»u kiá»‡n dá»«ng cá»§a GD lÃ  gÃ¬?**
    - Gradient ráº¥t nhá».
    - Sá»‘ vÃ²ng láº·p tá»‘i Ä‘a Ä‘áº¡t Ä‘Æ°á»£c.
    - Thay Ä‘á»•i giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t nhá».
    - Thay Ä‘á»•i gradient nhá».
    - Thá»i gian huáº¥n luyá»‡n tá»‘i Ä‘a.
    - â€¦
- **Má»—i láº§n cháº¡y thuáº­t toÃ¡n tÃ¬m kiáº¿m Ä‘iá»ƒm tá»‘i Æ°u, lÃ m sao Ä‘á»ƒ ta biáº¿t Ä‘iá»ƒm Ä‘Ã³ cÃ³ pháº£i lÃ  global minimum hay khÃ´ng?**
    
    > â€œYou can not say this is the best solution, you can just say this is the best that you can doâ€ - Prof. Haddou :)))
    > 
    
    Má»™t sá»‘ cÃ¡ch cÃ³ thá»ƒ ká»ƒ Ä‘áº¿n nhÆ°:
    
    1. Kiá»ƒm tra tÃ­nh lá»“i (Convexity)
    2. Cháº¡y thuáº­t toÃ¡n nhiá»u láº§n vá»›i cÃ¡c cÃ¡c bá»™ tham sá»‘ khá»Ÿi táº¡o khÃ¡c nhau (random initialization). Chá»n giÃ¡ trá»‹ tá»‘i Æ°u nháº¥t hoáº·c náº¿u nhiá»u Ä‘iá»ƒm khá»Ÿi Ä‘áº§u khÃ¡c nhau Ä‘á»u há»™i tá»¥ vá» cÃ¹ng má»™t Ä‘iá»ƒm tá»‘i Æ°u, kháº£ nÄƒng cao Ä‘Ã³ lÃ  global minimum.
    3. **Hessian matrix** lÃ  ma tráº­n cá»§a cÃ¡c Ä‘áº¡o hÃ m báº­c hai cá»§a hÃ m sá»‘. Náº¿u Hessian lÃ  **dÆ°Æ¡ng xÃ¡c Ä‘á»‹nh (positive definite)** táº¡i Ä‘iá»ƒm cá»±c tiá»ƒu, tá»©c lÃ  má»i giÃ¡ trá»‹ riÃªng cá»§a Hessian Ä‘á»u dÆ°Æ¡ng, thÃ¬ Ä‘iá»ƒm Ä‘Ã³ lÃ  **global minimum**.
    
    Tham kháº£o thÃªm 
    
    [Second partial derivative test](https://en.wikipedia.org/wiki/Second_partial_derivative_test)
    
- **Hyper-parameter (siÃªu tham sá»‘) & parameter (tham sá»‘) lÃ  gÃ¬?**
    
    Hyper-param (siÃªu tham sá»‘) : nhá»¯ng biáº¿n khÃ´ng thay Ä‘á»•i hoáº·c thay Ä‘á»•i ráº¥t Ã­t trong suá»‘t quÃ¡ trÃ¬nh training. VÃ­ dá»¥: learning-rate
    
    Param (tham sá»‘) : nhá»¯ng biáº¿n luÃ´n thay Ä‘á»•i. VÃ­ dá»¥ : $\theta$ (weights, trá»ng sá»‘)
    
- **Giáº£i thÃ­ch overfitting, underfitting?**
    - Overfit : tá»‘t trÃªn táº­p train, khÃ´ng tá»‘t trÃªn táº­p test
    - Underfit : khÃ´ng tá»‘t trÃªn táº­p train láº«n táº­p test
    
    overfit â†’ giáº£m Ä‘á»™ phá»©c táº¡p = regularization/penalty â†’ Lasso/Ridge Regression
    
    underfit â†’ tÄƒng Ä‘á»™ phá»©c táº¡p = non-linear regression , Deep Learning
    
- **KhÃ¡c nhau giá»¯a táº­p train, valid vÃ  test**
    
    Train : huáº¥n luyá»‡n mÃ´ hÃ¬nh
    
    Test : Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t tá»•ng quÃ¡t
    
    Valid : trÃ¡nh leak thÃ´ng tin cá»§a táº­p test, dÃ¹ng Ä‘á»ƒ tinh chá»‰nh cÃ¡c siÃªu tham sá»‘
    
- **Scaling lÃ  gÃ¬?**
    
    **Scaling** (chuáº©n hÃ³a dá»¯ liá»‡u) lÃ  quÃ¡ trÃ¬nh *Ä‘iá»u chá»‰nh giÃ¡ trá»‹* cá»§a cÃ¡c Ä‘áº·c trÆ°ng (features) dá»¯ liá»‡u *sao cho chÃºng náº±m trong má»™t pháº¡m vi nháº¥t Ä‘á»‹nh*. Má»¥c tiÃªu cá»§a viá»‡c scaling lÃ :
    
    - LÃ m cho cÃ¡c Ä‘áº·c trÆ°ng cÃ³ Ä‘á»™ Ä‘o khÃ¡c nhau vá» cÃ¹ng Ä‘á»™ Ä‘o hoáº·c giáº£m sá»± sai lá»‡ch vá» Ä‘á»™ Ä‘o.
    - Giáº£m Ä‘á»™ lá»›n/Ä‘á»™ nhá» cá»§a giÃ¡ trá»‹, trÃ¡nh hiá»‡n tÆ°á»£ng exploding/vanishing gradient.
    1. **Min-Max Scaling**:
        - Chuyá»ƒn cÃ¡c giÃ¡ trá»‹ vá» má»™t khoáº£ng xÃ¡c Ä‘á»‹nh (thÆ°á»ng lÃ  [0, 1]).
        
        $$
        \mathrm{x}_{\text {scaled }}=\frac{\mathrm{x}-{x}_{\min }}{{x}_{\max }-{x}_{\min }}
        $$
        
    - PhÃ¹ há»£p cho cÃ¡c thuáº­t toÃ¡n khÃ´ng giáº£ Ä‘á»‹nh phÃ¢n phá»‘i dá»¯ liá»‡u, nhÆ° K-means clustering vÃ  Neural Networks.
    1. **Standardization (Z-score Normalization)**:
        - ÄÆ°a trung bÃ¬nh vá» 0 vÃ  Ä‘á»™ lá»‡ch chuáº©n báº±ng 1.
        
        $$
        \mathrm x_{\text {scaled}}=\frac{\mathrm x-\mu}{\sigma}
        $$
        
        - PhÃ¹ há»£p cho cÃ¡c thuáº­t toÃ¡n giáº£ Ä‘á»‹nh dá»¯ liá»‡u cÃ³ phÃ¢n phá»‘i chuáº©n (Gaussian), nhÆ° Linear Regression vÃ  Logistic Regression.
    2. **Robust Scaling**:
    - Sá»­ dá»¥ng khi dataset cÃ³ nhiá»u giÃ¡ trá»‹ ngoáº¡i lai.
    - CÃ´ng thá»©c tÃ­nh dá»±a trÃªn giÃ¡ trá»‹ trung vá»‹ (median) vÃ  khoáº£ng tá»© phÃ¢n vi (interquartile range).
    
    $$
    \mathrm x_{\text {scaled }}=\frac{\mathrm x-\text {median}(\mathrm x)}{I Q R}
    
    $$
    
- **LiÃªn káº¿t viá»‡c scaling cáº£i thiá»‡n GD vá»›i cÃ¢u há»i trÆ°á»›c Ä‘Ã³ â€œÄiá»ƒm yáº¿u cá»§a viá»‡c Ä‘áº¡o hÃ m theo tá»«ng biáº¿n lÃ  gÃ¬?â€**
    
    ![Untitled](Untitled%2017.png)
    
- **PhÃ¢n biá»‡t Standardization vÃ  Normalization**
    
    Normalization Ä‘á»ƒ gá»i chung cÃ¡c phÆ°Æ¡ng phÃ¡p chuáº©n hÃ³a. Standardize lÃ  má»™t phÆ°Æ¡ng phÃ¡p cá»¥ thá»ƒ (Ä‘Æ°a vá» phÃ¢n phá»‘i chuáº©n táº¯c)
    
- **Khi thá»±c hiá»‡n normalize thÃ¬ ta cáº§n lÆ°u Ã½ Ä‘iá»u gÃ¬?**
    - KhÃ´ng normalize trÆ°á»›c khi split train-test Ä‘á»ƒ trÃ¡nh leak thÃ´ng tin cá»§a táº­p test khi normalize
    1. Split train-test
    2. Normalize táº­p train â†’ tÃ¬m ra trung bÃ¬nh, phÆ°Æ¡ng sai táº­p train
    3. Ãp dá»¥ng normalize vá»›i trung bÃ¬nh vÃ  phÆ°Æ¡ng sai Ä‘Ã³ cho táº­p test
- **Táº¡i sao ngta cÃ³ thá»ƒ thÃªm noise vÃ o táº­p train nhÆ°ng khÃ´ng thÃªm vÃ o táº­p test?**
    
    Má»¥c Ä‘Ã­ch thÃªm noise vÃ o táº­p train lÃ  Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c má»™t cÃ¡ch tá»•ng quÃ¡t hÆ¡n, trÃ¡nh bá»‹ overfit.
    
    Trong thá»±c táº¿ dá»¯ liá»‡u cÅ©ng sáº½ cÃ³ nhiá»u noise.
    
    ThÃªm vÃ o táº­p test khÃ´ng cÃ³ Ã½ nghÄ©a, vÃ¬ nÃ³ cÅ©ng chá»‰ nháº±m Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t, khÃ´ng dÃ¹ng Ä‘á»ƒ train.
    
- **Regularization lÃ  gÃ¬, nÃ³ cÃ³ tÃªn gá»i nÃ o khÃ¡c? Ká»ƒ tÃªn má»™t sá»‘ phÆ°Æ¡ng phÃ¡p Regression sá»­ dá»¥ng Regularization**
    
    **Regularization** hay **Penalty**  Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t ká»¹ thuáº­t giÃºp cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a (generalization) cá»§a mÃ´ hÃ¬nh.
    
    Regularization lÃ  má»™t phÆ°Æ¡ng phÃ¡p thÃªm má»™t rÃ ng buá»™c vÃ o hÃ m máº¥t mÃ¡t (loss function) Ä‘á»ƒ buá»™c mÃ´ hÃ¬nh pháº£i há»c cÃ¡c tham sá»‘ nhá» hÆ¡n hoáº·c Ã­t phá»©c táº¡p hÆ¡n. Äiá»u nÃ y giÃºp trÃ¡nh viá»‡c mÃ´ hÃ¬nh há»c thuá»™c (memorizing) dá»¯ liá»‡u thay vÃ¬ há»c cÃ¡ch tá»•ng quÃ¡t hÃ³a dá»¯ liá»‡u má»›i. Regularization thÃªm má»™t ğ—½ğ—²ğ—»ğ—®ğ—¹ğ˜ğ˜† vÃ o hÃ m máº¥t mÃ¡t, nháº±m ngÄƒn viá»‡c cÃ¡c há»‡ sá»‘ há»“i quy cÃ³ giÃ¡ trá»‹ quÃ¡ lá»›n.
    
    $$
    J_\theta(x) = \text{MSE} + \lambda_2\text{Regularization}
    $$
    
    ğŸ­. $\mathcal L_1$ ğ—¥ğ—²ğ—´ğ˜‚ğ—¹ğ—®ğ—¿ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» (ğ—Ÿğ—®ğ˜€ğ˜€ğ—¼ ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—») 
    
    $\mathcal L_1$ Regularization (thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  Lasso) thÃªm hÃ¬nh pháº¡t lÃ  tá»•ng giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a cÃ¡c trá»ng sá»‘ (weights) vÃ o hÃ m máº¥t mÃ¡t: 
    
    $$
    \mathcal L_1 = \sum |w_i|
    $$
    
    Vá»›i $\mathcal L_1$, trá»ng sá»‘ cá»§a cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng quan trá»ng sáº½ bá»‹ giáº£m vá» 0, tá»« Ä‘Ã³ loáº¡i bá» chÃºng khá»i mÃ´ hÃ¬nh. ÄÃ¢y lÃ  lÃ½ do táº¡i sao $\mathcal L_1$ Ä‘Æ°á»£c sá»­ dá»¥ng trong viá»‡c feature selection (lá»±a chá»n Ä‘áº·c trÆ°ng). Äáº·c Ä‘iá»ƒm ná»•i báº­t cá»§a $\mathcal L_1$ lÃ  nÃ³ táº¡o ra má»™t mÃ´ hÃ¬nh thÆ°a thá»›t (sparse) â€“ nhiá»u trá»ng sá»‘ cÃ³ giÃ¡ trá»‹ báº±ng 0.
    
    ğŸ®. $\mathcal L_2$ ğ—¥ğ—²ğ—´ğ˜‚ğ—¹ğ—®ğ—¿ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» (ğ—¥ğ—¶ğ—±ğ—´ğ—² ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—») 
    
    $\mathcal L_2$ Regularization (hay cÃ²n gá»i lÃ  Ridge Regression) thÃªm hÃ¬nh pháº¡t lÃ  tá»•ng bÃ¬nh phÆ°Æ¡ng cá»§a cÃ¡c há»‡ sá»‘ vÃ o hÃ m máº¥t mÃ¡t: 
    
    $$
    \mathcal L_2 = \sum (w_i)^2
    $$
    
    KhÃ¡c vá»›i $\mathcal L_1$, $\mathcal L_2$ khÃ´ng giáº£m cÃ¡c trá»ng sá»‘ vá» 0 mÃ  thay vÃ o Ä‘Ã³ giáº£m giÃ¡ trá»‹ cá»§a chÃºng má»™t cÃ¡ch mÆ°á»£t mÃ . Do Ä‘Ã³, Ridge Regression thÆ°á»ng giá»¯ láº¡i táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng nhÆ°ng giáº£m trá»ng sá»‘ cá»§a chÃºng Ä‘á»ƒ lÃ m cho mÃ´ hÃ¬nh Ã­t phá»©c táº¡p hÆ¡n.
    
    ğŸ¯. ğ—˜ğ—¹ğ—®ğ˜€ğ˜ğ—¶ğ—° ğ—¡ğ—²ğ˜ ğ—¥ğ—²ğ—´ğ˜‚ğ—¹ğ—®ğ—¿ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—»
    
    Elastic Net káº¿t há»£p cáº£ $\mathcal L_1$ vÃ  $\mathcal L_2$ regularization báº±ng cÃ¡ch thÃªm cáº£ hai hÃ¬nh pháº¡t vÃ o hÃ m máº¥t mÃ¡t. CÃ´ng thá»©c cá»§a Elastic Net: 
    
    $$
    \mathcal L  = \mathcal L + \lambda_1 \sum |w_i| + \lambda_2\sum(w_i)^2 
    $$
    
    Elastic Net lÃ  sá»± pha trá»™n giá»¯a $\mathcal L_1$ vÃ  $\mathcal L_2$, nghÄ©a lÃ  nÃ³ vá»«a cÃ³ kháº£ nÄƒng lá»±a chá»n Ä‘áº·c trÆ°ng nhÆ° $\mathcal L_1$, vá»«a giá»¯ láº¡i cÃ¡c Ä‘áº·c trÆ°ng quan trá»ng nhÆ° $\mathcal L_2$. Äiá»u nÃ y Ä‘áº·c biá»‡t há»¯u Ã­ch trong cÃ¡c trÆ°á»ng há»£p dá»¯ liá»‡u cÃ³ nhiá»u Ä‘áº·c trÆ°ng liÃªn quan Ä‘áº¿n nhau (correlated features).
    
- **Æ¯u Ä‘iá»ƒm vÃ  khuyáº¿t Ä‘iá»ƒm cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p Regularization? Náº¿u há»‡ sá»‘ cá»§a thÃ nh pháº§n Regularization quÃ¡ lá»›n thÃ¬ sao? QuÃ¡ bÃ© thÃ¬ sao?**
    - Náº¿u regularization quÃ¡ nhá», mÃ´ hÃ¬nh cÃ³ thá»ƒ bá»‹ overfit
    - Náº¿u regu quÃ¡ lá»›n, mÃ´ hÃ¬nh dá»… bá»‹ underfit.
- **Tá»« cÃ¢u há»i trÃªn, giáº£i thÃ­ch hÃ¬nh áº£nh bÃªn dÆ°á»›i**
    
    ![Untitled](Untitled%2018.png)
    
- **Ká»¹ thuáº­t giáº£m sá»‘ chiá»u lÃ  gÃ¬?**
    
    **Ká»¹ thuáº­t giáº£m sá»‘ chiá»u (Dimensionality Reduction)** lÃ  quÃ¡ trÃ¬nh biáº¿n Ä‘á»•i dá»¯ liá»‡u tá»« khÃ´ng gian cÃ³ nhiá»u chiá»u (nhiá»u Ä‘áº·c trÆ°ng/features/attributes) xuá»‘ng khÃ´ng gian cÃ³ Ã­t chiá»u/Ã­t Ä‘áº·c trÆ°ng hÆ¡n, trong khi váº«n giá»¯ láº¡i nhá»¯ng thÃ´ng tin quan trá»ng nháº¥t. 
    
    > VÃ­ dá»¥ : Ä‘Æ°a 2 cá»™t chiá»u dÃ i vÃ  chiá»u rá»™ng â†’ Diá»‡n tÃ­ch; Ä‘Æ°a chiá»u cao vÃ  cÃ¢n náº·ng â†’ BMI
    > 
    
    Má»¥c Ä‘Ã­ch cá»§a viá»‡c nÃ y lÃ  Ä‘á»ƒ:
    
    1. **ÄÆ¡n giáº£n hÃ³a dá»¯ liá»‡u**: GiÃºp viá»‡c phÃ¢n tÃ­ch vÃ  trá»±c quan hÃ³a dá»¯ liá»‡u dá»… dÃ ng hÆ¡n.
    2. **Giáº£m chi phÃ­ tÃ­nh toÃ¡n**: Giáº£m sá»‘ lÆ°á»£ng tÃ­nh toÃ¡n cáº§n thiáº¿t cho cÃ¡c mÃ´ hÃ¬nh mÃ¡y há»c.
    3. **Giáº£m nhiá»…u**: Loáº¡i bá» cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng quan trá»ng hoáº·c khÃ´ng liÃªn quan, giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh.
    
    CÃ¡c phÆ°Æ¡ng phÃ¡p phá»• biáº¿n bao gá»“m:
    
    - **PCA (Principal Component Analysis)**: TÃ¬m cÃ¡c hÆ°á»›ng (thÃ nh pháº§n chÃ­nh) mÃ  dá»¯ liá»‡u phÃ¢n tÃ¡n nhiá»u nháº¥t, rá»“i chiáº¿u dá»¯ liá»‡u lÃªn cÃ¡c hÆ°á»›ng nÃ y. ÄÃ¢y lÃ  má»™t phÆ°Æ¡ng phÃ¡p Unsupervised learning.
    - **LDA (Linear Discriminant Analysis)**: Giáº£m sá»‘ chiá»u sao cho giá»¯ Ä‘Æ°á»£c lÆ°á»£ng thÃ´ng tin nhiá»u nháº¥t, trong khi PCA chÆ°a cháº¯c lÃ m Ä‘Æ°á»£c Ä‘iá»u nÃ y. ÄÃ¢y lÃ  má»™t phÆ°Æ¡ng phÃ¡p Supervised.
    
    <aside>
    ğŸ“Œ Tuy nhiÃªn cáº§n nhá»› ráº±ng khi giáº£m chiá»u báº±ng cÃ¡c thuáº­t toÃ¡n, ta sáº½ khÃ´ng â€œÄ‘áº·t tÃªnâ€ Ä‘Æ°á»£c cÃ¡c chiá»u thuá»™c tÃ­nh má»›i táº¡o ra lÃ  gÃ¬. VÃ­ dá»¥ giáº£m tá»« 10 features â†’ 3 components thÃ¬ ta sáº½ gá»i lÃ  PC1, PC2 vÃ  PC3. VÃ¬ má»—i PC Ä‘á»u giá»¯ thÃ´ng tin cá»§a cáº£ 10 features ban Ä‘áº§u, chá»‰ lÃ  Ã­t hay nhiá»u mÃ  thÃ´i.
    
    </aside>
    
    <aside>
    ğŸ“Œ Báº¡n cÃ³ thá»ƒ tÆ°á»Ÿng tÆ°á»£ng viá»‡c giáº£m sá»‘ chiá»u giá»‘ng nhÆ° viá»‡c thay Ä‘á»•i gÃ³c nhÃ¬n.
    
    - Má»™t máº·t pháº³ng 3D â†’ 1 Ä‘Æ°á»ng tháº³ng 2D â†’ 1 Ä‘iá»ƒm 1D.
    
    Tá»« Ä‘Ã³ ta cÃ³ thá»ƒ tháº¥y viá»‡c giáº£m sá»‘ chiá»u luÃ´n lÃ m máº¥t thÃ´ng tin tá»« bá»™ dá»¯ liá»‡u.
    
    </aside>
    
    ![Untitled](Untitled%2019.png)
    

![image.png](image%2016.png)

- **Táº¡i sao khi sá»­ dá»¥ng SGD, Ä‘áº·c biá»‡t lÃ  vá»›i sá»‘ sample nhá», sau má»™t khoáº£ng thá»i gian thÃ¬ loss cÃ³ hiá»‡n tÆ°á»£ng fluctuation nhÆ° trÃªn?**
    - Model sáº½ cá»‘ gáº¯ng tá»‘i Æ°u cho tá»«ng sample â†’ tÃ¹y theo Ä‘iá»ƒm á»Ÿ trÃªn hay á»Ÿ dÆ°á»›i mÃ  nÃ³ sáº½ di chuyá»ƒn lÃªn xuá»‘ng tÆ°Æ¡ng á»©ng â†’ loss fluctuated
        
        ![image.png](image%2017.png)
        
    
    CÃ¡ch kháº¯c phá»¥c:
    
    - TÄƒng dá»¯ liá»‡u
    - Tá»‘i Æ°u toÃ n bá»™ cÃ¹ng lÃºc
    - Early stopping
    - Giáº£m learning-rate

### Tá»•ng quan vá» Computational Graph

![[https://zhangruochi.com/Computational-Graph/2019/12/06/](https://zhangruochi.com/Computational-Graph/2019/12/06/)](image%2018.png)

[https://zhangruochi.com/Computational-Graph/2019/12/06/](https://zhangruochi.com/Computational-Graph/2019/12/06/)

Viá»‡c tÃ­nh toÃ¡n **Ä‘áº¡o hÃ m** dá»±a trÃªn **Computational graph (Ä‘á»“ thá»‹ tÃ­nh toÃ¡n)** vÃ  á»©ng dá»¥ng cá»§a **Chain Rule** **(chuá»—i máº¯t xÃ­ch)** Ä‘Ã£ Ä‘Ã³ng má»™t vai trÃ² ráº¥t quan trá»ng trong sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c thuáº­t toÃ¡n Deep Learning. NÃ³ lÃ  ná»n táº£ng, cá»‘t lÃµi cá»§a má»i thuáº­t toÃ¡n cáº§n tÃ­nh Ä‘áº¿n Ä‘áº¡o hÃ m vÃ  háº§u háº¿t Ä‘á»u Ä‘Æ°á»£c cÃ¡c thÆ° viá»‡n nhÆ° pytorch, tensorflow,â€¦ Ä‘á»u sá»­ dá»¥ng Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t tÃ­nh toÃ¡n.

<aside>
ğŸ“Œ Tuy viá»‡c hiá»ƒu nÃ³ hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o khÃ´ng máº¥y quan trá»ng. NhÆ°ng dáº¡o nÃ y mÃ¬nh cÃ³ há»©ng thÃº viáº¿t lÃ¡ch nÃªn sáºµn soáº¡n luÃ´n :))

CÃ¡c báº¡n cÃ³ thá»ƒ skip qua pháº§n cÃ¢u há»i Ã´n táº­p.

</aside>

### Lá»‹ch sá»­ cá»§a viá»‡c tÃ­nh toÃ¡n Ä‘áº¡o hÃ m báº±ng mÃ¡y tÃ­nh

TrÆ°á»›c khi cÃ¡c thuáº­t toÃ¡n há»c mÃ¡y nhÆ° ngÃ y nay phÃ¡t triá»ƒn, viá»‡c tÃ­nh toÃ¡n Ä‘áº¡o hÃ m vÃ  gradient thÆ°á»ng Ä‘Æ°á»£c thá»±c hiá»‡n má»™t cÃ¡ch giáº£i tÃ­ch báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° **Sai phÃ¢n há»¯u háº¡n (Finite Difference Method - FDM)** hay tá»•ng quÃ¡t hÆ¡n lÃ  **chuá»—i Taylor**. 

Ã tÆ°á»Ÿng cÆ¡ báº£n cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y lÃ  xáº¥p xá»‰ Ä‘áº¡o hÃ m cá»§a má»™t hÃ m sá»‘ báº±ng cÃ¡ch thay Ä‘á»•i má»™t giÃ¡ trá»‹ ráº¥t nhá» theo má»™t biáº¿n vÃ  quan sÃ¡t sá»± thay Ä‘á»•i cá»§a hÃ m sá»‘ (cÃ¡c biáº¿n cÃ²n láº¡i) tÆ°Æ¡ng á»©ng:

$$
f'(x) \approx \frac{f(x+h) - f(x)}{\varepsilon}
$$

> Má»i ngÆ°á»i cÃ³ thá»ƒ tham kháº£o tÃ i liá»‡u mÃ¬nh soáº¡n nÄƒm ngoÃ¡i soáº¡n vá» FDM:
> 
> 
> [Overleaf, Online LaTeX Editor](https://www.overleaf.com/read/ffqscvsfxfkv#c57c64)
> 
> Hoáº·c xem Ã½ tÆ°á»Ÿng cá»§a nhá»¯ng phÆ°Æ¡ng phÃ¡p kiá»ƒu kiá»ƒu nhÆ° váº­y:
> 
> [Understanding the Finite Element Method](https://www.youtube.com/watch?v=GHjopp47vvQ&t=416s&pp=ygUYZmluaXRlIGRpZmZlcmVuY2UgbWV0aG9k)
> 

Máº·c dÃ¹ cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao, nhÆ°ng nÃ³ cÃ³ má»™t sá»‘ nhÆ°á»£c Ä‘iá»ƒm:

1. Chi phÃ­ tÃ­nh toÃ¡n cao, Ä‘áº·c biá»‡t khi phÆ°Æ¡ng trÃ¬nh cÃ³ nhiá»u tham sá»‘.
2. Giáº£i theo scheme (kiá»ƒu nhÆ° má»™t Ä‘á»‘ng cÃ´ng thá»©c cá»‘ Ä‘á»‹nh â†’ gáº§n nhÆ° lÃ  giáº£i nghiá»‡m chÃ­nh xÃ¡c) â†’ khÃ´ng thá»ƒ handle dá»¯ liá»‡u nhiá»…u.
3. CÃ¡c biáº¿n thá»ƒ cá»§a nÃ³ ngÃ y cÃ ng phá»©c táº¡p, lÃªn Ä‘áº¿n hÃ ng triá»‡u dÃ²ng code â†’ khÃ³ má»Ÿ rá»™ng, chá»‰nh sá»­a.
4. â€¦.

### Sá»± xuáº¥t hiá»‡n cá»§a **Chain Rule** vÃ  CÃ¡c cá»™t má»‘c Ä‘Ã¡ng chÃº Ã½

Váº¥n Ä‘á» lá»›n nháº¥t trong viá»‡c tÃ­nh toÃ¡n Ä‘áº¡o hÃ m lÃ  lÃ m sao cÃ³ thá»ƒ tÃ­nh toÃ¡n **hiá»‡u quáº£** vÃ  **chÃ­nh xÃ¡c** má»™t hÃ m sá»‘ phá»©c táº¡p khi cÃ³ nhiá»u biáº¿n phá»¥ thuá»™c láº«n nhau mÃ  ta chÆ°a tÃ¬m ra cÃ´ng thá»©c chung cho chÃºng â€” vÃ­ dá»¥ nhÆ° $f(g(x))$.  

**Chain Rule** lÃ  má»™t cÃ¡ch hiá»‡u quáº£ Ä‘á»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y, nÃ³ giÃºp ta tÃ­nh Ä‘áº¡o hÃ m cá»§a cÃ¡c hÃ m há»£p, tá»©c lÃ  khi má»™t biáº¿n phá»¥ thuá»™c vÃ o nhiá»u biáº¿n khÃ¡c thÃ´ng qua cÃ¡c hÃ m trung gian.

$$
\frac{\partial [f(g(x))]}{\partial x} = \frac{\partial f}{\partial g} \times \frac{\partial g}{\partial x}
$$

VÃ  tháº­t ra nÃ³ Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng ráº¥t lÃ¢u trong giáº£i tÃ­ch toÃ¡n há»c, vÃ o khoáº£ng cuá»‘i tháº¿ ká»· 20, cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u vÃ  **backpropagation** (lan truyá»n ngÆ°á»£c) báº¯t Ä‘áº§u trá»Ÿ thÃ nh phÆ°Æ¡ng phÃ¡p chá»§ Ä‘áº¡o Ä‘á»ƒ huáº¥n luyá»‡n máº¡ng neuron. 

**Cá»™t má»‘c Ä‘Ã¡ng chÃº Ã½**:

![image.png](image%2019.png)

- **Paul Werbos** vÃ o nÄƒm 1974 Ä‘Ã£ Ä‘á» xuáº¥t **Backpropagation (Lan truyá»n ngÆ°á»£c)** trong luáº­n Ã¡n tiáº¿n sÄ© cá»§a Ã´ng, backpropagation Ã¡p dá»¥ng Chain Rule má»™t cÃ¡ch cÃ³ há»‡ thá»‘ng Ä‘á»ƒ tÃ­nh toÃ¡n gradient trong máº¡ng neuron.
    - Máº·c dÃ¹ Ã´ng khÃ´ng trá»±c tiáº¿p sá»­ dá»¥ng **Computational graph**, nhÆ°ng Ã½ tÆ°á»Ÿng vá» viá»‡c tÃ­nh toÃ¡n theo tá»«ng bÆ°á»›c phá»¥ thuá»™c cá»§a hÃ m há»£p lÃ  cÆ¡ sá»Ÿ cho sá»± phÃ¡t triá»ƒn sau nÃ y.
- MÃ£i Ä‘áº¿n nÄƒm 1986, phÆ°Æ¡ng phÃ¡p nÃ y má»›i Ä‘Æ°á»£c phá»• biáº¿n rá»™ng rÃ£i hÆ¡n qua bÃ i viáº¿t cá»§a **David E. Rumelhart**, **Geoffrey Hinton**, vÃ  **Ronald J. Williams**, má»Ÿ ra ká»· nguyÃªn má»›i cho há»c sÃ¢u.
    - Tuy lÃºc Ä‘Ã³ khÃ¡i niá»‡m **Computational graph** chÆ°a Ä‘Æ°á»£c Ä‘á» cáº­p cá»¥ thá»ƒ, nhÆ°ng Ã½ tÆ°á»Ÿng vá» viá»‡c **phÃ¢n rÃ£ hÃ m phá»©c há»£p thÃ nh cÃ¡c phÃ©p tÃ­nh Ä‘Æ¡n giáº£n** vÃ  tÃ­nh gradient thÃ´ng qua cÃ¡c lá»›p cá»§a máº¡ng chÃ­nh lÃ  ná»n táº£ng cá»§a Ä‘á»“ thá»‹ tÃ­nh toÃ¡n.

### VÃ¬ sao ngÆ°á»i ta nghÄ© Ä‘áº¿n **Computational graph**?

**Giai Ä‘oáº¡n sau 2010 - Há»‡ thá»‘ng autodiff vÃ  deep learning frameworks**:

- Vá»›i sá»± bÃ¹ng ná»• cá»§a **Deep learning** vÃ o nhá»¯ng nÄƒm 2010, cÃ¡c **frameworks nhÆ° TensorFlow, PyTorch** Ä‘Ã£ chÃ­nh thá»©c Ä‘Æ°a khÃ¡i niá»‡m **computational graph** vÃ o sá»­ dá»¥ng Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a quÃ¡ trÃ¬nh tÃ­nh toÃ¡n gradient.
- CÃ¡c framework nÃ y táº¡o ra **computational graph** Ä‘á»™ng hoáº·c tÄ©nh Ä‘á»ƒ tá»• chá»©c quÃ¡ trÃ¬nh tÃ­nh toÃ¡n, cho phÃ©p thá»±c hiá»‡n cÃ¡c phÃ©p toÃ¡n phá»©c táº¡p nhÆ° cá»™ng, nhÃ¢n, hoáº·c cÃ¡c hÃ m phi tuyáº¿n, sau Ä‘Ã³ sá»­ dá»¥ng **backpropagation** vÃ  **Chain Rule** Ä‘á»ƒ tÃ­nh toÃ¡n gradient dá»±a trÃªn Ä‘á»“ thá»‹ tÃ­nh toÃ¡n Ä‘Ã³.

Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n máº¡ng neuron, viá»‡c tÃ­nh toÃ¡n Ä‘áº¡o hÃ m thÃ´ng qua **computational graph** giÃºp chÃºng ta phÃ¢n chia tá»«ng phÃ©p toÃ¡n thÃ nh cÃ¡c thÃ nh pháº§n nhá». Má»—i phÃ©p toÃ¡n (vÃ­ dá»¥: cá»™ng, nhÃ¢n, hÃ m sá»‘) Ä‘á»u cÃ³ Ä‘áº¡o hÃ m Ä‘Æ¡n giáº£n Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c. Äiá»u nÃ y cho phÃ©p chÃºng ta sá»­ dá»¥ng cÃ¡c **closed-form derivatives** (Ä‘áº¡o hÃ m dáº¡ng Ä‘Ã³ng) thay vÃ¬ pháº£i lÆ°u trá»¯ cÃ¡c cÃ´ng thá»©c phá»©c táº¡p cá»§a toÃ n bá»™ quÃ¡ trÃ¬nh tÃ­nh toÃ¡n.

- **Computational Graph lÃ  gÃ¬?**
    - Biá»ƒu diá»…n cÃ¡c phÃ©p toÃ¡n vÃ  cÃ¡c má»‘i quan há»‡ phá»¥ thuá»™c giá»¯a cÃ¡c giÃ¡ trá»‹ dÆ°á»›i dáº¡ng Ä‘á»“ thá»‹.
    - Má»—i nÃºt trong Ä‘á»“ thá»‹ biá»ƒu diá»…n má»™t phÃ©p toÃ¡n (nhÆ° cá»™ng, nhÃ¢n,â€¦), vÃ  cÃ¡c cáº¡nh biá»ƒu diá»…n sá»± phá»¥ thuá»™c (Ä‘áº¡o hÃ m) giá»¯a cÃ¡c phÃ©p toÃ¡n nÃ y.
    
    ![[https://zhangruochi.com/Computational-Graph/2019/12/06/](https://zhangruochi.com/Computational-Graph/2019/12/06/)](image%2018.png)
    
    [https://zhangruochi.com/Computational-Graph/2019/12/06/](https://zhangruochi.com/Computational-Graph/2019/12/06/)
    
- **Táº¡i sao Computational Graph láº¡i quan trá»ng trong tÃ­nh toÃ¡n Ä‘áº¡o hÃ m?**
    - Biá»ƒu diá»…n cÃ¡c phÃ©p toÃ¡n phá»©c táº¡p dÆ°á»›i dáº¡ng cÃ¡c bÆ°á»›c nhá» hÆ¡n.
    - Sá»­ dá»¥ng **Chain Rule** Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m má»™t cÃ¡ch cÃ³ há»‡ thá»‘ng.
- **Chain Rule lÃ  gÃ¬ vÃ  táº¡i sao nÃ³ quan trá»ng trong Machine Learning?**
    - Quy táº¯c Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m hÃ m há»£p.
    - GiÃºp tÃ­nh toÃ¡n gradient trong cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p
- **Backpropagation lÃ  gÃ¬ vÃ  liÃªn quan Ä‘áº¿n Chain Rule nhÆ° tháº¿ nÃ o?**
    - **Backpropagation** lÃ  má»™t phÆ°Æ¡ng phÃ¡p sá»­ dá»¥ng **Chain Rule** Ä‘á»ƒ tÃ­nh gradient cá»§a hÃ m loss Ä‘á»‘i vá»›i cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh.
    - NÃ³ lan truyá»n ngÆ°á»£c tá»« Ä‘áº§u ra vá» cÃ¡c lá»›p trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ má»™t cÃ¡ch hiá»‡u quáº£, giÃºp mÃ´ hÃ¬nh há»c tá»« dá»¯ liá»‡u.
- **VÃ­ dá»¥ vá» Computational Graph**
    
    ![image.png](image%2020.png)
    
    ![image.png](image%2021.png)
    
    ![image.png](image%2022.png)
    

### Mini-Batch training

- **Trong 3 loáº¡i phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n theo batch, loáº¡i nÃ o cÃ³ kháº£ nÄƒng handle nhiá»…u vÃ  vÆ°á»£t qua local minimum lá»›n nháº¥t?**
    - Full batch tá»‘t nháº¥t,
    - Mini-Batch hÃªn xui tÃ¹y theo sá»‘ batch
    - SGD kháº£ nÄƒng cao bá»‹ vÆ°á»›ng láº¡i local minimum nháº¥t.
- **ThÆ°á»ng ngÆ°á»i ta chá»n sá»‘ batch lÃ  máº¥y?**
    
    $2^n$ â†’ $2, 4, 8, 16, 32, 64, 128,...$
    
    - CÃ³ thá»ƒ lÃ  báº¥t kÃ¬ sá»‘ nÃ o khÃ¡c
- **Khi tá»‘i Æ°u báº±ng mini-batch, giáº£ sá»­ $m = 2$ thÃ¬ ta sáº½ tá»‘i Æ°u tuáº§n tá»± hay Ä‘á»“ng thá»i cáº£ 2 Ä‘iá»ƒm?**
    - Back-prop Ä‘á»“ng thá»i ra $\mathcal L^{(1)},\mathcal L^{(2)}$ â†’ sau Ä‘Ã³ láº¥y trung bÃ¬nh
    
    ![image.png](image%2023.png)
    
- **Khi thá»±c hiá»‡n mini-batch training (bao gá»“m cáº£ SGD), sau má»—i epoch chÃºng ta nÃªn thá»±c hiá»‡n thao tÃ¡c gÃ¬ vá»›i dá»¯ liá»‡u?**
    - Shuffle Ä‘á»ƒ táº¡o sá»± ngáº«u nhiÃªn, trÃ¡nh overfit
    
    > LÆ°u Ã½ ráº±ng khÃ´ng shuffle vá»›i dá»¯ liá»‡u Time Series hoáº·c NLP trong bá»‘i cáº£nh chÃºng ta muá»‘n giá»¯ cÃ¡c yáº¿u tá»‘ tuáº§n tá»± thá»i gian, ngá»¯ nghÄ©a, cáº¥u trÃºc ngá»¯ phÃ¡p,â€¦
    > 
    
    ![image.png](image%2024.png)
    
    ![image.png](image%2025.png)
    

# Tháº£o luáº­n thÃªm

- **Sá»­ dá»¥ng tá»« â€œbiáº¿n Ä‘á»™c láº­pâ€ (independent) cho cÃ¡c biáº¿n $x_i$ lÃ  má»™t cÃ¡ch gá»i tá»‡ háº¡i**
    
    [In Regression Analysis, why do we call independent variables "independent"?](https://stats.stackexchange.com/questions/357745/in-regression-analysis-why-do-we-call-independent-variables-independent)
    
    ![image.png](image%2026.png)
    
    <aside>
    ğŸ“Œ
    
    NghÄ©a lÃ  trÃªn lÃ½ thuyáº¿t thÃ¬ cÃ¡c biáº¿n $x_i$ pháº£i Ä‘á»™c láº­p vá»›i nhau, vÃ  $y$ pháº£i phá»¥ thuá»™c vÃ o $x$. Tuy nhiÃªn trong thá»±c táº¿ khÃ´ng nháº¥t thiáº¿t pháº£i nhÆ° váº­y.
    
    CÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ thá»±c hiá»‡n Linear Regression ngay cáº£ khi cÃ³ sá»± phá»¥ thuá»™c giá»¯a cÃ¡c biáº¿n hoáº·c khÃ´ng cÃ³ quÃ¡ nhiá»u sá»± tÆ°Æ¡ng quan giá»¯a $x$ vÃ  $y$.
    
    </aside>
    
    ![image.png](image%2027.png)
    
    ![image.png](image%2028.png)
    
- **Tháº£o luáº­n vá» tÃ­nh phÆ°Æ¡ng sai Ä‘á»“ng nháº¥t vÃ  khÃ´ng Ä‘á»“ng nháº¥t.**
    
    [**TÃ­nh khÃ´ng Ä‘á»“ng nháº¥t (Heteroskedasticity) vÃ  TÃ­nh Ä‘á»“ng nháº¥t (Homoskedasticity) cá»§a phÆ°Æ¡ng sai**](https://www.notion.so/T-nh-kh-ng-ng-nh-t-Heteroskedasticity-v-T-nh-ng-nh-t-Homoskedasticity-c-a-ph-ng-sai-17ce9755c5d0812abbeafdd064a7a80f?pvs=21) 
    
- **PhÆ°Æ¡ng trÃ¬nh Linear Regression trong thá»‘ng kÃª?**
    
    $$
    y = f(x) + \varepsilon
    $$
    
    Trong Ä‘Ã³ $\varepsilon$  lÃ  sai sá»‘ tá»± nhiÃªn.
    
- **Sai sá»‘ tá»± nhiÃªn lÃ  gÃ¬?**
    
    **Sai sá»‘ tá»± nhiÃªn** (hay cÃ²n gá»i lÃ  **sai sá»‘ khÃ´ng thá»ƒ trÃ¡nh khá»i**, **natural error**, hoáº·c **intrinsic error**) lÃ  cÃ¡c sai sá»‘ xáº£y ra do cÃ¡c yáº¿u tá»‘ khÃ¡ch quan vÃ  khÃ´ng thá»ƒ kiá»ƒm soÃ¡t hoáº·c loáº¡i bá» hoÃ n toÃ n, ngay cáº£ khi cÃ¡c phÃ©p Ä‘o Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao. ÄÃ¢y lÃ  nhá»¯ng sai sá»‘ tá»“n táº¡i trong báº¥t ká»³ há»‡ thá»‘ng Ä‘o lÆ°á»ng hoáº·c tÃ­nh toÃ¡n nÃ o vÃ  thÆ°á»ng khÃ´ng thá»ƒ loáº¡i bá» hoÃ n toÃ n.
    
    ### CÃ¡c yáº¿u tá»‘ gÃ¢y ra sai sá»‘ tá»± nhiÃªn bao gá»“m:
    
    1. **Sá»± thay Ä‘á»•i mÃ´i trÆ°á»ng**:
        - Äiá»u kiá»‡n mÃ´i trÆ°á»ng nhÆ° nhiá»‡t Ä‘á»™, Ã¡p suáº¥t, Ä‘á»™ áº©m cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n phÃ©p Ä‘o mÃ  khÃ´ng thá»ƒ kiá»ƒm soÃ¡t hoÃ n toÃ n.
    2. **Giá»›i háº¡n vá» cÃ´ng cá»¥ Ä‘o lÆ°á»ng**:
        - Ngay cáº£ cÃ¡c thiáº¿t bá»‹ Ä‘o lÆ°á»ng hiá»‡n Ä‘áº¡i vÃ  chÃ­nh xÃ¡c cÅ©ng cÃ³ nhá»¯ng giá»›i háº¡n vá» Ä‘á»™ chÃ­nh xÃ¡c. Äiá»u nÃ y dáº«n Ä‘áº¿n sai sá»‘ tá»± nhiÃªn trong cÃ¡c káº¿t quáº£ Ä‘o.
    3. **Sá»± khÃ´ng á»•n Ä‘á»‹nh cá»§a Ä‘á»‘i tÆ°á»£ng Ä‘o**:
        - Má»™t sá»‘ há»‡ thá»‘ng hoáº·c Ä‘á»‘i tÆ°á»£ng tá»± nhiÃªn cÃ³ thá»ƒ thay Ä‘á»•i theo thá»i gian, gÃ¢y ra sá»± khÃ´ng á»•n Ä‘á»‹nh vÃ  lÃ m xuáº¥t hiá»‡n sai sá»‘ trong cÃ¡c phÃ©p Ä‘o láº·p láº¡i.
    4. **Sá»± nhiá»…u (noise)**:
        - Trong cÃ¡c há»‡ thá»‘ng váº­t lÃ½ vÃ  ká»¹ thuáº­t, nhiá»…u luÃ´n tá»“n táº¡i á»Ÿ má»©c Ä‘á»™ nháº¥t Ä‘á»‹nh. ÄÃ¢y lÃ  má»™t dáº¡ng sai sá»‘ tá»± nhiÃªn khÃ´ng thá»ƒ trÃ¡nh khá»i, cháº³ng háº¡n nhÆ° nhiá»…u nhiá»‡t trong cÃ¡c há»‡ thá»‘ng Ä‘iá»‡n tá»­.
- **Äiá»u kiá»‡n cá»§a sai sá»‘ trong Linear Regression lÃ  gÃ¬?**
    
    **1. Ká»³ vá»ng cá»§a sai sá»‘ báº±ng 0**:
    
    $$
    \mathbb E[\varepsilon] = 0
    $$
    
    Äiá»u nÃ y cÃ³ nghÄ©a lÃ  khÃ´ng cÃ³ sai lá»‡ch cÃ³ há»‡ thá»‘ng trong mÃ´ hÃ¬nh, vÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c giÃ¡ trá»‹ trung bÃ¬nh cá»§a $y$ dá»±a trÃªn cÃ¡c biáº¿n Ä‘á»™c láº­p $x$. Náº¿u Ä‘iá»u nÃ y khÃ´ng Ä‘Æ°á»£c thá»a mÃ£n, mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°a ra cÃ¡c dá»± Ä‘oÃ¡n sai.
    
    **2.** **PhÆ°Æ¡ng sai cá»§a sai sá»‘ khÃ´ng Ä‘á»•i (Homoscedasticity)**:
    
    $$
    Var(\varepsilon|x) = \sigma^2
    $$
    
    Äiá»u nÃ y Ä‘áº£m báº£o ráº±ng Ä‘á»™ phÃ¢n tÃ¡n cá»§a sai sá»‘ khÃ´ng thay Ä‘á»•i theo cÃ¡c giÃ¡ trá»‹ cá»§a $x$. Náº¿u phÆ°Æ¡ng sai thay Ä‘á»•i, thÃ¬ mÃ´ hÃ¬nh há»“i quy cÃ³ thá»ƒ dáº«n Ä‘áº¿n cÃ¡c Æ°á»›c lÆ°á»£ng khÃ´ng chÃ­nh xÃ¡c vÃ  khÃ´ng hiá»‡u quáº£.
    
    **3. Sai sá»‘ cÃ³ phÃ¢n phá»‘i chuáº©n** **(Normality of errors):**
    
    $$
    \varepsilon \sim \mathcal N(0, \sigma^2)
    $$
    
- **Náº¿u khÃ´ng cÃ³ pháº§n sai sá»‘ thÃ¬ sao?**
    
    Náº¿u khÃ´ng cÃ³ pháº§n sai sá»‘, tá»©c lÃ  $\varepsilon = 0$, Ä‘iá»u nÃ y cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh **hoÃ n toÃ n khá»›p vá»›i dá»¯ liá»‡u**, hay nÃ³i cÃ¡ch khÃ¡c, giÃ¡ trá»‹ dá»± Ä‘oÃ¡n tá»« mÃ´ hÃ¬nh $f(x)$ luÃ´n chÃ­nh xÃ¡c báº±ng giÃ¡ trá»‹ thá»±c táº¿ $y$.
    
    ```python
    # Táº¡o láº¡i dá»¯ liá»‡u tá»« Ä‘áº§u
    np.random.seed(0)
    x = np.linspace(0, 10, 100)  # Táº¡o 100 Ä‘iá»ƒm tá»« 0 Ä‘áº¿n 10
    y_perfect = 2 * x + 3  # HÃ m tuyáº¿n tÃ­nh y = 2x + 3 khÃ´ng cÃ³ sai sá»‘
    y_actual = 2 * x + 3 + np.random.normal(0, 2, size=x.shape)  # ThÃªm nhiá»…u vá»›i sai sá»‘ chuáº©n (mean=0, std=2)
    
    # Váº½ biá»ƒu Ä‘á»“ tá»« Ä‘áº§u vá»›i scatter plot cho cáº£ hai bÃªn
    fig, ax = plt.subplots(1, 2, figsize=(14, 6))
    
    # Äá»“ thá»‹ 1: KhÃ´ng cÃ³ sai sá»‘ vá»›i scatter plot
    ax[0].plot(x, y_perfect, label="y = 2x + 3 (KhÃ´ng cÃ³ sai sá»‘)", color="blue")
    ax[0].scatter(x, y_perfect, label="Dá»¯ liá»‡u khÃ´ng cÃ³ sai sá»‘", color="green", alpha=0.6)
    ax[0].set_title("MÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh khÃ´ng cÃ³ pháº§n sai sá»‘")
    ax[0].set_xlabel("Biáº¿n Ä‘á»™c láº­p x")
    ax[0].set_ylabel("Biáº¿n phá»¥ thuá»™c y")
    ax[0].legend()
    ax[0].grid(True)
    
    # Äá»“ thá»‹ 2: CÃ³ sai sá»‘ (scatter plot)
    ax[1].plot(x, y_perfect, label="y = 2x + 3 (KhÃ´ng cÃ³ sai sá»‘)", color="blue")
    ax[1].scatter(x, y_actual, label="Dá»¯ liá»‡u thá»±c táº¿ (cÃ³ sai sá»‘)", color="red", alpha=0.6)
    ax[1].set_title("MÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh vá»›i pháº§n sai sá»‘")
    ax[1].set_xlabel("Biáº¿n Ä‘á»™c láº­p x")
    ax[1].set_ylabel("Biáº¿n phá»¥ thuá»™c y")
    ax[1].legend()
    ax[1].grid(True)
    
    # Hiá»ƒn thá»‹ Ä‘á»“ thá»‹
    plt.tight_layout()
    plt.show()
    ```
    
- **Khi nÃ o thÃ¬ má»™t hÃ m sá»‘ Ä‘Æ°á»£c xem lÃ  liÃªn tá»¥c (continuous) vÃ  khÃ´ng liÃªn tá»¥c (discontinuous)?**
    - Khi lim trÃ¡i = lim pháº£i táº¡i toÃ n bá»™ Ä‘iá»ƒm thÃ¬ hÃ m sá»‘ liÃªn tá»¥c
    - Náº¿u khÃ´ng = thÃ¬ liÃªn tá»¥c.
    
    ***VÃ­ dá»¥***: HÃ¬nh bÃªn trÃ¡i
    
    $$
    \lim_{x\to 1^+} = 1 \\ \lim_{x\to 1^-} = 0 
    $$
    
    ![image.png](image%2029.png)
    
    ![image.png](image%2030.png)
    
- **Khi nÃ o má»™t hÃ m sá»‘ Ä‘Æ°á»£c gá»i lÃ  kháº£ vi (differentiable)?**
    
    ![image.png](image%2031.png)
    
- **Táº¡i sao ta khÃ´ng tÃ­nh Ä‘Æ°á»£c nghiá»‡m chÃ­nh xÃ¡c náº¿u cÃ¡c feature bá»‹ phá»¥ thuá»™c tuyáº¿n tÃ­nh?**
    
    KhÃ´ng tá»“n táº¡i ma tráº­n nghá»‹ch Ä‘áº£o
    
- **Khi cÃ¡c feature bá»‹ phá»¥ thuá»™c tuyáº¿n tÃ­nh thÃ¬ ta cÃ³ thá»ƒ Ã¡p dá»¥ng ká»¹ thuáº­t gÃ¬ Ä‘á»ƒ kháº¯c phá»¥c?**
    1. XÃ³a duplicate, feature selection
    2. Ma tráº­n giáº£ nghá»‹ch Ä‘áº£o
        
        [Mooreâ€“Penrose inverse](https://en.wikipedia.org/wiki/Mooreâ€“Penrose_inverse)
        
    3. Äá»•i thuáº­t toÃ¡n khÃ¡c
- **Tá»« cÃ¢u â€£, hÃ£y cho biáº¿t cÃ¡ch Ä‘á»ƒ tÄƒng sá»‘ báº­c trong mÃ¡y tÃ­nh?**
    
    Tháº­t ra báº£n cháº¥t khÃ´ng cÃ³ gÃ¬ khÃ¡c so vá»›i Linear Regression háº¿t, nÃ³ thay Ä‘á»•i báº­c trÃªn chÃ­nh bá»™ data báº±ng cÃ¡ch táº¡o thÃªm cÃ¡c cá»™t má»›i nhÆ° sau:
    
    - Giáº£ sá»­ ta cÃ³ 2 feature vÃ  sá»‘ báº­c lÃ  2:
    
    $$
    y = \theta_0 + \theta_1x_1 + \theta_2x_2+\theta_3x_1^2+\theta_4x_2^2 + \theta_5x_1x_2
    $$
    
    | $Y$ | $X_1$ | $X_2$ | $X_1^2$ | $X_2^2$ | $X_1X_2$ |
    | --- | --- | --- | --- | --- | --- |
    | â€¦ | 1 | 3 | 1 | 9 | 3 |
    | â€¦ | 2 | 5 | 4 | 25 | 10 |
    
- **Tá»« cÃ¢u trÃªn, hÃ£y cho biáº¿t háº¡n cháº¿ cá»§a viá»‡c tÄƒng sá»‘ báº­c nÃ y lÃ  gÃ¬?**
    
    Dáº«n Ä‘áº¿n sá»‘ chiá»u tÄƒng theo cáº¥p sá»‘ nhÃ¢n â†’ curse of dimensionality â†’ khÃ³ Ä‘á»ƒ cÃ³ thá»ƒ thá»±c hiá»‡n phÃ¢n loáº¡i hoáº·c há»“i quy.
    
- **Má»™t sá»‘ cÃ¡ch giáº£i quyáº¿t háº¡n cháº¿ trÃªn?**
    1. Feature Selection, Feature Engineering
    2. Kernel Trick
    3. Sá»­ dá»¥ng thuáº­t toÃ¡n khÃ¡c nhÆ° Deep Learning
    4. â€¦

<aside>
ğŸ“Œ CÃ¢u há»i sau chÃºng ta sáº½ biáº¿t thÃªm má»™t háº¡n cháº¿ ná»¯a cá»§a MAE, MSE

</aside>

Thay vÃ¬ ta xáº¥p xá»‰ hÃ m $y = x^2$ báº±ng Polynomial Regression hoáº·c Neural Network báº±ng cÃ¡ch giáº£m thiá»ƒu sai sá»‘ MSE nhÆ° hÃ¬nh sau:

![image.png](image%2032.png)

ThÃ¬ chÃºng ta sáº½ Ä‘i láº­t ngang nÃ³ láº¡i $y^2 = x$:

![image.png](image%2033.png)

- **CÃ¢u há»i: Dá»± Ä‘oÃ¡n xem Ä‘iá»u gÃ¬ sáº½ xáº£y ra?**
    
    NÃ³ sáº½ luÃ´n láº¥y trung bÃ¬nh cÃ¡c Ä‘iá»ƒm nÃ y â†’ tráº£ vá» Ä‘Æ°á»ng tháº³ng trung bÃ¬nh náº±m ngang
    
    ![image.png](image%2034.png)
    
    ÄÃ³ lÃ  lÃ­ do chÃºng ta luÃ´n giáº£ sá»­ má»—i input chá»‰ cÃ³ Ä‘Ãºng 1 nghiá»‡m, hoáº·c cÃ¡c hÃ m sá»‘ khÃ´ng quÃ¡ Ä‘áº·c biá»‡t.
    
- **CÃ¡ch giáº£i quyáº¿t háº¡n cháº¿ trÃªn**
    
    TÃ¡ch ra xáº¥p xá»‰ tá»«ng Ä‘oáº¡n trÃªn, dÆ°á»›i
    
    ![image.png](image%2035.png)